[{"path":[]},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement [INSERT CONTACT METHOD]. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.0, available https://www.contributor-covenant.org/version/2/0/ code_of_conduct.html. Community Impact Guidelines inspired Mozilla’s code conduct enforcement ladder. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https:// www.contributor-covenant.org/translations.","code":""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"from_text_to_measurement","text":"vignette guides workflow described Thiele (2022) applying ‘Distributed Dictionary Representation’ (DDR) Method (Garten et al., 2018) introduces functions dictvectoR package. DDR method provides continuous measurement concept dataset documents. measurement obtained calculating average word vector representation concept dictionary, representations document, calculating cosine similarity two vectors. detailed description, see Garten et al. (2018) Thiele (2022).","code":""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"from_text_to_measurement","text":"workflow described starts scratch, using textual data input, guides steps finding inductively list keywords, demonstrates evaluation application. requires population dataset containing textual data hand-coded sample drawn dataset, annotated presence theoretical concept. use built-dataset tw_data population data tw_annot annotated sample. vignette follows steps: Pre-process data Train fastText model Finding keywords Add multi-words Get F1 scores Drop similar terms Narrowing hand Get combinations terms. Evaluate performance combinations. Apply best performing dictionary. Face validity.","code":""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"pre-process-data","dir":"Articles","previous_headings":"","what":"Pre-process data","title":"from_text_to_measurement","text":"First load pre-process built textual data. clean_text cleans text tailored German-speaking texts social media. using text languages, may want adapt helper-function needs. prepare_train_data prepares textual data training fastText model. tokenizes longer documents sentences, shuffles , calls clean_text fixed settings. first prepare texts character vector training fastText model, clean text tw_data tw_annot later analyses.","code":"# Prepare text data texts <- prepare_train_data(tw_data, text_field = \"full_text\", seed = 42)  # Clean text in tw_data tw_data %<>% clean_text(remove_stopwords = T,                         text_field = \"full_text\")  # Clean text in tw_annot tw_annot %<>% clean_text(remove_stopwords = T,                          text_field = \"full_text\")"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"train-fasttext-model","dir":"Articles","previous_headings":"","what":"Train fastText model","title":"from_text_to_measurement","text":"Now, train fastText model using fastrtext::build_vectors. customized word vector model advantage maps words contexts actually appear studied material. important , use vocabulary model starting point conceptual dictionary. downside training model expensive regarding memory computation. used machine CPU @ 1.80GHz processor 8 cores 16 GB RAM. run memory limitations, consider decreasing dim, number dimensions. Decreasing number epochs speed learing process, decreasing bucket size reduce size resulting model. details, see Bojanowski et al. (2017). obtain model better quality, also consider increasing size textual data used training. example limited size data can feasibly shipped R-package. Alternatively, may consider using pre-trained model . code creates local folder ft_model user home directory, saves model two files: ft_model.bin ft_model.vec. (Note: code runs model file exists. training algorithm create perfectly identical word vector models, can cause problems re-running vignette.) Let’s load model use nearest-neighbor query check works properly. ‘Spahn’ name former German minister health:","code":"# Create local folder and set name for model dir.create(\"~/ft_model\", showWarnings = FALSE) model_file <- path.expand(\"~/ft_model/ft_model\")  # Train a fasttext model using the twitter data (if model does not yet exist)  if (!file.exists(paste0(model_file, \".bin\"))) {    fastrtext::build_vectors(texts, model_file, modeltype = c(\"skipgram\"),                               dim = 150, epoch = 10, bucket = 1e+6,  lr = 0.05,                               lrUpdateRate = 100, maxn = 7,  minn = 4, minCount = 3,                               t = 1e-04, thread = 8, ws= 6)   } #> writing tempfile at ... /tmp/Rtmp2qiPkJ/file382153930e5a #> Starting training vectors with following commands:  #> $ skipgram -input /tmp/Rtmp2qiPkJ/file382153930e5a -output /home/runner/ft_model/ft_model -bucket 1000000 -dim 150 -epoch 10 -label __label__ -loss ns -lr 0.05 -lrUpdateRate 100 -maxn 7 -minCount 3 -minn 4 -neg 5 -t 0.0001 -thread 8 -verbose 2 -wordNgrams 1 -ws 6 #>  Read 0M words #> Number of words:  13363 #> Number of labels: 0 #>  Progress:   0.2% words/sec/thread:   11569 lr:  0.049922 avg.loss:  4.144461 ETA:   0h 1m 3s Progress:   0.3% words/sec/thread:   11626 lr:  0.049843 avg.loss:  4.136104 ETA:   0h 1m 3s Progress:   0.5% words/sec/thread:   11499 lr:  0.049767 avg.loss:  4.130494 ETA:   0h 1m 4s Progress:   0.6% words/sec/thread:   11647 lr:  0.049685 avg.loss:  4.127006 ETA:   0h 1m 3s Progress:   0.8% words/sec/thread:   11711 lr:  0.049604 avg.loss:  4.122545 ETA:   0h 1m 2s Progress:   0.9% words/sec/thread:   11680 lr:  0.049526 avg.loss:  4.098166 ETA:   0h 1m 2s Progress:   1.1% words/sec/thread:   11644 lr:  0.049449 avg.loss:  4.037491 ETA:   0h 1m 2s Progress:   1.3% words/sec/thread:   11684 lr:  0.049368 avg.loss:  3.939727 ETA:   0h 1m 2s Progress:   1.4% words/sec/thread:   11680 lr:  0.049289 avg.loss:  3.819400 ETA:   0h 1m 2s Progress:   1.6% words/sec/thread:   11695 lr:  0.049209 avg.loss:  3.711042 ETA:   0h 1m 2s Progress:   1.7% words/sec/thread:   11710 lr:  0.049129 avg.loss:  3.611560 ETA:   0h 1m 2s Progress:   1.9% words/sec/thread:   11732 lr:  0.049048 avg.loss:  3.515135 ETA:   0h 1m 1s Progress:   2.1% words/sec/thread:   11730 lr:  0.048969 avg.loss:  3.437572 ETA:   0h 1m 1s Progress:   2.2% words/sec/thread:   11672 lr:  0.048896 avg.loss:  3.374799 ETA:   0h 1m 2s Progress:   2.4% words/sec/thread:   11669 lr:  0.048817 avg.loss:  3.306263 ETA:   0h 1m 1s Progress:   2.5% words/sec/thread:   11659 lr:  0.048739 avg.loss:  3.253101 ETA:   0h 1m 1s Progress:   2.7% words/sec/thread:   11657 lr:  0.048661 avg.loss:  3.208446 ETA:   0h 1m 1s Progress:   2.8% words/sec/thread:   11627 lr:  0.048586 avg.loss:  3.166108 ETA:   0h 1m 1s Progress:   3.0% words/sec/thread:   11636 lr:  0.048506 avg.loss:  3.118573 ETA:   0h 1m 1s Progress:   3.1% words/sec/thread:   11626 lr:  0.048429 avg.loss:  3.087531 ETA:   0h 1m 1s Progress:   3.3% words/sec/thread:   11638 lr:  0.048348 avg.loss:  3.059378 ETA:   0h 1m 1s Progress:   3.5% words/sec/thread:   11631 lr:  0.048271 avg.loss:  3.034608 ETA:   0h 1m 1s Progress:   3.6% words/sec/thread:   11637 lr:  0.048191 avg.loss:  3.011932 ETA:   0h 1m 1s Progress:   3.8% words/sec/thread:   11625 lr:  0.048114 avg.loss:  2.990335 ETA:   0h 1m 1s Progress:   3.9% words/sec/thread:   11623 lr:  0.048036 avg.loss:  2.970490 ETA:   0h 1m 1s Progress:   4.1% words/sec/thread:   11614 lr:  0.047959 avg.loss:  2.945493 ETA:   0h 1m 1s Progress:   4.2% words/sec/thread:   11588 lr:  0.047885 avg.loss:  2.927922 ETA:   0h 1m 1s Progress:   4.4% words/sec/thread:   11579 lr:  0.047809 avg.loss:  2.909702 ETA:   0h 1m 1s Progress:   4.5% words/sec/thread:   11580 lr:  0.047731 avg.loss:  2.896610 ETA:   0h 1m 1s Progress:   4.7% words/sec/thread:   11583 lr:  0.047651 avg.loss:  2.881911 ETA:   0h 1m 0s Progress:   4.9% words/sec/thread:   11589 lr:  0.047572 avg.loss:  2.867826 ETA:   0h 1m 0s Progress:   5.0% words/sec/thread:   11596 lr:  0.047492 avg.loss:  2.854898 ETA:   0h 1m 0s Progress:   5.2% words/sec/thread:   11588 lr:  0.047416 avg.loss:  2.843877 ETA:   0h 1m 0s Progress:   5.3% words/sec/thread:   11590 lr:  0.047337 avg.loss:  2.833630 ETA:   0h 1m 0s Progress:   5.5% words/sec/thread:   11583 lr:  0.047260 avg.loss:  2.823563 ETA:   0h 1m 0s Progress:   5.6% words/sec/thread:   11590 lr:  0.047180 avg.loss:  2.811770 ETA:   0h 1m 0s Progress:   5.8% words/sec/thread:   11591 lr:  0.047102 avg.loss:  2.802235 ETA:   0h 1m 0s Progress:   6.0% words/sec/thread:   11585 lr:  0.047025 avg.loss:  2.791227 ETA:   0h 1m 0s Progress:   6.1% words/sec/thread:   11589 lr:  0.046946 avg.loss:  2.783926 ETA:   0h 1m 0s Progress:   6.3% words/sec/thread:   11589 lr:  0.046867 avg.loss:  2.775412 ETA:   0h 0m59s Progress:   6.4% words/sec/thread:   11579 lr:  0.046791 avg.loss:  2.767698 ETA:   0h 0m59s Progress:   6.6% words/sec/thread:   11587 lr:  0.046712 avg.loss:  2.760190 ETA:   0h 0m59s Progress:   6.7% words/sec/thread:   11583 lr:  0.046634 avg.loss:  2.753663 ETA:   0h 0m59s Progress:   6.9% words/sec/thread:   11588 lr:  0.046554 avg.loss:  2.745933 ETA:   0h 0m59s Progress:   7.0% words/sec/thread:   11574 lr:  0.046480 avg.loss:  2.737643 ETA:   0h 0m59s Progress:   7.2% words/sec/thread:   11562 lr:  0.046405 avg.loss:  2.730896 ETA:   0h 0m59s Progress:   7.4% words/sec/thread:   11572 lr:  0.046324 avg.loss:  2.725098 ETA:   0h 0m59s Progress:   7.5% words/sec/thread:   11579 lr:  0.046244 avg.loss:  2.721658 ETA:   0h 0m59s Progress:   7.7% words/sec/thread:   11572 lr:  0.046168 avg.loss:  2.713965 ETA:   0h 0m59s Progress:   7.8% words/sec/thread:   11572 lr:  0.046090 avg.loss:  2.710169 ETA:   0h 0m59s Progress:   8.0% words/sec/thread:   11568 lr:  0.046013 avg.loss:  2.704687 ETA:   0h 0m58s Progress:   8.1% words/sec/thread:   11567 lr:  0.045936 avg.loss:  2.698222 ETA:   0h 0m58s Progress:   8.3% words/sec/thread:   11569 lr:  0.045856 avg.loss:  2.693997 ETA:   0h 0m58s Progress:   8.4% words/sec/thread:   11565 lr:  0.045779 avg.loss:  2.689467 ETA:   0h 0m58s Progress:   8.6% words/sec/thread:   11564 lr:  0.045701 avg.loss:  2.684975 ETA:   0h 0m58s Progress:   8.7% words/sec/thread:   11560 lr:  0.045625 avg.loss:  2.679621 ETA:   0h 0m58s Progress:   8.9% words/sec/thread:   11567 lr:  0.045544 avg.loss:  2.676542 ETA:   0h 0m58s Progress:   9.1% words/sec/thread:   11576 lr:  0.045462 avg.loss:  2.671997 ETA:   0h 0m58s Progress:   9.2% words/sec/thread:   11566 lr:  0.045388 avg.loss:  2.667341 ETA:   0h 0m58s Progress:   9.4% words/sec/thread:   11575 lr:  0.045306 avg.loss:  2.663849 ETA:   0h 0m57s Progress:   9.5% words/sec/thread:   11573 lr:  0.045229 avg.loss:  2.660522 ETA:   0h 0m57s Progress:   9.7% words/sec/thread:   11581 lr:  0.045148 avg.loss:  2.657433 ETA:   0h 0m57s Progress:   9.9% words/sec/thread:   11574 lr:  0.045072 avg.loss:  2.653862 ETA:   0h 0m57s Progress:  10.0% words/sec/thread:   11576 lr:  0.044993 avg.loss:  2.649561 ETA:   0h 0m57s Progress:  10.2% words/sec/thread:   11583 lr:  0.044912 avg.loss:  2.646841 ETA:   0h 0m57s Progress:  10.3% words/sec/thread:   11575 lr:  0.044837 avg.loss:  2.644192 ETA:   0h 0m57s Progress:  10.5% words/sec/thread:   11573 lr:  0.044760 avg.loss:  2.640606 ETA:   0h 0m57s Progress:  10.6% words/sec/thread:   11575 lr:  0.044680 avg.loss:  2.637727 ETA:   0h 0m57s Progress:  10.8% words/sec/thread:   11573 lr:  0.044603 avg.loss:  2.634834 ETA:   0h 0m57s Progress:  11.0% words/sec/thread:   11575 lr:  0.044524 avg.loss:  2.633169 ETA:   0h 0m56s Progress:  11.1% words/sec/thread:   11572 lr:  0.044447 avg.loss:  2.629989 ETA:   0h 0m56s Progress:  11.3% words/sec/thread:   11567 lr:  0.044371 avg.loss:  2.625414 ETA:   0h 0m56s Progress:  11.4% words/sec/thread:   11574 lr:  0.044290 avg.loss:  2.623259 ETA:   0h 0m56s Progress:  11.6% words/sec/thread:   11572 lr:  0.044213 avg.loss:  2.621012 ETA:   0h 0m56s Progress:  11.7% words/sec/thread:   11565 lr:  0.044138 avg.loss:  2.619160 ETA:   0h 0m56s Progress:  11.9% words/sec/thread:   11566 lr:  0.044059 avg.loss:  2.616668 ETA:   0h 0m56s Progress:  12.0% words/sec/thread:   11561 lr:  0.043980 avg.loss:  2.613875 ETA:   0h 0m56s Progress:  12.2% words/sec/thread:   11529 lr:  0.043919 avg.loss:  2.612866 ETA:   0h 0m56s Progress:  12.3% words/sec/thread:   11517 lr:  0.043847 avg.loss:  2.611102 ETA:   0h 0m56s Progress:  12.5% words/sec/thread:   11516 lr:  0.043769 avg.loss:  2.609565 ETA:   0h 0m56s Progress:  12.6% words/sec/thread:   11518 lr:  0.043691 avg.loss:  2.607714 ETA:   0h 0m56s Progress:  12.8% words/sec/thread:   11522 lr:  0.043610 avg.loss:  2.605772 ETA:   0h 0m56s Progress:  12.9% words/sec/thread:   11521 lr:  0.043534 avg.loss:  2.604091 ETA:   0h 0m55s Progress:  13.1% words/sec/thread:   11519 lr:  0.043457 avg.loss:  2.602036 ETA:   0h 0m55s Progress:  13.2% words/sec/thread:   11522 lr:  0.043377 avg.loss:  2.599936 ETA:   0h 0m55s Progress:  13.4% words/sec/thread:   11521 lr:  0.043300 avg.loss:  2.597664 ETA:   0h 0m55s Progress:  13.6% words/sec/thread:   11524 lr:  0.043220 avg.loss:  2.596417 ETA:   0h 0m55s Progress:  13.7% words/sec/thread:   11522 lr:  0.043143 avg.loss:  2.594425 ETA:   0h 0m55s Progress:  13.9% words/sec/thread:   11522 lr:  0.043066 avg.loss:  2.592960 ETA:   0h 0m55s Progress:  14.0% words/sec/thread:   11519 lr:  0.042990 avg.loss:  2.590933 ETA:   0h 0m55s Progress:  14.2% words/sec/thread:   11517 lr:  0.042913 avg.loss:  2.588586 ETA:   0h 0m55s Progress:  14.3% words/sec/thread:   11516 lr:  0.042836 avg.loss:  2.586833 ETA:   0h 0m55s Progress:  14.5% words/sec/thread:   11513 lr:  0.042760 avg.loss:  2.584612 ETA:   0h 0m55s Progress:  14.6% words/sec/thread:   11516 lr:  0.042680 avg.loss:  2.582687 ETA:   0h 0m54s Progress:  14.8% words/sec/thread:   11517 lr:  0.042602 avg.loss:  2.580929 ETA:   0h 0m54s Progress:  14.9% words/sec/thread:   11511 lr:  0.042528 avg.loss:  2.579537 ETA:   0h 0m54s Progress:  15.1% words/sec/thread:   11512 lr:  0.042449 avg.loss:  2.577600 ETA:   0h 0m54s Progress:  15.3% words/sec/thread:   11514 lr:  0.042370 avg.loss:  2.576024 ETA:   0h 0m54s Progress:  15.4% words/sec/thread:   11514 lr:  0.042292 avg.loss:  2.574608 ETA:   0h 0m54s Progress:  15.6% words/sec/thread:   11515 lr:  0.042214 avg.loss:  2.573080 ETA:   0h 0m54s Progress:  15.7% words/sec/thread:   11511 lr:  0.042138 avg.loss:  2.570956 ETA:   0h 0m54s Progress:  15.9% words/sec/thread:   11510 lr:  0.042061 avg.loss:  2.569218 ETA:   0h 0m54s Progress:  16.0% words/sec/thread:   11510 lr:  0.041984 avg.loss:  2.567899 ETA:   0h 0m54s Progress:  16.2% words/sec/thread:   11510 lr:  0.041906 avg.loss:  2.566482 ETA:   0h 0m53s Progress:  16.3% words/sec/thread:   11506 lr:  0.041831 avg.loss:  2.564558 ETA:   0h 0m53s Progress:  16.5% words/sec/thread:   11504 lr:  0.041755 avg.loss:  2.562604 ETA:   0h 0m53s Progress:  16.6% words/sec/thread:   11502 lr:  0.041678 avg.loss:  2.561034 ETA:   0h 0m53s Progress:  16.8% words/sec/thread:   11503 lr:  0.041600 avg.loss:  2.559725 ETA:   0h 0m53s Progress:  17.0% words/sec/thread:   11508 lr:  0.041519 avg.loss:  2.558986 ETA:   0h 0m53s Progress:  17.1% words/sec/thread:   11504 lr:  0.041443 avg.loss:  2.557194 ETA:   0h 0m53s Progress:  17.3% words/sec/thread:   11506 lr:  0.041364 avg.loss:  2.555626 ETA:   0h 0m53s Progress:  17.4% words/sec/thread:   11505 lr:  0.041287 avg.loss:  2.554334 ETA:   0h 0m53s Progress:  17.6% words/sec/thread:   11503 lr:  0.041211 avg.loss:  2.551281 ETA:   0h 0m53s Progress:  17.7% words/sec/thread:   11502 lr:  0.041134 avg.loss:  2.550200 ETA:   0h 0m52s Progress:  17.9% words/sec/thread:   11505 lr:  0.041054 avg.loss:  2.549217 ETA:   0h 0m52s Progress:  18.1% words/sec/thread:   11507 lr:  0.040975 avg.loss:  2.547161 ETA:   0h 0m52s Progress:  18.4% words/sec/thread:   11492 lr:  0.040786 avg.loss:  2.544173 ETA:   0h 0m52s Progress:  18.6% words/sec/thread:   11489 lr:  0.040711 avg.loss:  2.542242 ETA:   0h 0m52s Progress:  18.7% words/sec/thread:   11486 lr:  0.040635 avg.loss:  2.541165 ETA:   0h 0m52s Progress:  18.9% words/sec/thread:   11489 lr:  0.040556 avg.loss:  2.540242 ETA:   0h 0m52s Progress:  19.0% words/sec/thread:   11492 lr:  0.040475 avg.loss:  2.538961 ETA:   0h 0m52s Progress:  19.2% words/sec/thread:   11494 lr:  0.040396 avg.loss:  2.537195 ETA:   0h 0m52s Progress:  19.4% words/sec/thread:   11494 lr:  0.040318 avg.loss:  2.535961 ETA:   0h 0m51s Progress:  19.5% words/sec/thread:   11496 lr:  0.040240 avg.loss:  2.534339 ETA:   0h 0m51s Progress:  19.7% words/sec/thread:   11493 lr:  0.040163 avg.loss:  2.533398 ETA:   0h 0m51s Progress:  19.8% words/sec/thread:   11495 lr:  0.040085 avg.loss:  2.532504 ETA:   0h 0m51s Progress:  20.0% words/sec/thread:   11496 lr:  0.040006 avg.loss:  2.531317 ETA:   0h 0m51s Progress:  21.8% words/sec/thread:   11381 lr:  0.039085 avg.loss:  2.518485 ETA:   0h 0m50s Progress:  22.0% words/sec/thread:   11387 lr:  0.039003 avg.loss:  2.517434 ETA:   0h 0m50s Progress:  22.1% words/sec/thread:   11388 lr:  0.038925 avg.loss:  2.516048 ETA:   0h 0m50s Progress:  22.3% words/sec/thread:   11391 lr:  0.038845 avg.loss:  2.514809 ETA:   0h 0m50s Progress:  22.5% words/sec/thread:   11390 lr:  0.038768 avg.loss:  2.514081 ETA:   0h 0m50s Progress:  22.6% words/sec/thread:   11392 lr:  0.038690 avg.loss:  2.513111 ETA:   0h 0m50s Progress:  22.8% words/sec/thread:   11393 lr:  0.038612 avg.loss:  2.512059 ETA:   0h 0m50s Progress:  22.9% words/sec/thread:   11398 lr:  0.038530 avg.loss:  2.510759 ETA:   0h 0m50s Progress:  23.1% words/sec/thread:   11398 lr:  0.038453 avg.loss:  2.509505 ETA:   0h 0m49s Progress:  23.3% words/sec/thread:   11400 lr:  0.038373 avg.loss:  2.508215 ETA:   0h 0m49s Progress:  23.4% words/sec/thread:   11401 lr:  0.038295 avg.loss:  2.507177 ETA:   0h 0m49s Progress:  23.5% words/sec/thread:   11390 lr:  0.038229 avg.loss:  2.506776 ETA:   0h 0m49s Progress:  23.7% words/sec/thread:   11391 lr:  0.038152 avg.loss:  2.505282 ETA:   0h 0m49s Progress:  23.8% words/sec/thread:   11390 lr:  0.038075 avg.loss:  2.504658 ETA:   0h 0m49s Progress:  24.0% words/sec/thread:   11389 lr:  0.038000 avg.loss:  2.503528 ETA:   0h 0m49s Progress:  24.2% words/sec/thread:   11391 lr:  0.037921 avg.loss:  2.502495 ETA:   0h 0m49s Progress:  24.3% words/sec/thread:   11391 lr:  0.037844 avg.loss:  2.502014 ETA:   0h 0m49s Progress:  24.5% words/sec/thread:   11393 lr:  0.037765 avg.loss:  2.500848 ETA:   0h 0m49s Progress:  24.6% words/sec/thread:   11395 lr:  0.037685 avg.loss:  2.499763 ETA:   0h 0m48s Progress:  24.8% words/sec/thread:   11396 lr:  0.037607 avg.loss:  2.498544 ETA:   0h 0m48s Progress:  24.9% words/sec/thread:   11397 lr:  0.037529 avg.loss:  2.497637 ETA:   0h 0m48s Progress:  25.1% words/sec/thread:   11401 lr:  0.037448 avg.loss:  2.496254 ETA:   0h 0m48s Progress:  25.3% words/sec/thread:   11399 lr:  0.037373 avg.loss:  2.495643 ETA:   0h 0m48s Progress:  25.4% words/sec/thread:   11400 lr:  0.037295 avg.loss:  2.494631 ETA:   0h 0m48s Progress:  25.6% words/sec/thread:   11404 lr:  0.037212 avg.loss:  2.494057 ETA:   0h 0m48s Progress:  25.7% words/sec/thread:   11404 lr:  0.037135 avg.loss:  2.493005 ETA:   0h 0m48s Progress:  25.9% words/sec/thread:   11408 lr:  0.037054 avg.loss:  2.492308 ETA:   0h 0m48s Progress:  26.1% words/sec/thread:   11411 lr:  0.036973 avg.loss:  2.491534 ETA:   0h 0m48s Progress:  26.2% words/sec/thread:   11413 lr:  0.036893 avg.loss:  2.490446 ETA:   0h 0m47s Progress:  26.4% words/sec/thread:   11414 lr:  0.036815 avg.loss:  2.489607 ETA:   0h 0m47s Progress:  26.5% words/sec/thread:   11415 lr:  0.036737 avg.loss:  2.488487 ETA:   0h 0m47s Progress:  26.7% words/sec/thread:   11416 lr:  0.036659 avg.loss:  2.487348 ETA:   0h 0m47s Progress:  26.8% words/sec/thread:   11417 lr:  0.036580 avg.loss:  2.486089 ETA:   0h 0m47s Progress:  27.0% words/sec/thread:   11418 lr:  0.036502 avg.loss:  2.484818 ETA:   0h 0m47s Progress:  27.2% words/sec/thread:   11421 lr:  0.036421 avg.loss:  2.484110 ETA:   0h 0m47s Progress:  27.3% words/sec/thread:   11425 lr:  0.036339 avg.loss:  2.482961 ETA:   0h 0m47s Progress:  27.5% words/sec/thread:   11427 lr:  0.036260 avg.loss:  2.481792 ETA:   0h 0m47s Progress:  27.6% words/sec/thread:   11427 lr:  0.036183 avg.loss:  2.481001 ETA:   0h 0m46s Progress:  27.8% words/sec/thread:   11430 lr:  0.036102 avg.loss:  2.479758 ETA:   0h 0m46s Progress:  28.0% words/sec/thread:   11430 lr:  0.036024 avg.loss:  2.479162 ETA:   0h 0m46s Progress:  28.1% words/sec/thread:   11432 lr:  0.035944 avg.loss:  2.477921 ETA:   0h 0m46s Progress:  28.3% words/sec/thread:   11434 lr:  0.035864 avg.loss:  2.477212 ETA:   0h 0m46s Progress:  28.4% words/sec/thread:   11439 lr:  0.035781 avg.loss:  2.476258 ETA:   0h 0m46s Progress:  28.6% words/sec/thread:   11440 lr:  0.035702 avg.loss:  2.475415 ETA:   0h 0m46s Progress:  28.8% words/sec/thread:   11442 lr:  0.035623 avg.loss:  2.474522 ETA:   0h 0m46s Progress:  28.9% words/sec/thread:   11442 lr:  0.035545 avg.loss:  2.473518 ETA:   0h 0m46s Progress:  29.1% words/sec/thread:   11443 lr:  0.035466 avg.loss:  2.472501 ETA:   0h 0m45s Progress:  29.2% words/sec/thread:   11445 lr:  0.035387 avg.loss:  2.471440 ETA:   0h 0m45s Progress:  29.4% words/sec/thread:   11445 lr:  0.035308 avg.loss:  2.470673 ETA:   0h 0m45s Progress:  29.5% words/sec/thread:   11446 lr:  0.035229 avg.loss:  2.469744 ETA:   0h 0m45s Progress:  29.7% words/sec/thread:   11449 lr:  0.035148 avg.loss:  2.469081 ETA:   0h 0m45s Progress:  29.9% words/sec/thread:   11449 lr:  0.035070 avg.loss:  2.468684 ETA:   0h 0m45s Progress:  30.0% words/sec/thread:   11452 lr:  0.034990 avg.loss:  2.467964 ETA:   0h 0m45s Progress:  30.2% words/sec/thread:   11452 lr:  0.034912 avg.loss:  2.466761 ETA:   0h 0m45s Progress:  30.3% words/sec/thread:   11454 lr:  0.034831 avg.loss:  2.465929 ETA:   0h 0m45s Progress:  30.5% words/sec/thread:   11455 lr:  0.034752 avg.loss:  2.464872 ETA:   0h 0m44s Progress:  30.6% words/sec/thread:   11453 lr:  0.034678 avg.loss:  2.464131 ETA:   0h 0m44s Progress:  30.8% words/sec/thread:   11456 lr:  0.034596 avg.loss:  2.463301 ETA:   0h 0m44s Progress:  31.0% words/sec/thread:   11455 lr:  0.034519 avg.loss:  2.462257 ETA:   0h 0m44s Progress:  31.1% words/sec/thread:   11455 lr:  0.034442 avg.loss:  2.461333 ETA:   0h 0m44s Progress:  31.3% words/sec/thread:   11457 lr:  0.034362 avg.loss:  2.460595 ETA:   0h 0m44s Progress:  31.4% words/sec/thread:   11459 lr:  0.034281 avg.loss:  2.459941 ETA:   0h 0m44s Progress:  31.6% words/sec/thread:   11461 lr:  0.034201 avg.loss:  2.459041 ETA:   0h 0m44s Progress:  31.8% words/sec/thread:   11462 lr:  0.034122 avg.loss:  2.458599 ETA:   0h 0m44s Progress:  31.9% words/sec/thread:   11464 lr:  0.034042 avg.loss:  2.457657 ETA:   0h 0m43s Progress:  32.1% words/sec/thread:   11465 lr:  0.033962 avg.loss:  2.456807 ETA:   0h 0m43s Progress:  32.2% words/sec/thread:   11462 lr:  0.033889 avg.loss:  2.456322 ETA:   0h 0m43s Progress:  32.4% words/sec/thread:   11464 lr:  0.033808 avg.loss:  2.455807 ETA:   0h 0m43s Progress:  32.5% words/sec/thread:   11464 lr:  0.033731 avg.loss:  2.455275 ETA:   0h 0m43s Progress:  32.7% words/sec/thread:   11464 lr:  0.033652 avg.loss:  2.454418 ETA:   0h 0m43s Progress:  32.9% words/sec/thread:   11465 lr:  0.033574 avg.loss:  2.453871 ETA:   0h 0m43s Progress:  33.0% words/sec/thread:   11463 lr:  0.033499 avg.loss:  2.453097 ETA:   0h 0m43s Progress:  33.2% words/sec/thread:   11464 lr:  0.033420 avg.loss:  2.452219 ETA:   0h 0m43s Progress:  33.3% words/sec/thread:   11467 lr:  0.033338 avg.loss:  2.451510 ETA:   0h 0m43s Progress:  33.5% words/sec/thread:   11470 lr:  0.033256 avg.loss:  2.450549 ETA:   0h 0m42s Progress:  33.6% words/sec/thread:   11471 lr:  0.033177 avg.loss:  2.449799 ETA:   0h 0m42s Progress:  33.8% words/sec/thread:   11472 lr:  0.033098 avg.loss:  2.449195 ETA:   0h 0m42s Progress:  34.0% words/sec/thread:   11473 lr:  0.033018 avg.loss:  2.448474 ETA:   0h 0m42s Progress:  34.1% words/sec/thread:   11475 lr:  0.032937 avg.loss:  2.447928 ETA:   0h 0m42s Progress:  34.3% words/sec/thread:   11477 lr:  0.032856 avg.loss:  2.447228 ETA:   0h 0m42s Progress:  34.4% words/sec/thread:   11478 lr:  0.032778 avg.loss:  2.446412 ETA:   0h 0m42s Progress:  34.6% words/sec/thread:   11477 lr:  0.032701 avg.loss:  2.445784 ETA:   0h 0m42s Progress:  34.8% words/sec/thread:   11480 lr:  0.032619 avg.loss:  2.445007 ETA:   0h 0m42s Progress:  34.9% words/sec/thread:   11481 lr:  0.032540 avg.loss:  2.444307 ETA:   0h 0m41s Progress:  35.1% words/sec/thread:   11482 lr:  0.032460 avg.loss:  2.443752 ETA:   0h 0m41s Progress:  35.2% words/sec/thread:   11483 lr:  0.032382 avg.loss:  2.443132 ETA:   0h 0m41s Progress:  35.4% words/sec/thread:   11484 lr:  0.032302 avg.loss:  2.442705 ETA:   0h 0m41s Progress:  35.6% words/sec/thread:   11482 lr:  0.032211 avg.loss:  2.441689 ETA:   0h 0m41s Progress:  35.7% words/sec/thread:   11484 lr:  0.032130 avg.loss:  2.440654 ETA:   0h 0m41s Progress:  35.9% words/sec/thread:   11486 lr:  0.032050 avg.loss:  2.440177 ETA:   0h 0m41s Progress:  36.1% words/sec/thread:   11487 lr:  0.031971 avg.loss:  2.439480 ETA:   0h 0m41s Progress:  36.2% words/sec/thread:   11487 lr:  0.031893 avg.loss:  2.438387 ETA:   0h 0m41s Progress:  36.4% words/sec/thread:   11490 lr:  0.031811 avg.loss:  2.437649 ETA:   0h 0m41s Progress:  36.5% words/sec/thread:   11490 lr:  0.031733 avg.loss:  2.436981 ETA:   0h 0m40s Progress:  36.7% words/sec/thread:   11490 lr:  0.031654 avg.loss:  2.436521 ETA:   0h 0m40s Progress:  36.9% words/sec/thread:   11492 lr:  0.031574 avg.loss:  2.435729 ETA:   0h 0m40s Progress:  37.0% words/sec/thread:   11495 lr:  0.031491 avg.loss:  2.434860 ETA:   0h 0m40s Progress:  37.2% words/sec/thread:   11497 lr:  0.031411 avg.loss:  2.434138 ETA:   0h 0m40s Progress:  37.3% words/sec/thread:   11497 lr:  0.031333 avg.loss:  2.433664 ETA:   0h 0m40s Progress:  37.5% words/sec/thread:   11498 lr:  0.031253 avg.loss:  2.432875 ETA:   0h 0m40s Progress:  37.7% words/sec/thread:   11498 lr:  0.031175 avg.loss:  2.432312 ETA:   0h 0m40s Progress:  37.8% words/sec/thread:   11501 lr:  0.031092 avg.loss:  2.431886 ETA:   0h 0m40s Progress:  38.0% words/sec/thread:   11502 lr:  0.031014 avg.loss:  2.431398 ETA:   0h 0m39s Progress:  38.1% words/sec/thread:   11503 lr:  0.030933 avg.loss:  2.430700 ETA:   0h 0m39s Progress:  38.3% words/sec/thread:   11503 lr:  0.030856 avg.loss:  2.429831 ETA:   0h 0m39s Progress:  38.4% words/sec/thread:   11503 lr:  0.030779 avg.loss:  2.428777 ETA:   0h 0m39s Progress:  38.6% words/sec/thread:   11504 lr:  0.030699 avg.loss:  2.427797 ETA:   0h 0m39s Progress:  38.8% words/sec/thread:   11505 lr:  0.030619 avg.loss:  2.426813 ETA:   0h 0m39s Progress:  38.9% words/sec/thread:   11506 lr:  0.030540 avg.loss:  2.425960 ETA:   0h 0m39s Progress:  39.1% words/sec/thread:   11506 lr:  0.030461 avg.loss:  2.425604 ETA:   0h 0m39s Progress:  39.2% words/sec/thread:   11507 lr:  0.030382 avg.loss:  2.424722 ETA:   0h 0m39s Progress:  39.4% words/sec/thread:   11510 lr:  0.030299 avg.loss:  2.423723 ETA:   0h 0m38s Progress:  39.6% words/sec/thread:   11509 lr:  0.030223 avg.loss:  2.422964 ETA:   0h 0m38s Progress:  39.7% words/sec/thread:   11510 lr:  0.030145 avg.loss:  2.422664 ETA:   0h 0m38s Progress:  39.9% words/sec/thread:   11510 lr:  0.030065 avg.loss:  2.422163 ETA:   0h 0m38s Progress:  40.0% words/sec/thread:   11512 lr:  0.029985 avg.loss:  2.421319 ETA:   0h 0m38s Progress:  40.2% words/sec/thread:   11512 lr:  0.029906 avg.loss:  2.420569 ETA:   0h 0m38s Progress:  40.3% words/sec/thread:   11514 lr:  0.029826 avg.loss:  2.419882 ETA:   0h 0m38s Progress:  40.5% words/sec/thread:   11516 lr:  0.029744 avg.loss:  2.419405 ETA:   0h 0m38s Progress:  40.7% words/sec/thread:   11517 lr:  0.029665 avg.loss:  2.418332 ETA:   0h 0m38s Progress:  40.8% words/sec/thread:   11518 lr:  0.029585 avg.loss:  2.417779 ETA:   0h 0m38s Progress:  41.0% words/sec/thread:   11519 lr:  0.029506 avg.loss:  2.417071 ETA:   0h 0m37s Progress:  41.2% words/sec/thread:   11521 lr:  0.029424 avg.loss:  2.416195 ETA:   0h 0m37s Progress:  41.3% words/sec/thread:   11522 lr:  0.029344 avg.loss:  2.415324 ETA:   0h 0m37s Progress:  41.5% words/sec/thread:   11525 lr:  0.029261 avg.loss:  2.414842 ETA:   0h 0m37s Progress:  41.6% words/sec/thread:   11525 lr:  0.029182 avg.loss:  2.414305 ETA:   0h 0m37s Progress:  41.8% words/sec/thread:   11526 lr:  0.029103 avg.loss:  2.413251 ETA:   0h 0m37s Progress:  41.9% words/sec/thread:   11525 lr:  0.029027 avg.loss:  2.412466 ETA:   0h 0m37s Progress:  42.1% words/sec/thread:   11527 lr:  0.028945 avg.loss:  2.411752 ETA:   0h 0m37s Progress:  42.3% words/sec/thread:   11528 lr:  0.028866 avg.loss:  2.411383 ETA:   0h 0m37s Progress:  42.4% words/sec/thread:   11528 lr:  0.028787 avg.loss:  2.410423 ETA:   0h 0m36s Progress:  42.6% words/sec/thread:   11528 lr:  0.028709 avg.loss:  2.409947 ETA:   0h 0m36s Progress:  42.7% words/sec/thread:   11530 lr:  0.028629 avg.loss:  2.409375 ETA:   0h 0m36s Progress:  42.9% words/sec/thread:   11531 lr:  0.028549 avg.loss:  2.409017 ETA:   0h 0m36s Progress:  43.1% words/sec/thread:   11532 lr:  0.028468 avg.loss:  2.408400 ETA:   0h 0m36s Progress:  43.2% words/sec/thread:   11533 lr:  0.028388 avg.loss:  2.407470 ETA:   0h 0m36s Progress:  43.4% words/sec/thread:   11534 lr:  0.028308 avg.loss:  2.406904 ETA:   0h 0m36s Progress:  43.5% words/sec/thread:   11534 lr:  0.028230 avg.loss:  2.406321 ETA:   0h 0m36s Progress:  43.7% words/sec/thread:   11535 lr:  0.028151 avg.loss:  2.405683 ETA:   0h 0m36s Progress:  43.9% words/sec/thread:   11537 lr:  0.028069 avg.loss:  2.405271 ETA:   0h 0m36s Progress:  44.0% words/sec/thread:   11539 lr:  0.027986 avg.loss:  2.404727 ETA:   0h 0m35s Progress:  44.2% words/sec/thread:   11539 lr:  0.027909 avg.loss:  2.404065 ETA:   0h 0m35s Progress:  44.3% words/sec/thread:   11542 lr:  0.027826 avg.loss:  2.403308 ETA:   0h 0m35s Progress:  44.5% words/sec/thread:   11543 lr:  0.027746 avg.loss:  2.402587 ETA:   0h 0m35s Progress:  44.7% words/sec/thread:   11543 lr:  0.027667 avg.loss:  2.402095 ETA:   0h 0m35s Progress:  44.8% words/sec/thread:   11545 lr:  0.027585 avg.loss:  2.401390 ETA:   0h 0m35s Progress:  45.0% words/sec/thread:   11545 lr:  0.027507 avg.loss:  2.400513 ETA:   0h 0m35s Progress:  45.2% words/sec/thread:   11543 lr:  0.027420 avg.loss:  2.399528 ETA:   0h 0m35s Progress:  45.3% words/sec/thread:   11544 lr:  0.027340 avg.loss:  2.398808 ETA:   0h 0m35s Progress:  45.5% words/sec/thread:   11542 lr:  0.027265 avg.loss:  2.398127 ETA:   0h 0m34s Progress:  45.6% words/sec/thread:   11544 lr:  0.027183 avg.loss:  2.397427 ETA:   0h 0m34s Progress:  45.8% words/sec/thread:   11546 lr:  0.027101 avg.loss:  2.396804 ETA:   0h 0m34s Progress:  46.0% words/sec/thread:   11546 lr:  0.027023 avg.loss:  2.396270 ETA:   0h 0m34s Progress:  46.1% words/sec/thread:   11547 lr:  0.026943 avg.loss:  2.395704 ETA:   0h 0m34s Progress:  46.3% words/sec/thread:   11548 lr:  0.026863 avg.loss:  2.395034 ETA:   0h 0m34s Progress:  46.4% words/sec/thread:   11549 lr:  0.026783 avg.loss:  2.394517 ETA:   0h 0m34s Progress:  46.6% words/sec/thread:   11551 lr:  0.026701 avg.loss:  2.393993 ETA:   0h 0m34s Progress:  46.8% words/sec/thread:   11551 lr:  0.026623 avg.loss:  2.393515 ETA:   0h 0m34s Progress:  46.9% words/sec/thread:   11548 lr:  0.026550 avg.loss:  2.393286 ETA:   0h 0m34s Progress:  47.1% words/sec/thread:   11551 lr:  0.026467 avg.loss:  2.392631 ETA:   0h 0m33s Progress:  47.2% words/sec/thread:   11550 lr:  0.026390 avg.loss:  2.392102 ETA:   0h 0m33s Progress:  47.4% words/sec/thread:   11550 lr:  0.026312 avg.loss:  2.391138 ETA:   0h 0m33s Progress:  47.5% words/sec/thread:   11550 lr:  0.026233 avg.loss:  2.390440 ETA:   0h 0m33s Progress:  47.7% words/sec/thread:   11550 lr:  0.026155 avg.loss:  2.389917 ETA:   0h 0m33s Progress:  47.9% words/sec/thread:   11552 lr:  0.026074 avg.loss:  2.389404 ETA:   0h 0m33s Progress:  48.0% words/sec/thread:   11551 lr:  0.025997 avg.loss:  2.388876 ETA:   0h 0m33s Progress:  48.2% words/sec/thread:   11551 lr:  0.025919 avg.loss:  2.388455 ETA:   0h 0m33s Progress:  48.3% words/sec/thread:   11552 lr:  0.025840 avg.loss:  2.387435 ETA:   0h 0m33s Progress:  48.5% words/sec/thread:   11553 lr:  0.025760 avg.loss:  2.386732 ETA:   0h 0m33s Progress:  48.6% words/sec/thread:   11552 lr:  0.025682 avg.loss:  2.386079 ETA:   0h 0m32s Progress:  48.8% words/sec/thread:   11553 lr:  0.025602 avg.loss:  2.385549 ETA:   0h 0m32s Progress:  49.0% words/sec/thread:   11554 lr:  0.025522 avg.loss:  2.384840 ETA:   0h 0m32s Progress:  49.1% words/sec/thread:   11555 lr:  0.025441 avg.loss:  2.384341 ETA:   0h 0m32s Progress:  49.3% words/sec/thread:   11556 lr:  0.025362 avg.loss:  2.383813 ETA:   0h 0m32s Progress:  49.4% words/sec/thread:   11557 lr:  0.025282 avg.loss:  2.382895 ETA:   0h 0m32s Progress:  49.6% words/sec/thread:   11556 lr:  0.025205 avg.loss:  2.382291 ETA:   0h 0m32s Progress:  49.8% words/sec/thread:   11559 lr:  0.025121 avg.loss:  2.381411 ETA:   0h 0m32s Progress:  49.9% words/sec/thread:   11559 lr:  0.025041 avg.loss:  2.380776 ETA:   0h 0m32s Progress:  50.1% words/sec/thread:   11561 lr:  0.024960 avg.loss:  2.380264 ETA:   0h 0m31s Progress:  50.2% words/sec/thread:   11560 lr:  0.024885 avg.loss:  2.379686 ETA:   0h 0m31s Progress:  50.4% words/sec/thread:   11555 lr:  0.024817 avg.loss:  2.378980 ETA:   0h 0m31s Progress:  50.5% words/sec/thread:   11551 lr:  0.024747 avg.loss:  2.378689 ETA:   0h 0m31s Progress:  50.6% words/sec/thread:   11546 lr:  0.024679 avg.loss:  2.378290 ETA:   0h 0m31s Progress:  50.8% words/sec/thread:   11545 lr:  0.024605 avg.loss:  2.377606 ETA:   0h 0m31s Progress:  50.9% words/sec/thread:   11545 lr:  0.024526 avg.loss:  2.377059 ETA:   0h 0m31s Progress:  51.1% words/sec/thread:   11545 lr:  0.024447 avg.loss:  2.376530 ETA:   0h 0m31s Progress:  51.3% words/sec/thread:   11546 lr:  0.024367 avg.loss:  2.376247 ETA:   0h 0m31s Progress:  51.4% words/sec/thread:   11545 lr:  0.024292 avg.loss:  2.375952 ETA:   0h 0m31s Progress:  51.6% words/sec/thread:   11546 lr:  0.024210 avg.loss:  2.375438 ETA:   0h 0m31s Progress:  51.7% words/sec/thread:   11547 lr:  0.024132 avg.loss:  2.374813 ETA:   0h 0m30s Progress:  51.9% words/sec/thread:   11547 lr:  0.024052 avg.loss:  2.374474 ETA:   0h 0m30s Progress:  52.1% words/sec/thread:   11547 lr:  0.023974 avg.loss:  2.373843 ETA:   0h 0m30s Progress:  52.2% words/sec/thread:   11548 lr:  0.023894 avg.loss:  2.373230 ETA:   0h 0m30s Progress:  52.4% words/sec/thread:   11547 lr:  0.023818 avg.loss:  2.372832 ETA:   0h 0m30s Progress:  52.5% words/sec/thread:   11548 lr:  0.023739 avg.loss:  2.372286 ETA:   0h 0m30s Progress:  52.7% words/sec/thread:   11547 lr:  0.023662 avg.loss:  2.371809 ETA:   0h 0m30s Progress:  52.8% words/sec/thread:   11548 lr:  0.023582 avg.loss:  2.371164 ETA:   0h 0m30s Progress:  53.0% words/sec/thread:   11548 lr:  0.023496 avg.loss:  2.370508 ETA:   0h 0m30s Progress:  53.2% words/sec/thread:   11549 lr:  0.023416 avg.loss:  2.369971 ETA:   0h 0m30s Progress:  53.3% words/sec/thread:   11549 lr:  0.023338 avg.loss:  2.369630 ETA:   0h 0m29s Progress:  53.5% words/sec/thread:   11549 lr:  0.023258 avg.loss:  2.368914 ETA:   0h 0m29s Progress:  53.6% words/sec/thread:   11549 lr:  0.023181 avg.loss:  2.368499 ETA:   0h 0m29s Progress:  53.8% words/sec/thread:   11550 lr:  0.023100 avg.loss:  2.367937 ETA:   0h 0m29s Progress:  54.0% words/sec/thread:   11551 lr:  0.023020 avg.loss:  2.367493 ETA:   0h 0m29s Progress:  54.1% words/sec/thread:   11552 lr:  0.022940 avg.loss:  2.366699 ETA:   0h 0m29s Progress:  54.3% words/sec/thread:   11552 lr:  0.022861 avg.loss:  2.366012 ETA:   0h 0m29s Progress:  54.4% words/sec/thread:   11552 lr:  0.022782 avg.loss:  2.365494 ETA:   0h 0m29s Progress:  54.6% words/sec/thread:   11552 lr:  0.022704 avg.loss:  2.365042 ETA:   0h 0m29s Progress:  54.7% words/sec/thread:   11552 lr:  0.022626 avg.loss:  2.364418 ETA:   0h 0m29s Progress:  54.9% words/sec/thread:   11552 lr:  0.022548 avg.loss:  2.363814 ETA:   0h 0m28s Progress:  55.1% words/sec/thread:   11553 lr:  0.022468 avg.loss:  2.363312 ETA:   0h 0m28s Progress:  55.2% words/sec/thread:   11552 lr:  0.022392 avg.loss:  2.363008 ETA:   0h 0m28s Progress:  55.4% words/sec/thread:   11552 lr:  0.022313 avg.loss:  2.362321 ETA:   0h 0m28s Progress:  55.5% words/sec/thread:   11552 lr:  0.022236 avg.loss:  2.361839 ETA:   0h 0m28s Progress:  55.7% words/sec/thread:   11552 lr:  0.022158 avg.loss:  2.361211 ETA:   0h 0m28s Progress:  55.8% words/sec/thread:   11552 lr:  0.022080 avg.loss:  2.360593 ETA:   0h 0m28s Progress:  56.0% words/sec/thread:   11552 lr:  0.022000 avg.loss:  2.360122 ETA:   0h 0m28s Progress:  56.2% words/sec/thread:   11553 lr:  0.021921 avg.loss:  2.359719 ETA:   0h 0m28s Progress:  56.3% words/sec/thread:   11553 lr:  0.021842 avg.loss:  2.359306 ETA:   0h 0m28s Progress:  56.5% words/sec/thread:   11553 lr:  0.021764 avg.loss:  2.358684 ETA:   0h 0m27s Progress:  56.6% words/sec/thread:   11553 lr:  0.021687 avg.loss:  2.358290 ETA:   0h 0m27s Progress:  56.8% words/sec/thread:   11553 lr:  0.021607 avg.loss:  2.357941 ETA:   0h 0m27s Progress:  56.9% words/sec/thread:   11554 lr:  0.021526 avg.loss:  2.357224 ETA:   0h 0m27s Progress:  57.1% words/sec/thread:   11554 lr:  0.021450 avg.loss:  2.356904 ETA:   0h 0m27s Progress:  57.3% words/sec/thread:   11554 lr:  0.021372 avg.loss:  2.356302 ETA:   0h 0m27s Progress:  57.4% words/sec/thread:   11553 lr:  0.021294 avg.loss:  2.355870 ETA:   0h 0m27s Progress:  57.6% words/sec/thread:   11554 lr:  0.021215 avg.loss:  2.355392 ETA:   0h 0m27s Progress:  57.7% words/sec/thread:   11553 lr:  0.021139 avg.loss:  2.354998 ETA:   0h 0m27s Progress:  57.9% words/sec/thread:   11554 lr:  0.021058 avg.loss:  2.354458 ETA:   0h 0m27s Progress:  58.0% words/sec/thread:   11555 lr:  0.020977 avg.loss:  2.353570 ETA:   0h 0m26s Progress:  58.2% words/sec/thread:   11555 lr:  0.020898 avg.loss:  2.352987 ETA:   0h 0m26s Progress:  58.4% words/sec/thread:   11556 lr:  0.020818 avg.loss:  2.352758 ETA:   0h 0m26s Progress:  58.5% words/sec/thread:   11558 lr:  0.020736 avg.loss:  2.352298 ETA:   0h 0m26s Progress:  58.7% words/sec/thread:   11557 lr:  0.020660 avg.loss:  2.351648 ETA:   0h 0m26s Progress:  58.8% words/sec/thread:   11558 lr:  0.020580 avg.loss:  2.350970 ETA:   0h 0m26s Progress:  59.0% words/sec/thread:   11558 lr:  0.020501 avg.loss:  2.350634 ETA:   0h 0m26s Progress:  59.2% words/sec/thread:   11558 lr:  0.020422 avg.loss:  2.350236 ETA:   0h 0m26s Progress:  59.3% words/sec/thread:   11559 lr:  0.020341 avg.loss:  2.349710 ETA:   0h 0m26s Progress:  59.5% words/sec/thread:   11560 lr:  0.020261 avg.loss:  2.349261 ETA:   0h 0m25s Progress:  59.6% words/sec/thread:   11559 lr:  0.020184 avg.loss:  2.348702 ETA:   0h 0m25s Progress:  59.8% words/sec/thread:   11560 lr:  0.020097 avg.loss:  2.348308 ETA:   0h 0m25s Progress:  60.0% words/sec/thread:   11561 lr:  0.020017 avg.loss:  2.347818 ETA:   0h 0m25s Progress:  60.1% words/sec/thread:   11563 lr:  0.019933 avg.loss:  2.347347 ETA:   0h 0m25s Progress:  60.3% words/sec/thread:   11564 lr:  0.019853 avg.loss:  2.347016 ETA:   0h 0m25s Progress:  60.5% words/sec/thread:   11564 lr:  0.019774 avg.loss:  2.346642 ETA:   0h 0m25s Progress:  60.6% words/sec/thread:   11563 lr:  0.019699 avg.loss:  2.346273 ETA:   0h 0m25s Progress:  60.8% words/sec/thread:   11564 lr:  0.019617 avg.loss:  2.345687 ETA:   0h 0m25s Progress:  60.9% words/sec/thread:   11564 lr:  0.019539 avg.loss:  2.345208 ETA:   0h 0m25s Progress:  61.1% words/sec/thread:   11564 lr:  0.019462 avg.loss:  2.344594 ETA:   0h 0m24s Progress:  61.2% words/sec/thread:   11563 lr:  0.019386 avg.loss:  2.344024 ETA:   0h 0m24s Progress:  61.4% words/sec/thread:   11564 lr:  0.019306 avg.loss:  2.343719 ETA:   0h 0m24s Progress:  61.5% words/sec/thread:   11563 lr:  0.019228 avg.loss:  2.343301 ETA:   0h 0m24s Progress:  61.7% words/sec/thread:   11564 lr:  0.019148 avg.loss:  2.342862 ETA:   0h 0m24s Progress:  61.9% words/sec/thread:   11565 lr:  0.019066 avg.loss:  2.342535 ETA:   0h 0m24s Progress:  62.0% words/sec/thread:   11566 lr:  0.018986 avg.loss:  2.341879 ETA:   0h 0m24s Progress:  62.2% words/sec/thread:   11565 lr:  0.018910 avg.loss:  2.341534 ETA:   0h 0m24s Progress:  62.3% words/sec/thread:   11565 lr:  0.018833 avg.loss:  2.341170 ETA:   0h 0m24s Progress:  62.5% words/sec/thread:   11566 lr:  0.018751 avg.loss:  2.340762 ETA:   0h 0m24s Progress:  62.7% words/sec/thread:   11568 lr:  0.018669 avg.loss:  2.340306 ETA:   0h 0m23s Progress:  62.8% words/sec/thread:   11568 lr:  0.018590 avg.loss:  2.339840 ETA:   0h 0m23s Progress:  63.0% words/sec/thread:   11568 lr:  0.018512 avg.loss:  2.339225 ETA:   0h 0m23s Progress:  63.1% words/sec/thread:   11567 lr:  0.018436 avg.loss:  2.338901 ETA:   0h 0m23s Progress:  63.3% words/sec/thread:   11568 lr:  0.018355 avg.loss:  2.338348 ETA:   0h 0m23s Progress:  63.4% words/sec/thread:   11568 lr:  0.018276 avg.loss:  2.337890 ETA:   0h 0m23s Progress:  63.6% words/sec/thread:   11568 lr:  0.018197 avg.loss:  2.337279 ETA:   0h 0m23s Progress:  63.8% words/sec/thread:   11568 lr:  0.018119 avg.loss:  2.336741 ETA:   0h 0m23s Progress:  63.9% words/sec/thread:   11569 lr:  0.018040 avg.loss:  2.336180 ETA:   0h 0m23s Progress:  64.1% words/sec/thread:   11569 lr:  0.017961 avg.loss:  2.335570 ETA:   0h 0m23s Progress:  64.2% words/sec/thread:   11569 lr:  0.017883 avg.loss:  2.335162 ETA:   0h 0m22s Progress:  64.4% words/sec/thread:   11569 lr:  0.017804 avg.loss:  2.334889 ETA:   0h 0m22s Progress:  64.5% words/sec/thread:   11569 lr:  0.017726 avg.loss:  2.334507 ETA:   0h 0m22s Progress:  64.7% words/sec/thread:   11568 lr:  0.017652 avg.loss:  2.333886 ETA:   0h 0m22s Progress:  64.8% words/sec/thread:   11567 lr:  0.017575 avg.loss:  2.333596 ETA:   0h 0m22s Progress:  65.0% words/sec/thread:   11568 lr:  0.017493 avg.loss:  2.332957 ETA:   0h 0m22s Progress:  65.2% words/sec/thread:   11569 lr:  0.017412 avg.loss:  2.332513 ETA:   0h 0m22s Progress:  65.3% words/sec/thread:   11569 lr:  0.017334 avg.loss:  2.332019 ETA:   0h 0m22s Progress:  65.5% words/sec/thread:   11570 lr:  0.017253 avg.loss:  2.331651 ETA:   0h 0m22s Progress:  65.7% words/sec/thread:   11571 lr:  0.017173 avg.loss:  2.331078 ETA:   0h 0m21s Progress:  65.8% words/sec/thread:   11568 lr:  0.017092 avg.loss:  2.330549 ETA:   0h 0m21s Progress:  66.0% words/sec/thread:   11568 lr:  0.017012 avg.loss:  2.329885 ETA:   0h 0m21s Progress:  66.1% words/sec/thread:   11568 lr:  0.016934 avg.loss:  2.329696 ETA:   0h 0m21s Progress:  66.3% words/sec/thread:   11569 lr:  0.016852 avg.loss:  2.329391 ETA:   0h 0m21s Progress:  66.5% words/sec/thread:   11569 lr:  0.016775 avg.loss:  2.328911 ETA:   0h 0m21s Progress:  66.6% words/sec/thread:   11571 lr:  0.016692 avg.loss:  2.328076 ETA:   0h 0m21s Progress:  66.8% words/sec/thread:   11572 lr:  0.016611 avg.loss:  2.327631 ETA:   0h 0m21s Progress:  66.9% words/sec/thread:   11570 lr:  0.016538 avg.loss:  2.327127 ETA:   0h 0m21s Progress:  67.1% words/sec/thread:   11571 lr:  0.016455 avg.loss:  2.326404 ETA:   0h 0m21s Progress:  67.2% words/sec/thread:   11572 lr:  0.016376 avg.loss:  2.326037 ETA:   0h 0m20s Progress:  67.4% words/sec/thread:   11572 lr:  0.016297 avg.loss:  2.325445 ETA:   0h 0m20s Progress:  67.6% words/sec/thread:   11573 lr:  0.016216 avg.loss:  2.325110 ETA:   0h 0m20s Progress:  67.7% words/sec/thread:   11574 lr:  0.016135 avg.loss:  2.324636 ETA:   0h 0m20s Progress:  67.9% words/sec/thread:   11574 lr:  0.016057 avg.loss:  2.324225 ETA:   0h 0m20s Progress:  68.0% words/sec/thread:   11573 lr:  0.015982 avg.loss:  2.323824 ETA:   0h 0m20s Progress:  68.2% words/sec/thread:   11573 lr:  0.015902 avg.loss:  2.323427 ETA:   0h 0m20s Progress:  68.4% words/sec/thread:   11573 lr:  0.015824 avg.loss:  2.322893 ETA:   0h 0m20s Progress:  68.5% words/sec/thread:   11574 lr:  0.015742 avg.loss:  2.322352 ETA:   0h 0m20s Progress:  68.7% words/sec/thread:   11575 lr:  0.015662 avg.loss:  2.321892 ETA:   0h 0m20s Progress:  68.8% words/sec/thread:   11575 lr:  0.015583 avg.loss:  2.321448 ETA:   0h 0m19s Progress:  69.0% words/sec/thread:   11577 lr:  0.015499 avg.loss:  2.321004 ETA:   0h 0m19s Progress:  69.2% words/sec/thread:   11577 lr:  0.015421 avg.loss:  2.320404 ETA:   0h 0m19s Progress:  69.3% words/sec/thread:   11579 lr:  0.015337 avg.loss:  2.319988 ETA:   0h 0m19s Progress:  69.5% words/sec/thread:   11577 lr:  0.015263 avg.loss:  2.319645 ETA:   0h 0m19s Progress:  69.6% words/sec/thread:   11577 lr:  0.015185 avg.loss:  2.319266 ETA:   0h 0m19s Progress:  69.8% words/sec/thread:   11578 lr:  0.015105 avg.loss:  2.318805 ETA:   0h 0m19s Progress:  69.9% words/sec/thread:   11577 lr:  0.015028 avg.loss:  2.318409 ETA:   0h 0m19s Progress:  70.1% words/sec/thread:   11578 lr:  0.014948 avg.loss:  2.317954 ETA:   0h 0m19s Progress:  70.3% words/sec/thread:   11579 lr:  0.014865 avg.loss:  2.317536 ETA:   0h 0m19s Progress:  70.4% words/sec/thread:   11579 lr:  0.014789 avg.loss:  2.317065 ETA:   0h 0m18s Progress:  70.6% words/sec/thread:   11580 lr:  0.014705 avg.loss:  2.316579 ETA:   0h 0m18s Progress:  70.7% words/sec/thread:   11580 lr:  0.014628 avg.loss:  2.316205 ETA:   0h 0m18s Progress:  70.9% words/sec/thread:   11580 lr:  0.014549 avg.loss:  2.315780 ETA:   0h 0m18s Progress:  71.1% words/sec/thread:   11580 lr:  0.014471 avg.loss:  2.315253 ETA:   0h 0m18s Progress:  71.2% words/sec/thread:   11580 lr:  0.014393 avg.loss:  2.314921 ETA:   0h 0m18s Progress:  71.4% words/sec/thread:   11580 lr:  0.014300 avg.loss:  2.314364 ETA:   0h 0m18s Progress:  71.6% words/sec/thread:   11580 lr:  0.014220 avg.loss:  2.314035 ETA:   0h 0m18s Progress:  71.7% words/sec/thread:   11580 lr:  0.014141 avg.loss:  2.313771 ETA:   0h 0m18s Progress:  71.9% words/sec/thread:   11580 lr:  0.014063 avg.loss:  2.313306 ETA:   0h 0m17s Progress:  72.0% words/sec/thread:   11582 lr:  0.013980 avg.loss:  2.312870 ETA:   0h 0m17s Progress:  72.2% words/sec/thread:   11583 lr:  0.013898 avg.loss:  2.312602 ETA:   0h 0m17s Progress:  72.4% words/sec/thread:   11582 lr:  0.013822 avg.loss:  2.312178 ETA:   0h 0m17s Progress:  72.5% words/sec/thread:   11583 lr:  0.013742 avg.loss:  2.311711 ETA:   0h 0m17s Progress:  72.7% words/sec/thread:   11583 lr:  0.013662 avg.loss:  2.311305 ETA:   0h 0m17s Progress:  72.8% words/sec/thread:   11583 lr:  0.013584 avg.loss:  2.310776 ETA:   0h 0m17s Progress:  73.0% words/sec/thread:   11584 lr:  0.013503 avg.loss:  2.310245 ETA:   0h 0m17s Progress:  73.2% words/sec/thread:   11584 lr:  0.013423 avg.loss:  2.309938 ETA:   0h 0m17s Progress:  73.3% words/sec/thread:   11584 lr:  0.013345 avg.loss:  2.309558 ETA:   0h 0m17s Progress:  73.5% words/sec/thread:   11583 lr:  0.013269 avg.loss:  2.309006 ETA:   0h 0m16s Progress:  73.6% words/sec/thread:   11584 lr:  0.013189 avg.loss:  2.308653 ETA:   0h 0m16s Progress:  73.8% words/sec/thread:   11583 lr:  0.013113 avg.loss:  2.308243 ETA:   0h 0m16s Progress:  73.9% words/sec/thread:   11583 lr:  0.013034 avg.loss:  2.307769 ETA:   0h 0m16s Progress:  74.1% words/sec/thread:   11584 lr:  0.012954 avg.loss:  2.307304 ETA:   0h 0m16s Progress:  74.3% words/sec/thread:   11584 lr:  0.012874 avg.loss:  2.306993 ETA:   0h 0m16s Progress:  74.4% words/sec/thread:   11585 lr:  0.012792 avg.loss:  2.306686 ETA:   0h 0m16s Progress:  74.6% words/sec/thread:   11585 lr:  0.012714 avg.loss:  2.306013 ETA:   0h 0m16s Progress:  74.7% words/sec/thread:   11586 lr:  0.012634 avg.loss:  2.305754 ETA:   0h 0m16s Progress:  74.9% words/sec/thread:   11587 lr:  0.012552 avg.loss:  2.305370 ETA:   0h 0m16s Progress:  75.1% words/sec/thread:   11587 lr:  0.012473 avg.loss:  2.305059 ETA:   0h 0m15s Progress:  75.2% words/sec/thread:   11587 lr:  0.012395 avg.loss:  2.304662 ETA:   0h 0m15s Progress:  75.4% words/sec/thread:   11587 lr:  0.012315 avg.loss:  2.304398 ETA:   0h 0m15s Progress:  75.5% words/sec/thread:   11587 lr:  0.012238 avg.loss:  2.303942 ETA:   0h 0m15s Progress:  75.7% words/sec/thread:   11587 lr:  0.012161 avg.loss:  2.303465 ETA:   0h 0m15s Progress:  75.8% words/sec/thread:   11587 lr:  0.012080 avg.loss:  2.302922 ETA:   0h 0m15s Progress:  76.0% words/sec/thread:   11588 lr:  0.012000 avg.loss:  2.302522 ETA:   0h 0m15s Progress:  76.2% words/sec/thread:   11588 lr:  0.011921 avg.loss:  2.302126 ETA:   0h 0m15s Progress:  76.3% words/sec/thread:   11588 lr:  0.011844 avg.loss:  2.301686 ETA:   0h 0m15s Progress:  76.5% words/sec/thread:   11587 lr:  0.011755 avg.loss:  2.301239 ETA:   0h 0m15s Progress:  76.6% words/sec/thread:   11587 lr:  0.011677 avg.loss:  2.300741 ETA:   0h 0m14s Progress:  76.8% words/sec/thread:   11587 lr:  0.011596 avg.loss:  2.300319 ETA:   0h 0m14s Progress:  77.0% words/sec/thread:   11588 lr:  0.011516 avg.loss:  2.299862 ETA:   0h 0m14s Progress:  77.1% words/sec/thread:   11588 lr:  0.011438 avg.loss:  2.299464 ETA:   0h 0m14s Progress:  77.3% words/sec/thread:   11588 lr:  0.011360 avg.loss:  2.299111 ETA:   0h 0m14s Progress:  77.4% words/sec/thread:   11588 lr:  0.011280 avg.loss:  2.298677 ETA:   0h 0m14s Progress:  77.6% words/sec/thread:   11589 lr:  0.011199 avg.loss:  2.298161 ETA:   0h 0m14s Progress:  77.8% words/sec/thread:   11588 lr:  0.011123 avg.loss:  2.297763 ETA:   0h 0m14s Progress:  77.9% words/sec/thread:   11588 lr:  0.011044 avg.loss:  2.297527 ETA:   0h 0m14s Progress:  78.1% words/sec/thread:   11589 lr:  0.010963 avg.loss:  2.297077 ETA:   0h 0m14s Progress:  78.2% words/sec/thread:   11588 lr:  0.010887 avg.loss:  2.296632 ETA:   0h 0m13s Progress:  78.4% words/sec/thread:   11589 lr:  0.010805 avg.loss:  2.296151 ETA:   0h 0m13s Progress:  78.5% words/sec/thread:   11589 lr:  0.010727 avg.loss:  2.295945 ETA:   0h 0m13s Progress:  78.7% words/sec/thread:   11591 lr:  0.010641 avg.loss:  2.295630 ETA:   0h 0m13s Progress:  78.9% words/sec/thread:   11591 lr:  0.010563 avg.loss:  2.295413 ETA:   0h 0m13s Progress:  79.0% words/sec/thread:   11592 lr:  0.010482 avg.loss:  2.294906 ETA:   0h 0m13s Progress:  79.2% words/sec/thread:   11592 lr:  0.010403 avg.loss:  2.294564 ETA:   0h 0m13s Progress:  79.3% words/sec/thread:   11592 lr:  0.010326 avg.loss:  2.294304 ETA:   0h 0m13s Progress:  79.5% words/sec/thread:   11593 lr:  0.010244 avg.loss:  2.293815 ETA:   0h 0m13s Progress:  79.7% words/sec/thread:   11593 lr:  0.010166 avg.loss:  2.293314 ETA:   0h 0m12s Progress:  79.8% words/sec/thread:   11593 lr:  0.010088 avg.loss:  2.292934 ETA:   0h 0m12s Progress:  80.0% words/sec/thread:   11593 lr:  0.010009 avg.loss:  2.292515 ETA:   0h 0m12s Progress:  80.1% words/sec/thread:   11592 lr:  0.009932 avg.loss:  2.292146 ETA:   0h 0m12s Progress:  80.3% words/sec/thread:   11593 lr:  0.009851 avg.loss:  2.291735 ETA:   0h 0m12s Progress:  80.5% words/sec/thread:   11593 lr:  0.009772 avg.loss:  2.291544 ETA:   0h 0m12s Progress:  80.6% words/sec/thread:   11594 lr:  0.009690 avg.loss:  2.291088 ETA:   0h 0m12s Progress:  80.8% words/sec/thread:   11594 lr:  0.009611 avg.loss:  2.290708 ETA:   0h 0m12s Progress:  80.9% words/sec/thread:   11595 lr:  0.009531 avg.loss:  2.290408 ETA:   0h 0m12s Progress:  81.1% words/sec/thread:   11596 lr:  0.009450 avg.loss:  2.290083 ETA:   0h 0m12s Progress:  81.3% words/sec/thread:   11594 lr:  0.009369 avg.loss:  2.289671 ETA:   0h 0m11s Progress:  81.4% words/sec/thread:   11594 lr:  0.009291 avg.loss:  2.289238 ETA:   0h 0m11s Progress:  81.6% words/sec/thread:   11595 lr:  0.009210 avg.loss:  2.288838 ETA:   0h 0m11s Progress:  81.7% words/sec/thread:   11595 lr:  0.009130 avg.loss:  2.288366 ETA:   0h 0m11s Progress:  81.9% words/sec/thread:   11595 lr:  0.009051 avg.loss:  2.287931 ETA:   0h 0m11s Progress:  82.1% words/sec/thread:   11595 lr:  0.008972 avg.loss:  2.287466 ETA:   0h 0m11s Progress:  82.2% words/sec/thread:   11595 lr:  0.008896 avg.loss:  2.287117 ETA:   0h 0m11s Progress:  82.4% words/sec/thread:   11595 lr:  0.008817 avg.loss:  2.286718 ETA:   0h 0m11s Progress:  82.5% words/sec/thread:   11595 lr:  0.008738 avg.loss:  2.286431 ETA:   0h 0m11s Progress:  82.7% words/sec/thread:   11595 lr:  0.008660 avg.loss:  2.286007 ETA:   0h 0m11s Progress:  82.8% words/sec/thread:   11595 lr:  0.008580 avg.loss:  2.285598 ETA:   0h 0m10s Progress:  83.0% words/sec/thread:   11595 lr:  0.008502 avg.loss:  2.285226 ETA:   0h 0m10s Progress:  83.2% words/sec/thread:   11596 lr:  0.008422 avg.loss:  2.284972 ETA:   0h 0m10s Progress:  83.3% words/sec/thread:   11595 lr:  0.008345 avg.loss:  2.284566 ETA:   0h 0m10s Progress:  83.5% words/sec/thread:   11596 lr:  0.008264 avg.loss:  2.284107 ETA:   0h 0m10s Progress:  83.6% words/sec/thread:   11595 lr:  0.008188 avg.loss:  2.283792 ETA:   0h 0m10s Progress:  83.8% words/sec/thread:   11595 lr:  0.008110 avg.loss:  2.283406 ETA:   0h 0m10s Progress:  83.9% words/sec/thread:   11596 lr:  0.008027 avg.loss:  2.283090 ETA:   0h 0m10s Progress:  84.1% words/sec/thread:   11596 lr:  0.007949 avg.loss:  2.282608 ETA:   0h 0m10s Progress:  84.2% words/sec/thread:   11595 lr:  0.007876 avg.loss:  2.282284 ETA:   0h 0m10s Progress:  84.4% words/sec/thread:   11595 lr:  0.007796 avg.loss:  2.281952 ETA:   0h 0m 9s Progress:  84.6% words/sec/thread:   11595 lr:  0.007717 avg.loss:  2.281560 ETA:   0h 0m 9s Progress:  84.7% words/sec/thread:   11596 lr:  0.007637 avg.loss:  2.281269 ETA:   0h 0m 9s Progress:  84.9% words/sec/thread:   11596 lr:  0.007558 avg.loss:  2.280942 ETA:   0h 0m 9s Progress:  85.0% words/sec/thread:   11597 lr:  0.007477 avg.loss:  2.280576 ETA:   0h 0m 9s Progress:  85.2% words/sec/thread:   11597 lr:  0.007398 avg.loss:  2.280189 ETA:   0h 0m 9s Progress:  85.4% words/sec/thread:   11597 lr:  0.007317 avg.loss:  2.279715 ETA:   0h 0m 9s Progress:  85.5% words/sec/thread:   11597 lr:  0.007240 avg.loss:  2.279304 ETA:   0h 0m 9s Progress:  85.7% words/sec/thread:   11597 lr:  0.007162 avg.loss:  2.278968 ETA:   0h 0m 9s Progress:  85.9% words/sec/thread:   11597 lr:  0.007075 avg.loss:  2.278499 ETA:   0h 0m 9s Progress:  86.0% words/sec/thread:   11596 lr:  0.006998 avg.loss:  2.278084 ETA:   0h 0m 8s Progress:  86.2% words/sec/thread:   11596 lr:  0.006919 avg.loss:  2.277604 ETA:   0h 0m 8s Progress:  86.3% words/sec/thread:   11597 lr:  0.006836 avg.loss:  2.277396 ETA:   0h 0m 8s Progress:  86.5% words/sec/thread:   11598 lr:  0.006756 avg.loss:  2.276895 ETA:   0h 0m 8s Progress:  86.6% words/sec/thread:   11598 lr:  0.006677 avg.loss:  2.276423 ETA:   0h 0m 8s Progress:  86.8% words/sec/thread:   11598 lr:  0.006599 avg.loss:  2.276054 ETA:   0h 0m 8s Progress:  87.0% words/sec/thread:   11599 lr:  0.006518 avg.loss:  2.275730 ETA:   0h 0m 8s Progress:  87.1% words/sec/thread:   11598 lr:  0.006443 avg.loss:  2.275464 ETA:   0h 0m 8s Progress:  87.3% words/sec/thread:   11599 lr:  0.006361 avg.loss:  2.275070 ETA:   0h 0m 8s Progress:  87.4% words/sec/thread:   11598 lr:  0.006287 avg.loss:  2.274756 ETA:   0h 0m 8s Progress:  87.6% words/sec/thread:   11597 lr:  0.006209 avg.loss:  2.274432 ETA:   0h 0m 7s Progress:  87.7% words/sec/thread:   11597 lr:  0.006131 avg.loss:  2.274046 ETA:   0h 0m 7s Progress:  87.9% words/sec/thread:   11597 lr:  0.006052 avg.loss:  2.273689 ETA:   0h 0m 7s Progress:  88.1% words/sec/thread:   11598 lr:  0.005972 avg.loss:  2.273410 ETA:   0h 0m 7s Progress:  88.2% words/sec/thread:   11597 lr:  0.005896 avg.loss:  2.273124 ETA:   0h 0m 7s Progress:  88.4% words/sec/thread:   11598 lr:  0.005815 avg.loss:  2.272893 ETA:   0h 0m 7s Progress:  88.5% words/sec/thread:   11599 lr:  0.005732 avg.loss:  2.272524 ETA:   0h 0m 7s Progress:  88.7% words/sec/thread:   11600 lr:  0.005651 avg.loss:  2.272052 ETA:   0h 0m 7s Progress:  88.9% words/sec/thread:   11599 lr:  0.005572 avg.loss:  2.271700 ETA:   0h 0m 7s Progress:  89.0% words/sec/thread:   11599 lr:  0.005495 avg.loss:  2.271429 ETA:   0h 0m 7s Progress:  89.2% words/sec/thread:   11600 lr:  0.005414 avg.loss:  2.271103 ETA:   0h 0m 6s Progress:  89.3% words/sec/thread:   11600 lr:  0.005336 avg.loss:  2.270698 ETA:   0h 0m 6s Progress:  89.5% words/sec/thread:   11600 lr:  0.005258 avg.loss:  2.270390 ETA:   0h 0m 6s Progress:  89.6% words/sec/thread:   11599 lr:  0.005182 avg.loss:  2.269919 ETA:   0h 0m 6s Progress:  89.8% words/sec/thread:   11599 lr:  0.005103 avg.loss:  2.269626 ETA:   0h 0m 6s Progress:  90.0% words/sec/thread:   11599 lr:  0.005023 avg.loss:  2.269247 ETA:   0h 0m 6s Progress:  90.1% words/sec/thread:   11599 lr:  0.004928 avg.loss:  2.268721 ETA:   0h 0m 6s Progress:  90.3% words/sec/thread:   11599 lr:  0.004849 avg.loss:  2.268444 ETA:   0h 0m 6s Progress:  90.5% words/sec/thread:   11599 lr:  0.004770 avg.loss:  2.268111 ETA:   0h 0m 6s Progress:  90.6% words/sec/thread:   11599 lr:  0.004692 avg.loss:  2.267852 ETA:   0h 0m 5s Progress:  90.8% words/sec/thread:   11598 lr:  0.004617 avg.loss:  2.267497 ETA:   0h 0m 5s Progress:  90.9% words/sec/thread:   11599 lr:  0.004533 avg.loss:  2.267161 ETA:   0h 0m 5s Progress:  91.1% words/sec/thread:   11599 lr:  0.004454 avg.loss:  2.266712 ETA:   0h 0m 5s Progress:  91.3% words/sec/thread:   11600 lr:  0.004374 avg.loss:  2.266514 ETA:   0h 0m 5s Progress:  91.4% words/sec/thread:   11600 lr:  0.004293 avg.loss:  2.266273 ETA:   0h 0m 5s Progress:  91.6% words/sec/thread:   11600 lr:  0.004214 avg.loss:  2.265953 ETA:   0h 0m 5s Progress:  91.7% words/sec/thread:   11600 lr:  0.004138 avg.loss:  2.265527 ETA:   0h 0m 5s Progress:  91.9% words/sec/thread:   11600 lr:  0.004060 avg.loss:  2.265179 ETA:   0h 0m 5s Progress:  92.0% words/sec/thread:   11600 lr:  0.003981 avg.loss:  2.264781 ETA:   0h 0m 5s Progress:  92.2% words/sec/thread:   11600 lr:  0.003900 avg.loss:  2.264543 ETA:   0h 0m 4s Progress:  92.4% words/sec/thread:   11600 lr:  0.003823 avg.loss:  2.264261 ETA:   0h 0m 4s Progress:  92.5% words/sec/thread:   11601 lr:  0.003742 avg.loss:  2.263876 ETA:   0h 0m 4s Progress:  92.7% words/sec/thread:   11601 lr:  0.003664 avg.loss:  2.263551 ETA:   0h 0m 4s Progress:  92.8% words/sec/thread:   11600 lr:  0.003587 avg.loss:  2.263199 ETA:   0h 0m 4s Progress:  93.0% words/sec/thread:   11600 lr:  0.003507 avg.loss:  2.262669 ETA:   0h 0m 4s Progress:  93.1% words/sec/thread:   11601 lr:  0.003427 avg.loss:  2.262385 ETA:   0h 0m 4s Progress:  93.3% words/sec/thread:   11602 lr:  0.003346 avg.loss:  2.262199 ETA:   0h 0m 4s Progress:  93.5% words/sec/thread:   11602 lr:  0.003266 avg.loss:  2.261831 ETA:   0h 0m 4s Progress:  93.6% words/sec/thread:   11603 lr:  0.003184 avg.loss:  2.261554 ETA:   0h 0m 4s Progress:  93.8% words/sec/thread:   11603 lr:  0.003104 avg.loss:  2.261200 ETA:   0h 0m 3s Progress:  93.9% words/sec/thread:   11603 lr:  0.003025 avg.loss:  2.260856 ETA:   0h 0m 3s Progress:  94.1% words/sec/thread:   11603 lr:  0.002945 avg.loss:  2.260421 ETA:   0h 0m 3s Progress:  94.3% words/sec/thread:   11603 lr:  0.002851 avg.loss:  2.260058 ETA:   0h 0m 3s Progress:  94.5% words/sec/thread:   11603 lr:  0.002772 avg.loss:  2.259774 ETA:   0h 0m 3s Progress:  94.6% words/sec/thread:   11603 lr:  0.002695 avg.loss:  2.259382 ETA:   0h 0m 3s Progress:  94.8% words/sec/thread:   11604 lr:  0.002614 avg.loss:  2.258968 ETA:   0h 0m 3s Progress:  94.9% words/sec/thread:   11605 lr:  0.002531 avg.loss:  2.258694 ETA:   0h 0m 3s Progress:  95.1% words/sec/thread:   11604 lr:  0.002455 avg.loss:  2.258391 ETA:   0h 0m 3s Progress:  95.3% words/sec/thread:   11605 lr:  0.002374 avg.loss:  2.258090 ETA:   0h 0m 3s Progress:  95.4% words/sec/thread:   11605 lr:  0.002295 avg.loss:  2.257756 ETA:   0h 0m 2s Progress:  95.6% words/sec/thread:   11605 lr:  0.002215 avg.loss:  2.257350 ETA:   0h 0m 2s Progress:  95.7% words/sec/thread:   11605 lr:  0.002138 avg.loss:  2.257131 ETA:   0h 0m 2s Progress:  95.9% words/sec/thread:   11605 lr:  0.002056 avg.loss:  2.256807 ETA:   0h 0m 2s Progress:  96.1% words/sec/thread:   11606 lr:  0.001974 avg.loss:  2.256452 ETA:   0h 0m 2s Progress:  96.2% words/sec/thread:   11606 lr:  0.001896 avg.loss:  2.256225 ETA:   0h 0m 2s Progress:  96.4% words/sec/thread:   11607 lr:  0.001816 avg.loss:  2.255754 ETA:   0h 0m 2s Progress:  96.5% words/sec/thread:   11606 lr:  0.001740 avg.loss:  2.255543 ETA:   0h 0m 2s Progress:  96.7% words/sec/thread:   11606 lr:  0.001662 avg.loss:  2.255179 ETA:   0h 0m 2s Progress:  96.8% words/sec/thread:   11605 lr:  0.001585 avg.loss:  2.254942 ETA:   0h 0m 2s Progress:  97.0% words/sec/thread:   11606 lr:  0.001506 avg.loss:  2.254619 ETA:   0h 0m 1s Progress:  97.2% words/sec/thread:   11607 lr:  0.001423 avg.loss:  2.254225 ETA:   0h 0m 1s Progress:  97.3% words/sec/thread:   11606 lr:  0.001345 avg.loss:  2.253958 ETA:   0h 0m 1s Progress:  97.5% words/sec/thread:   11607 lr:  0.001265 avg.loss:  2.253625 ETA:   0h 0m 1s Progress:  97.6% words/sec/thread:   11607 lr:  0.001185 avg.loss:  2.253241 ETA:   0h 0m 1s Progress:  97.8% words/sec/thread:   11607 lr:  0.001107 avg.loss:  2.252798 ETA:   0h 0m 1s Progress:  97.9% words/sec/thread:   11607 lr:  0.001028 avg.loss:  2.252566 ETA:   0h 0m 1s Progress:  98.1% words/sec/thread:   11608 lr:  0.000947 avg.loss:  2.252133 ETA:   0h 0m 1s Progress:  98.3% words/sec/thread:   11607 lr:  0.000856 avg.loss:  2.251725 ETA:   0h 0m 1s Progress:  98.4% words/sec/thread:   11607 lr:  0.000777 avg.loss:  2.251456 ETA:   0h 0m 0s Progress:  98.6% words/sec/thread:   11607 lr:  0.000698 avg.loss:  2.251118 ETA:   0h 0m 0s Progress:  98.8% words/sec/thread:   11608 lr:  0.000617 avg.loss:  2.250690 ETA:   0h 0m 0s Progress:  98.9% words/sec/thread:   11607 lr:  0.000539 avg.loss:  2.250510 ETA:   0h 0m 0s Progress:  99.1% words/sec/thread:   11608 lr:  0.000460 avg.loss:  2.250171 ETA:   0h 0m 0s Progress:  99.2% words/sec/thread:   11607 lr:  0.000382 avg.loss:  2.249959 ETA:   0h 0m 0s Progress:  99.4% words/sec/thread:   11607 lr:  0.000306 avg.loss:  2.249712 ETA:   0h 0m 0s Progress:  99.5% words/sec/thread:   11606 lr:  0.000230 avg.loss:  2.249357 ETA:   0h 0m 0s Progress:  99.7% words/sec/thread:   11606 lr:  0.000151 avg.loss:  2.249057 ETA:   0h 0m 0s Progress:  99.9% words/sec/thread:   11607 lr:  0.000069 avg.loss:  2.248739 ETA:   0h 0m 0s Progress: 100.0% words/sec/thread:   11605 lr: -0.000001 avg.loss:  2.248444 ETA:   0h 0m 0s Progress: 100.0% words/sec/thread:   11605 lr:  0.000000 avg.loss:  2.248444 ETA:   0h 0m 0s #> [1] \"/home/runner/ft_model/ft_model.bin\" # Load model: model_path <- path.expand(\"~/ft_model/ft_model.bin\") model <- fastrtext::load_model(paste0(model_path))  # Nearest-neigbor query: fastrtext::get_nn(model, \"spahn\", k=8) #>           spahns    spahnversagen             jens  spahnruecktritt  #>        0.9192327        0.7751982        0.7709382        0.7231418  #>    maskenskandal     maskenaffäre personalnotstand          tönnies  #>        0.7174485        0.6899417        0.6754116        0.6590577"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"finding-keywords","dir":"Articles","previous_headings":"","what":"Finding keywords","title":"from_text_to_measurement","text":"Next, want use fastText model manually annotated dataset find short dictionary populist communication. First, split annotated data train test sample, using caret::createDataPartition. starting point dictionary, use vocabulary fastText model. add ID vocabulary use clean_text remove stopwords, drop words less 3 characters list: Next, want narrow list. identify words similar hand-coded corpus populist Tweets dissimilar non-populist Tweets df_train using find_distinctive. find_distinctive computes average representation subset annotated corpus populism coded present, well representation negative counterpart, .e. non-populist corpus. computes cosine similarities word dataframe two corpora, calculates difference two similarity scores. provides us quick computationally inexpensive shortcut find keywords capture specific concept, nothing else. However, method narrowing list keywords bit crude informative assessing well single word perform used DDR method. Hence, want get actual F1 scores single word, used one-word-dicitonary DDR method. Note DDR method returns cosine similarity continuous measure, theoretically ranging -1 +1, manually annotated dataset coded binary fashion (populist non-populist). solve problem, continuous measure used independent variable logistic regression, predicting manual binary coding. obtain Recall, Precision F1 scores, dictvectoR compares binary predictions resulting regression manual coding. direct gold standard test circumvents problem producing reliable granular codes manually (Grimmer & Stewart, 2013, p. 275). Recall evaluation metric indicates well automated measure captures true positives, precision indicates well captures true positives, F1 harmonic mean (Chinchor 1992). get_many_F1s function efficiently returns F1 scores list words dictionaries, used dictionary DDR method. ’re interested best-performing quartile, extract using helper-function filter_ntile:","code":"# set seed set.seed(42)  # get index for splitting train_id <- caret::createDataPartition(tw_annot$pop,                                 times = 1,                                 p = .7,                                 list = F) # split annotated data df_train <- tw_annot %>% slice(train_id)  df_test <- tw_annot %>% slice(-train_id) # # Get vocabulary vocab_df <- fastrtext::get_dictionary(model) %>% data.frame(words = .)  # Get vocabulary ID vocab_df$word_id <- fastrtext::get_word_ids(model, vocab_df$words)  # remove stopwords vocab_df <- clean_text(vocab_df,                         text_field = \"words\",                        remove_stopwords = T) %>%             filter(n_words == 1,                    n_chars > 2) # Get distinctive word scores vocab_df <- find_distinctive(df_train,                               concept_field = \"pop\",                              text_field = \"text\",                              word_df = vocab_df,                              word_field = \"words\",                              model = model)  vocab_df %<>% mutate(pop_distinctiveXpossim = pop_distinctive*pop_possim) %>%                filter_ntile(\"pop_distinctiveXpossim\", .75) vocab_df$popF1 <- get_many_F1s(vocab_df$words,                               model = model,                               df = df_train,                               reference = \"pop\") top <- vocab_df %>%               filter_ntile(\"popF1\", .75) %>%              filter(popF1 > 0)"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"add-multi-words","dir":"Articles","previous_headings":"","what":"Add multi-words","title":"from_text_to_measurement","text":"concepts expressed rather multi-word expressions single words. case, suspect people-centrism, one core dimension populism, may involve multiword expressions, example constructing -group references ‘’, e.g., ‘taxpayers’, ‘land’ etc. find common multiword expressions, use population data (75 percent speed process bit), tokenize using quanteda::tokens. add_multiwords adds multiword expressions dataframe single words counts occurences. look multiwords 2-word-window (level = 1), can increased 3-words window (level = 2). process might take two three minutes. want narrow list , filtering terms occur . add unique word_id add_word_id.","code":"tw_data %<>% filter(n_words >= 2)  # Use 75% sample set.seed(42) tw_split <- tw_data %>% slice_sample(prop = .75)  # tokenize toks <- quanteda::tokens(tw_split$text) top <- add_multiwords(top,                        tokens = toks,                        min_hits = 1,                       word_field = \"words\",                       levels = 1) #> [1] \"Finding multi-word expressions in window = 1...\" #> Joining, by = \"from\" #> [1] \"Adding missing count of original words:\" #> [1] \"Counting word occurrences...\" top %<>% filter(hits > 1)  top %<>% add_word_id()"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"get-f1-scores","dir":"Articles","previous_headings":"","what":"Get F1 scores","title":"from_text_to_measurement","text":"Now, also want know F1 scores new multiword terms, hence just run get_many_F1s .","code":"top$popF1 <- get_many_F1s(top$words,                           model = model,                           df = df_train,                           reference = \"pop\")"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"drop-similar-terms","dir":"Articles","previous_headings":"","what":"Drop similar terms","title":"from_text_to_measurement","text":"resulting list words quite long includes lot redundancy. Note unlike traditional dictionary approaches, DDR method performs better input dictionary short clear-cut. redundancy problem traditional dictionaries, may distort performance dictionary DDR (Garten et al., 2018). remove_similar_words helps us detect similar terms - computing pairwise cosine similarity words representations datafram. can use two score input decide words drop: use F1 score calculated popF1, number occurences (compare_hits = T). set function compare terms reach smiliarity 60% (min_simil = .6). win_threshold defines share comparisons term must win order remain dataframe. Additionally, drop terms originate uni-word afd, German populist party, since want populism measure rather non-partisan:","code":"## narrow down top_subs <- top %>% remove_similar_words(model,                                          compare_by = \"popF1\",                                         compare_hits = T,                                          min_simil = .6,                                          win_threshold = .65) %>%                      filter(!from == \"afd\")  #> Warning in .recacheSubclasses(def@className, def, env): undefined subclass #> \"unpackedMatrix\" of class \"mMatrix\"; definition not updated #> Warning in .recacheSubclasses(def@className, def, env): undefined subclass #> \"unpackedMatrix\" of class \"replValueSp\"; definition not updated"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"narrowing-down-by-hand","dir":"Articles","previous_headings":"","what":"Narrowing down by hand","title":"from_text_to_measurement","text":"Let’s look terms found far. (won’t print 400 terms top 50) list keywors still quite long. includes many terms seem theoretically plausible, also many words obviously either specific, seem place. Since next steps involve combinatorics finding good combination keywords, want narrow list words drastically possible. theoretical reasons, decided group found words three categories reflect different aspects populist communication: Terms reflect ‘elites’, terms reflect ‘people’, words relate two groups. 400 terms, hand-picked five distinctive terms categories. mode selection theory-driven different inductive logic used far. may want opt different modes selection, depending task quality inductively generated keywords. words picked processing: use hand-picked list words create shortlist add variable indicates category:","code":"top_subs %>% arrange(desc(popF1)) %>% pull(words) %>% head(50) #>  [1] \"panzer\"                      \"wahn\"                        #>  [3] \"deutschen steuerzahler\"      \"wand\"                        #>  [5] \"verrechnet\"                  \"steuergeldverschwendung\"     #>  [7] \"teures\"                      \"gaga\"                        #>  [9] \"steuerzahler\"                \"skandalösen\"                 #> [11] \"rücken wand\"                 \"gender gaga\"                 #> [13] \"mdb nimmt\"                   \"verrückt\"                    #> [15] \"mdb groko\"                   \"deutschen mittelstand\"       #> [17] \"vernichtet\"                  \"igmetall\"                    #> [19] \"skandalöse\"                  \"deckt\"                       #> [21] \"diesel fahrverbote\"          \"daimler vw\"                  #> [23] \"banken\"                      \"maskenskandal\"               #> [25] \"irrweg\"                      \"skandalös\"                   #> [27] \"deutschland groko\"           \"gendergaga\"                  #> [29] \"lobbyregister\"               \"autofahrer\"                  #> [31] \"entlarven\"                   \"mittels\"                     #> [33] \"existenzen vernichtet\"       \"kette\"                       #> [35] \"sondervermögen\"              \"staatshaushalt\"              #> [37] \"suizid\"                      \"diesel benziner\"             #> [39] \"maut desaster\"               \"dax\"                         #> [41] \"betreibt\"                    \"steuergeld\"                  #> [43] \"abgasnorm\"                   \"monetäre staatsfinanzierung\" #> [45] \"vernichten\"                  \"merkels groko\"               #> [47] \"rechnet\"                     \"deutsche steuerzahler\"       #> [49] \"kurzstreckenflüge verbieten\" \"irrsinnige\" elites <- c( \"altparteien\",  \"lobbyisten\",  \"merkel co\",  \"plänen bundesregierung\",  \"zwangsgebühr\"  )  people <- c( \"arbeitnehmern\",   \"existenzen\",  \"deutschen\",  \"deutschen mittelstand\",  \"deutschen steuerzahler\"  )  relation <- c( \"entlarven\",  \"schamlos\",  \"verrückt\", \"wahn\",  \"steuergeldverschwendung\"  ) top_elites <- top_subs %>% filter(words %in% elites) %>% mutate(cat = \"elites\") top_people <- top_subs %>% filter(words %in% people) %>% mutate(cat = \"people\") top_relation <- top_subs %>% filter(words %in% relation) %>% mutate(cat = \"relation\")  shortlist <- bind_rows(top_elites, top_people, top_relation)  shortlist$cat %<>% factor()  shortlist$cat %>% summary() #>   elites   people relation  #>        5        4        5"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"get-combinations","dir":"Articles","previous_headings":"","what":"Get combinations","title":"from_text_to_measurement","text":"now use shortlist get possible combinations various length per category, possible combinations combinations. get_combis helps us finding combinations, provides useful random sampling mechanism limits number returned combinations: Setting limit, limits number combinations given length returned per dimension. example, consider shortlist words 2 categories B, 10 words per category, want find combinations words include least 3 words per category (min_per_dim = 3) maximum 10 words overall (max_overall = 10), .e. max. 5 words per dimension. 120 possible combinations length 3 category B: However, also get 210 possible combinations length 4, 252 combinations length 5 per category: want get possible combinations combinations, number increases really quickly, can see . first two lines return possible cominations B varying length 3 5. expand_grid returns combinations combinations: Setting limit , let’s say 30, get_combis radomly draws 30 possible combinations one length per category. case, limit really necessary, 5 words per category want lengths 3 4. maximum number combinations per length & category 10: Although unnecessary case, specify limit (won’t drop combinations) - just demonstration. Setting seed makes results reproducible, default 1. get_cobmis returns data.frame includes column ’re settings stored, case want come back draw different round combinations. , let’s get combinations list words:","code":"A <- c(1:10) B <- c(11:20)  choose(10, 3) #> [1] 120 choose(10, 4) #> [1] 210 choose(10, 5) #> [1] 252 A_c <- do.call(\"c\", lapply(3:5, function(i) combn(A, i, FUN = list))) B_c <- do.call(\"c\", lapply(3:5, function(i) combn(B, i, FUN = list))) expand.grid(A_c, B_c)  %>% nrow() #> [1] 338724 max(   choose(5, 3),   choose(5, 4) ) #> [1] 10 combis_rd <- get_combis(shortlist,                         dims = \"cat\",                         min_per_dim = 3,                         max_overall = 12,                         limit = 10,                         seed = 42) #> Joining, by = \"rowid\""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"evaluate-combinations","dir":"Articles","previous_headings":"","what":"Evaluate combinations","title":"from_text_to_measurement","text":"Now want get Recall, Precision, F1 scores 3,375 combinations. get_many_RPFs made task: Now, let’s pick best performing short dictionary: 10 words cover various aspects populist communication highly context-specific. However, place dive theoretical discussions. Let’s check performance dictionary instead. training sample, dictionary reaches F1 .56, great, OK. Let’s check F1 score applied test sample: short dictionary reaches F1 .55 test sample, almost par result train data. Drawing evaluation, reason assume avoided overfitting dictionary training data. may therefore assume measurement can least approximately capture populist communication population dataset. mean short dictionary perform equally well different contexts, e.g., user-generated material, timeframes. special applications, might intereste DDR measurement perform different subsets data, e.g., using translated corpus originating various languages. F1 scores grouped subsets text dataframe, can obtained get_many_F1s_by_group. case, might interested dictionaries perform different political parties. , demonstrate function using top 5% short dictionaries. Since populist communication annotated sample indeed mainly concentrated parties AfD Die Linke, calculate 3rd root product overall F1 score two party-specific F1-scores pick best performing, balanced dictionary: Let’s check performance ‘balanced’ dictionary test data: decreased performance test data, stick dictionary picked .","code":"combis_df <- get_many_RPFs(keyword_df = combis_rd,                              keyword_field = \"combs_split\",                              model = model,                               text_df = df_train,                              reference = \"pop\",                               text_field = \"text\") #> Joining, by = \"rowid\" # Pick dictionary that maximizes F1: dict  <- combis_df %>%            filter(F1 == max(F1)) %>%            pull(combs_split) %>%           unlist()  # Let's see: dict #>  [1] \"deutschen\"               \"deutschen mittelstand\"   #>  [3] \"deutschen steuerzahler\"  \"entlarven\"               #>  [5] \"merkel co\"               \"plänen bundesregierung\"  #>  [7] \"steuergeldverschwendung\" \"verrückt\"                #>  [9] \"wahn\"                    \"zwangsgebühr\"            #> [11] \"altparteien\"             \"deutschen mittelstand\"   #> [13] \"deutschen steuerzahler\"  \"entlarven\"               #> [15] \"existenzen\"              \"merkel co\"               #> [17] \"plänen bundesregierung\"  \"steuergeldverschwendung\" #> [19] \"verrückt\"                \"wahn\" combis_df$F1 %>% max()   #> [1] 0.5568627 # Performance for test_df get_F1(df_test, dict, model, 'pop') #> [1] 0.52 combis_subset <- combis_df %>% filter_ntile(\"F1\", .95)  combis_df_by_group <-  get_many_F1s_by_group(keyword_df = combis_subset,                                              keyword_field = \"combs_split\",                                              id = \"id\",                                              model = model,                                              text_df = df_train,                                              group_field = \"party\",                                              reference = 'pop') #> Joining, by = \"id\" # Compute 3rd root product of 3 F1s: combis_df_by_group %<>% mutate(F1_balanced = (F1*F1_AfD*F1_Linke)^(1/3))  dict_bal  <- combis_df_by_group %>%                          filter(F1_balanced == max(F1_balanced)) %>%                          pull(combs_split) %>%                         unlist()  dict_bal #> [1] \"deutschen\"               \"deutschen mittelstand\"   #> [3] \"entlarven\"               \"existenzen\"              #> [5] \"lobbyisten\"              \"merkel co\"               #> [7] \"steuergeldverschwendung\" \"wahn\"                    #> [9] \"zwangsgebühr\" get_F1(df_test, dict_bal, model, 'pop') #> [1] 0.54"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"apply-ddr","dir":"Articles","previous_headings":"","what":"Apply DDR","title":"from_text_to_measurement","text":"Now, want apply best performing dictionary datasets. apply , population data tw_data annotated data tw_annot, using core function package cossim2dict. fill missing values, occure sometimes text cleaning empties text field mean minus 1 SD:","code":"tw_annot$pop_ddr <- cossim2dict(df = tw_annot,                                dictionary = dict,                                model = model,                                replace_na = 'mean-sd')  tw_data$pop_ddr <- cossim2dict(df = tw_data,                                dictionary = dict,                                model = model,                                replace_na = 'mean-sd')"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"face-validity","dir":"Articles","previous_headings":"","what":"Face validity","title":"from_text_to_measurement","text":"Now, lets inspect top 3 populist Tweets population data: examples include Tweets AfD directed ‘genderism’ leftist ideology, directed EU, directed climate politics. Judging face validity Tweets clearly populist. Let’s inspect lower end spectrum: least populist Tweets - according measurement - include quite harmless “get well soon” wishes.","code":"tw_data %>% arrange(desc(pop_ddr)) %>% pull(full_text) %>% head(3) #> [1] \"#AfD-Fraktionsvize @PeterFelser hat den linksideologischen Hype um neue Geschlechter als „teures #Gendergaga“ kritisiert: \\\"Riesen Aufwand für Nichts! Gender-Gaga auf Kosten der #Steuerzahler ersatzlos streichen!\\\" #Bundestag https://t.co/paIVvtTkfg\"                                      #> [2] \"Entwicklungsminister Gerd Müller &amp; Arbeitsminister @hubertus_heil wollen ein Lieferkettengesetz durchsetzen, das eine nahezu unbegrenzte Haftungsausweitung für 🇩🇪 Unternehmen vorsieht. @Frohnmaier_AfD: \\\"Schlussstrich unter Lieferketten-Irrsinn ziehen\\\" #AfD https://t.co/mW9wvDrFaI\" #> [3] \"\\\"Wie betrügerische Haustürgeschäfte! #Euro-Politik-Wahn treibt Deutschland in den Ruin.\\\" #AfD-MdB Prof. Harald @h_weyel antwortet auf Angela Merkels #Regierungserklärung zur #EU-Ratspräsidentschaft!  #Bundestag https://t.co/TqzlYrbxfv\" tw_data %>% arrange(desc(pop_ddr)) %>% pull(full_text) %>% tail(3) #> [1] \"@ralphruthe @marga_owski Gute Besserung!\"                       #> [2] \"@BelaAnda1 @walli5 @PolyPepper Ohne Diblom glaub‘ ich des ned.\" #> [3] \"@grischdjane @doktordab @MarkusBlume So ein Käse Käse ;-)\""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"plots","dir":"Articles","previous_headings":"","what":"Plots","title":"from_text_to_measurement","text":"Let’s use DDR measurements plots. start plotting gradual populism measurement human, binary coding populism, using complete annotated dataset:  see non-populist Tweets score higher , mean populism score groups clearly different - indicated boxplots. can also use parallel-coded 90 Tweets get visual evaluation gradual measurement returned DDR method. ‘Populism’ annotated dataset coded two distinct categories, anti-elitism people-centrism. obtain gradual score, just calculate sum coding two categories two coders, resulting score ranges 0 4. plot DDR score:  two (semi-)continuous scores Pearson’s r correlation .53: Finally, assess external validity measurement, compare aggregated level POPPA expert rating political parties (Meijers Zaslove, 2020). expert survey provides gradual rating populism political parties. score already merged tw_data.  aggregate level, expert ratings populism political parties align well DDR score populist communication. Indeed, scores strongly correlated:","code":"tw_annot$pop %<>% factor()   ggplot(tw_annot, aes(x = pop, y = pop_ddr, color = pop)) +    geom_boxplot()+   geom_jitter(width = .2)+   theme_bw()+   labs(y = \"Populism (DDR)\",         x = \"Populism (Human coding)\")+   theme(axis.title.x = element_text(hjust = 0),         legend.position=\"none\") parallel_df <- tw_annot %>%   filter(rel_test == 1) %>%   mutate(pop_cum = ppc_A + ppc_B + ane_A + ane_B)   ggplot(parallel_df, aes(x = pop_ddr, y = pop_cum))+   geom_jitter(height = .1,               width = 0)+   geom_smooth(method = lm, se = T)+   coord_cartesian(ylim = c(0,4)) cor(parallel_df$pop_cum, parallel_df$pop_ddr) #> [1] 0.5329156 # aggregate on party level tw_party <- tw_data %>%                group_by(party) %>%               summarise(pop_ddr = mean(pop_ddr),                         poppa_populism = mean(poppa_populism))    ggplot(tw_party, aes(x = pop_ddr, y = poppa_populism, label=party))+   geom_point(na.rm = T)+   geom_smooth(method = lm)+   geom_text(hjust=0, vjust=0) cor(tw_party$pop_ddr, tw_party$poppa_populism) #> [1] 0.8707323"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"simple-visual-analysis","dir":"Articles","previous_headings":"","what":"Simple visual analysis","title":"from_text_to_measurement","text":"resulting DDR scores used, e.g. investigate temporal shifts strategies political parties. plot mean populism score per party time. see ‘AfD’ clearly populist party, followed ‘Die Linke’, theoretically absolutely plausible. sudden decrease populist communication ‘Die Linke’ parties August 2020, driven growth populist coronasceptic movement - opposed parties ‘AfD’. end timeframe, shortly German General Elections September 2021, level populism rises notably ‘AfD’.","code":"tw_data %>%     mutate(day = as.Date(created_at)) %>%      group_by(party, day) %>%      summarise(pop_ddr = mean(pop_ddr)) %>%      ggplot(aes(x = day, y = pop_ddr, color = party))+     geom_smooth(method = 'loess', span = 0.15, se = F)"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"from_text_to_measurement","text":"Summing , dictionary measurement produced vignette good job differentiating levels populism communication political parties, aggregate level. also produces somehow plausible results compared cumulative coding two coders. performs OK compared directly binary hand-codings Tweets. However, much room improvement. possible ways improve measurement: Improve fastText model using much larger training dataset. Pre-trained models trained millions sentences, example. However, unlike generic models, may want stick material specific context (: communication political elites). Improve fastText model parameter tuning. E.g., donsider increasing number dimensions (dim, 100-300 popular choices), increasing number epochs (epoch), play around different ngram lengths (minn, maxn), play around window size context (ws). can use get_nn face validity plausibility checks, use formal test (e.g., analogy tests) determine quality fastText model. Use larger annotated sample finding good keywords evaluating quality. larger sample, lower risk overfitting short dictionary specific context. Play around different thresholds remove_similar_terms. function can quite dramatic effect words included excluded. Think theory-driven, explicit coding-scheme hand-picking words narrowed subset. Allow combinations per length dimension (limit) get_combis. Good luck!","code":""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"from_text_to_measurement","text":"Bojanowski, P., Grave, E., Joulin, ., & Mikolov, T. (2017). Enriching Word Vectors Subword Information. ArXiv:1607.04606 [Cs]. Retrieved http://arxiv.org/abs/1607.04606 Chinchor, N. (1992). MUC-4 evaluation metrics. Proceedings 4th Conference Message Understanding, 22–29. USA: Association Computational Linguistics. https://doi.org/10.3115/1072064.1072067 Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018). Dictionaries distributions: Combining expert knowledge large scale textual data content analysis. Behavior Research Methods, 50(1), 344–361. https://doi.org/10.3758/s13428-017-0875-9 Grimmer, J., & Stewart, B. M. (2013). Text data: promise pitfalls automatic content analysis methods political texts. Political Analysis, 21(3), 267–297. https://doi.org/10.1093/pan/mps028 Meijers, M., & Zaslove, . (2020). Populism Political Parties Expert Survey 2018 (POPPA) (Data set). Harvard Dataverse. https://doi.org/10.7910/DVN/8NEL7B Thiele, D. (2022, June 27). “Don’t believe media’s pandemic propaganda!!” Covid-19 affected populist Facebook user comments seven European countries. Presented ICA Regional Conference 2022. Computational Communication Research Central Eastern Europe, Helsinki, Finland. Retrieved https://ucloud.univie.ac./index.php/s/PzGzChXroLCXrtt","code":""},{"path":"https://thieled.github.io/dictvectoR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Daniel Thiele. Maintainer, author.","code":""},{"path":"https://thieled.github.io/dictvectoR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Thiele D (2022). dictvectoR:  Word vectors dictionaries . https://github.com/thieled, https://thieled.github.io/dictvectoR/.","code":"@Manual{,   title = {dictvectoR: {{ Word vectors for dictionaries }}},   author = {Daniel Thiele},   year = {2022},   note = {https://github.com/thieled, https://thieled.github.io/dictvectoR/}, }"},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"dictvector-","dir":"","previous_headings":"","what":"{{ Word vectors for dictionaries }}","title":"{{ Word vectors for dictionaries }}","text":"dictvectoR implements “Distributed Dictionary Representation” (DDR) Method (Garten et al., 2018). DDR uses word vector representation dictionary, .e. list words reflect theoretical concept, calculate “continuous measure similarity concept piece text” [1]. Word vectors, also called word embeddings, aim represent semantic proximity words vector space [2]. package uses fastText word vectors [3]. Average vector representations computed concept dictionary document. DDR score cosine similarity vectors [1]. addition core function, package comes range functions help find keywords assess performance words dictionaries. can learn workflow training fastText model, finding good dictionary, evaluating performance vignette(\"from_text_to_measurement\"). workflow described greater detail [4]. demonstrate usage, package comes corpus 20,838 Tweets 32 Twitter accounts German politics, published 2020-03-11 2021-09-25, hand-coded sample 1,000 Tweets corpus, coded along two binary variables populist communication: Anti-elitism people-centrism. package also includes minimal fastText model, serves provide working examples functions package recommended used actual analysis.","code":""},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"{{ Word vectors for dictionaries }}","text":"can install development version dictvectoR GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"thieled/dictvectoR\")"},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"{{ Word vectors for dictionaries }}","text":"basic example, starting scratch, using built-Twitter data German politicians Covid-crisis. get started, need fastText word vector model, train pre-processed data, cleaned, split, shuffled [prepare_train_data]. demonstration purposes, keep model simple small. actual application, might want consider using much textual data, epochs, dimensions larger bucket size. might also consider using (task-specific) pre-trained model . (Note: code create fastText model user home directory deleted end readme.) text data loaded Tweets German politicians, posted beginning Covid pandemic German General Elections September 2021. First, clean textual data, using helper function [clean_text], tailored German social media texts. Now, lets apply DDR method. Let’s say want measure degree populism Tweets. first define ad-hoc short dictionary consider reflect important dimensions populist communication apply DDR method using short dictionary [cossim2dict]. Let’s look top 3 populist Tweets, according ad-hoc measurement: speak German, can judge face-validity top results appear quite plausible. Let’s quick look measurement can re-create scores POPPA expert survey, rated parties along contiuous score populism. score already included tw_data:  aggregate level, expert survey rating ad-hoc DDR measurement highly correlated:","code":"library(dictvectoR) library(dplyr)  # Prepare text data. Cleans, splits, and shuffles textual data texts <- prepare_train_data(tw_data, text_field = \"full_text\", seed = 42)  # Create local folder, set model file name dir.create(\"~/ft_model_readme\", showWarnings = FALSE) model_file <- path.expand(\"~/ft_model_readme/ft_model_demo\")  # Train a fasttext model using the twitter data fastrtext::build_vectors(texts,                           model_file,                           modeltype = c(\"skipgram\"),                          dim = 70, epoch = 5, bucket = 5e+4, lr = 0.1,                          maxn = 6,  minn = 4, minCount = 4,                          verbose = 1, ws= 5)  # Load model: model_path <- path.expand(\"~/ft_model_readme/ft_model_demo.bin\") model <- fastrtext::load_model(paste0(model_path)) tw_data %<>% clean_text(text_field = \"full_text\", remove_stopwords = T) # Define a short dictionary pop_dict <- c(\"merkel\",               \"irrsinn\",               \"diktatur\",               \"lobbyismus\",               \"arbeitende menschen\",               \"deutschland\",               \"unsere steuergelder\")  # Get the DDR score tw_data$pop_ddr <- cossim2dict(tw_data, pop_dict, model, replace_na = 'mean-sd') tw_data %>%   dplyr::arrange(desc(pop_ddr)) %>%   head(3) %>%   dplyr::pull(full_text) #> [1] \"Ob Deutschkurs oder Job - die Bilanz von \\\"geflüchteten\\\" #Frauen stellt sich nach fünf Jahren in #Deutschland ernüchternd dar. Merkels Fachkräfte-Märchen zerschellt krachend an der Realität - - der deutsche #Sozialstaat machts möglich! #AfD #Migration https://t.co/oTrP2ewkpv https://t.co/kr4PPh2a1z\" #> [2] \"#UNO finanziert #Islamisten-#Hass mit unserem #Steuergeld!\\n\\nSeit Jahrzehnten haben die Vereinten Nationen ein massives Problem mit #Islamismus und islamischem #Antisemitismus in den eigenen Reihen – und es wird nicht besser.\\n\\nhttps://t.co/G9MMaQrqIY https://t.co/fzjQ8Ty2qq\"                        #> [3] \"Seit es das #Grundgesetz gibt, hatte #Deutschland keinen Regierungschef, der deutsche Interessen so schlecht vertritt und den Ausverkauf des deutschen Volksvermögens und der Staatsfinanzen so schamlos betreibt, wie Angela #Merkel! #AfD #Corona #Covid19 https://t.co/qGKYUgqcSe\" # aggregate tw_party <- tw_data %>%                group_by(party) %>%               summarise(pop_ddr = mean(pop_ddr),                         poppa_populism = mean(poppa_populism))   # plot library(ggplot2) ggplot(tw_party, aes(x = pop_ddr, y = poppa_populism, label=party))+   geom_point(na.rm = T)+   geom_smooth(method = lm)+   geom_text(hjust=0, vjust=0) cor(tw_party$pop_ddr, tw_party$poppa_populism) #> [1] 0.8342808"},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"finding-keywords","dir":"","previous_headings":"","what":"Finding keywords","title":"{{ Word vectors for dictionaries }}","text":"dictvectoR provides variety functions inductively find good keywords efficiently assess performance many . can learn complete workflow vignette(\"from_text_to_measurement\"). fun! (first, let’s delete demo model:)","code":"unlink(\"~/ft_model_readme\", recursive = TRUE)"},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"{{ Word vectors for dictionaries }}","text":"[1] Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018). Dictionaries distributions: Combining expert knowledge large scale textual data content analysis. Behavior Research Methods, 50(1), 344–361. https://doi.org/10.3758/s13428-017-0875-9 [2] Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation Word Representations Vector Space. ArXiv:1301.3781 [Cs]. http://arxiv.org/abs/1301.3781 [3] Bojanowski, P., Grave, E., Joulin, ., & Mikolov, T. (2017). Enriching Word Vectors Subword Information. ArXiv:1607.04606 [Cs]. Retrieved http://arxiv.org/abs/1607.04606 [4] Thiele, D. (2022, June 27). “Don’t believe media’s pandemic propaganda!!” Covid-19 affected populist Facebook user comments seven European countries. Presented ICA Regional Conference 2022. Computational Communication Research Central Eastern Europe, Helsinki, Finland. Retrieved https://ucloud.univie.ac./index.php/s/PzGzChXroLCXrtt [5] Meijers, M., & Zaslove, . (2020). Populism Political Parties Expert Survey 2018 (POPPA) [Data set]. Harvard Dataverse. https://doi.org/10.7910/DVN/8NEL7B","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Add id — add_id","title":"Add id — add_id","text":"Function add 'id' data.frame already .","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add id — add_id","text":"","code":"add_id(df)"},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add id — add_id","text":"df data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add id — add_id","text":"data.frame includes column called 'id' type 'character' contains unique identifiers rows data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add id — add_id","text":"Searches data.frame variables include 'id' name. Checks unique identifier. , variable renamed 'word_id' forced type 'character'. , new column named 'word_id' type 'character' created, containing string numbers uniquely identify rows data.frame.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add id — add_id","text":"","code":"t1 <- data.frame(id = 1:10, a = sample(letters, 10)) t2 <- data.frame(id = rep(1, 10),                 word_id = sprintf(\"%02d\", 1:10),                 a = sample(letters, 10)) t3 <- data.frame(a = sample(letters, 10)) add_id(t1) #>    id a #> 1   1 k #> 2   2 j #> 3   3 t #> 4   4 q #> 5   5 f #> 6   6 a #> 7   7 e #> 8   8 x #> 9   9 p #> 10 10 m add_id(t2) #>    id a #> 1  01 k #> 2  02 d #> 3  03 x #> 4  04 f #> 5  05 h #> 6  06 s #> 7  07 r #> 8  08 g #> 9  09 i #> 10 10 z add_id(t3) #>    a id #> 1  b 01 #> 2  u 02 #> 3  w 03 #> 4  k 04 #> 5  z 05 #> 6  f 06 #> 7  a 07 #> 8  y 08 #> 9  l 09 #> 10 v 10"},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":null,"dir":"Reference","previous_headings":"","what":"Find multi-word expressions. — add_multiwords","title":"Find multi-word expressions. — add_multiwords","text":"Adds multi-word expressions found quanteda tokens object data.frame words.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find multi-word expressions. — add_multiwords","text":"","code":"add_multiwords(   word_df,   tokens,   min_hits = 3,   word_field = \"words\",   levels = c(1, 2) )"},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find multi-word expressions. — add_multiwords","text":"word_df data.frame containing words. tokens word-tokens object, returned quanteda::tokens. min_hits Numerical. Default 3. Minimum occurrence found multi-word expressions. word_field Character. Default \"words\". Name column word_df contains words. levels Numerical (1 2). window size multi-word expressions.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find multi-word expressions. — add_multiwords","text":"data.frame information word_df, added multi-word expressions rows. Additionally, returned data.frame contains columns... 'orig_id' unique identifier original word, re-used word_df created new present '' indicating word origin 'word_id (character) unique identifier words multi-words 'hits indicating number occurrences word multi-word","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find multi-word expressions. — add_multiwords","text":"","code":"tw_data %<>% head(100) %>% clean_text(text_field = 'full_text') #> Warning: undefined subclass \"unpackedMatrix\" of class \"mMatrix\"; definition not updated #> Warning: undefined subclass \"unpackedMatrix\" of class \"replValueSp\"; definition not updated toks <- quanteda::tokens(tw_data$text) data.frame(words = c(\"deutschen\", \"millionen\")) %>%             add_multiwords(tokens = toks,             min_hits = 1,             levels = 1) #> [1] \"Finding multi-word expressions in window = 1...\" #> Joining, by = \"from\" #> [1] \"Adding missing count of original words:\" #> [1] \"Counting word occurrences...\" #>                               words      from word_id orig_id hits #> 3                     der deutschen deutschen   1_001       1    3 #> 1                         deutschen deutschen       1       1    6 #> 7             deutschen autokonzern deutschen   1_005       1    1 #> 9               deutschen bundestag deutschen   1_007       1    1 #> 11 deutschen kommissionspräsidentin deutschen   1_009       1    1 #> 12                deutschen politik deutschen   1_010       1    1 #> 10            deutschen unternehmen deutschen   1_008       1    1 #> 8              deutschen wirtschaft deutschen   1_006       1    1 #> 6                   einer deutschen deutschen   1_004       1    1 #> 5                      im deutschen deutschen   1_003       1    1 #> 4                  keinen deutschen deutschen   1_002       1    1 #> 2                         millionen millionen       2       2    3 #> 17                   millionen euro millionen   2_005       2    1 #> 16                millionen pendler millionen   2_004       2    1 #> 15                    millionen von millionen   2_003       2    1 #> 13                   sind millionen millionen   2_001       2    1 #> 14                vierzig millionen millionen   2_002       2    1"},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Add word_id. — add_word_id","title":"Add word_id. — add_word_id","text":"Function add word_id data.frame already .","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add word_id. — add_word_id","text":"","code":"add_word_id(df)"},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add word_id. — add_word_id","text":"df data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add word_id. — add_word_id","text":"data.frame includes column called 'word_id' type 'character' contains unique identifiers rows data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add word_id. — add_word_id","text":"Searches data.frame variables include 'id' name. Checks unique identifier. , variable renamed 'word_id' forced type 'character'. , new column named 'word_id' type 'character' created, containing string numbers uniquely identify rows data.frame.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add word_id. — add_word_id","text":"","code":"t1 <- data.frame(wid = 1:10, a = sample(letters, 10)) t2 <- data.frame(id = rep(1, 10),                 word_id = sprintf(\"%02d\", 1:10),                 a = sample(letters, 10)) t3 <- data.frame(a = sample(letters, 10)) add_word_id(t1) #>    a word_id #> 1  u       1 #> 2  j       2 #> 3  d       3 #> 4  f       4 #> 5  g       5 #> 6  m       6 #> 7  l       7 #> 8  k       8 #> 9  r       9 #> 10 b      10 add_word_id(t2) #>    id word_id a #> 1   1      01 m #> 2   1      02 f #> 3   1      03 d #> 4   1      04 q #> 5   1      05 j #> 6   1      06 k #> 7   1      07 h #> 8   1      08 n #> 9   1      09 l #> 10  1      10 s add_word_id(t3) #>    a word_id #> 1  r      01 #> 2  s      02 #> 3  b      03 #> 4  c      04 #> 5  y      05 #> 6  l      06 #> 7  d      07 #> 8  z      08 #> 9  h      09 #> 10 f      10"},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean text — clean_text","title":"Clean text — clean_text","text":"Cleans text stored data frame. Function tailored German texts.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean text — clean_text","text":"","code":"clean_text(   df,   text_field = \"text\",   clean_field = \"text\",   tolower = T,   remove_punct = T,   simplify_punct = F,   replace_emojis = T,   replace_numbers = T,   remove_stopwords = F,   store_uncleaned = T,   count = T )"},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean text — clean_text","text":"df data frame. text_field character. Default 'text'. Name column df contains text cleaned. Default 'text'. clean_field character. Default 'text'. Name given new column df contain cleaned text. tolower logical. Lowercase text? Default TRUE. remove_punct logical. Remove punctuation?  Default TRUE. simplify_punct logical. Replace !, :, ?, ... single . Default FALSE. replace_emojis logical. Replace list emojis German words certain emotions ('wut', 'angst', 'freude'). Default TRUE. replace_numbers logical. Replace numbers German words numbers. Default TRUE. remove_stopwords logical. Remove German stopwords 'nltk' list exception words defined author. Default FALSE. store_uncleaned logical. Save uncleaned text 'uncleaned_text' column? Default TRUE. count logical. Count number words strings cleaning? create columns 'n_words' 'n_chars'. Default TRUE.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean text — clean_text","text":"data.frame columns df. cleaned text column specified text_field , specified 'text.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean text — clean_text","text":"","code":"tw_data %<>% head(10) %>% clean_text(text_field = 'full_text') tw_data$text #>  [1] \"es geht bei dieser richtungswahl um die frage bleiben wir auf einem kurs der mitte und der stabilität oder bekommen wir ein rot grün rotes abenteuer mit unabsehbaren konsequenzen für arbeitsplätze den wohlstand unseres landes und die zukunft unserer kinder tm merzmail\"       #>  [2] \"aus vielen veranstaltungen der letzten wochen nehme ich den eindruck mit die stimmung für cdu und csu ist besser als die umfragen darin liegt jetzt unsere chance bitte gehen sie alle morgen wählen und geben sie beide stimmen der union tm merzmail unionstärkstekraft\"          #>  [3] \"sendung verpasst sehen sie hier die diskussion zwischen friedrich merz und hubertus heil bei sandra maischberger in der ard mediathek ab ca neunzehn tm\"                                                                                                                            #>  [4] \"ich bin mir nicht sicher ob wir corona einschränkungen wie die maskenpflicht noch brauchen wir sehen in ländern die rascher gelockert haben niedrige inzidenzen und hospitalisierungsquoten und wir sehen wie viel gesellschaftlichen dissens die maßnahmen verursachen tm\"         #>  [5] \"frau nahles hatte recht der mechanismus der mindestlohnkommission ist wichtig damit der mindestlohn kein wahlkampfthema wird der vorschlag für zwölf euro mindestlohn ist rein populistisch jetzt startet ein politischer überbietungswettbewerb zwischen spd und linkspartei tm\"   #>  [6] \"ich war immer für den mindestlohn die entscheidende frage ist wer ihn in zukunft festlegt die tarifvertragsparteien oder der gesetzgeber aus meiner sicht sollte dieses thema in der mindestlohnkommission bleiben und gehört nicht in den bundestag tm merz maischberger\"          #>  [7] \"die oberen zehn der steuerpflichtigen bezahlen mehr als fünfzig des einkommenssteueraufkommens zu behaupten dass sich diese menschen nicht genug an der finanzierung des gemeinwesens beteiligen ist unseriös wir haben die höchsten steuern und abgaben in europa tm maischberger\" #>  [8] \"die vollständige abschaffung des solidaritätszuschlags ist eine frage des respekts und der fairness gegenüber all denjenigen die dreißig jahre lang die deutsche einheit finanziert haben alles andere ist ein gebrochenes versprechen des steuergesetzgebers tm maischberger merz\" #>  [9] \"die cdu hat damals als oppositionspartei der agenda tausende der spd zugestimmt wir sehen es mit bedauern dass diese reformen von den sozialdemokraten schritt für schritt wieder zurückgenommen werden wir müssen zurückkehren zum prinzip fördern und fordern tm maischberger\"    #> [10] \"von analog tausende zu digital tausende schauen sie mal was mein team und ich heute zum abschluss des wahlkampfes im hochsauerlandkreis online gestellt haben leiten sie das video auch gerne an freunde und bekannte weiter hier geht s zum download\""},{"path":"https://thieled.github.io/dictvectoR/reference/compound.html","id":null,"dir":"Reference","previous_headings":"","what":"Assignment pipe — %<>%","title":"Assignment pipe — %<>%","text":"See magrittr::%<>% details.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/compound.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assignment pipe — %<>%","text":"lhs object serves initial value target. rhs function call using magrittr semantics.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":null,"dir":"Reference","previous_headings":"","what":"Get confusion matrix — confuse","title":"Get confusion matrix — confuse","text":"Returns confusion matrix validation scores binary reference variable, binary prediction variable.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get confusion matrix — confuse","text":"","code":"confuse(reference, prediction)"},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get confusion matrix — confuse","text":"reference binary reference vector. prediction binary prediction vector.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get confusion matrix — confuse","text":"confusionMatrix object caret::confusionMatrix().","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get confusion matrix — confuse","text":"values reference prediction must 0 1. variables can stored data.frame vectors equal length. Can type numeric factor.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get confusion matrix — confuse","text":"","code":"mtcars$pred <- get_prediction(mtcars, mtcars$am, mtcars$drat) confuse(mtcars$am, mtcars$pred) #> Confusion Matrix and Statistics #>  #>     #>      0  1 #>   0 16  2 #>   1  3 11 #>                                            #>                Accuracy : 0.8438           #>                  95% CI : (0.6721, 0.9472) #>     No Information Rate : 0.5938           #>     P-Value [Acc > NIR] : 0.002273         #>                                            #>                   Kappa : 0.68             #>                                            #>  Mcnemar's Test P-Value : 1.000000         #>                                            #>               Precision : 0.7857           #>                  Recall : 0.8462           #>                      F1 : 0.8148           #>              Prevalence : 0.4062           #>          Detection Rate : 0.3438           #>    Detection Prevalence : 0.4375           #>       Balanced Accuracy : 0.8441           #>                                            #>        'Positive' Class : 1                #>"},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":null,"dir":"Reference","previous_headings":"","what":"Similarity of documents to a dictionary — cossim2dict","title":"Similarity of documents to a dictionary — cossim2dict","text":"Computes cosinal similarity average word vector representation document data frame average word vector representation dictionary, using fasttext word vector model.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Similarity of documents to a dictionary — cossim2dict","text":"","code":"cossim2dict(   df,   dictionary,   model,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Similarity of documents to a dictionary — cossim2dict","text":"df dataframe containing one document per row. dictionary character vector containing keywords dictionary. model fasttext model loaded load_model. text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Similarity of documents to a dictionary — cossim2dict","text":"Numerical. Cosinal similarity, ranging (theoretically) -1 +1. Indicating similarity average fasttext word vector words dictionary average fasttext word vector document df.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Similarity of documents to a dictionary — cossim2dict","text":"Implements method called 'Distributed Dictionary Representation' (DDR), introduced Garten et al. (2018). average dictionary vector calculated mean vector words dictioary, stored character vector. document vectors calculated mean vectors words per observation column named 'text' dataframe. One row dataframe represents one document. , average dictionary vector document vectors L2 normalized. function returns cosinal similarity dictionary vector document dataframe.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Similarity of documents to a dictionary — cossim2dict","text":"Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018). Dictionaries distributions: Combining expert knowledge large scale textual data content analysis. Behavior Research Methods, 50(1), 344 - 361. https://doi.org/10.3758/s13428-017-0875-9","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Similarity of documents to a dictionary — cossim2dict","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\", \"tw_demo_model_sml.bin\", package = \"dictvectoR\")) tw_annot %<>% head(100) %>% clean_text(remove_stopwords = TRUE,                                        text_field = \"full_text\") dict <- c(\"skandal\", \"deutschland\", \"steuerzahler\") tw_annot$ddr <- cossim2dict(tw_annot, dict, model)"},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect similar words — detect_similar_words","title":"Detect similar words — detect_similar_words","text":"Detects similar words data.frame, using fastText model. requested, compares similar words along variable specified compare_by, number occurrences stored variable named hits. Counts often one word 'beats' similar pairwise comparison. count returned 'wins_'.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect similar words — detect_similar_words","text":"","code":"detect_similar_words(   word_df,   model,   word_field = \"words\",   compare_by = NULL,   compare_hits = T,   min_simil = 0.7 )"},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect similar words — detect_similar_words","text":"word_df data.frame containing column words multi-word expressions. model fastText model, loaded load_model. word_field Character. name column word_df contains words. compare_by Character. Default NULL. name column compared. compare_hits Logical. Default TRUE. true counts often one word 'beats' similar regard occurrences. min_simil Numerical (0-1). Default .7. Similarity threshold. Word pairs threshold considered dissimilar.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect similar words — detect_similar_words","text":"data.frame. Containing pairwise similarity table similar words.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Detect similar words — detect_similar_words","text":"Forces 'word_df' unique identifying variable called 'word_id'; re-uses variable named 'id' unique.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect similar words — detect_similar_words","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\", \"tw_demo_model_sml.bin\", package = \"dictvectoR\")) word_df <- data.frame(words = c(\"unsere steuern\", \"steuerzahler\",  \"unsere\",  \"steuern\"), hits = c(2, 3, 15, 4)) detect_similar_words(word_df, model) #> Warning: undefined subclass \"unpackedMatrix\" of class \"mMatrix\"; definition not updated #> Warning: undefined subclass \"unpackedMatrix\" of class \"replValueSp\"; definition not updated #>       simil word_id1          word1 hits1 word_id2          word2 hits2 #> 1 0.8014612        3         unsere    15        1 unsere steuern     2 #> 2 0.8014612        4        steuern     4        1 unsere steuern     2 #> 3 0.8445002        4        steuern     4        2   steuerzahler     3 #> 4 0.8014612        1 unsere steuern     2        3         unsere    15 #> 5 0.8014612        1 unsere steuern     2        4        steuern     4 #> 6 0.8445002        2   steuerzahler     3        4        steuern     4 #>   wins_hits1 #> 1          1 #> 2          1 #> 3          1 #> 4          0 #> 5          0 #> 6          0"},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine which similar terms to drop — drop_which","title":"Determine which similar terms to drop — drop_which","text":"Takes pairwise similarity table returned detect_similar_words. Returns data.frame terms dropped according specified decision rules. function can compare words similarity table along two aspects: number comparison wins regarding score specified 'compare_by' detect_similar_words, resp. remove_similar_words. variable must stored similarity table 'wins_score1. number comparison wins regarding frequency occurrences, set 'compare_hits = T' detect_similar_words, resp. remove_similar_words. variable must stored similarity table 'wins_hits1. variables can compared time. 'wins_score1' missing, function compare 'hits'.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine which similar terms to drop — drop_which","text":"","code":"drop_which(simil_table, compare_hits = T, win_threshold = 0.5)"},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine which similar terms to drop — drop_which","text":"simil_table data.frame. pairwise similarity table returned detect_similar_words. compare_hits logical. Default TRUE. TRUE, consider 'hits' comparison. win_threshold Numerical (0-1). Default .5. Determines threshold drop words, defined proportion won pairwise comparisons. hits scores compared, mean proportions. Words suggested dropping computed value smaller value set .","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine which similar terms to drop — drop_which","text":"data.frame words suggested dropping.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine which similar terms to drop — drop_which","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                           \"tw_demo_model_sml.bin\",                                            package = \"dictvectoR\")) set.seed(1) word_df <- data.frame(words = c(\"unsere steuern\", \"steuerzahler\",                                \"unsere\", \"steuern\"), hits = c(2, 3, 15, 4), score = rnorm(4)) sim_t <- detect_similar_words(word_df, model, compare_by = \"score\") drop_which(sim_t, compare_hits = TRUE, win_threshold = .4) #> # A tibble: 2 × 6 #>   word_id1 words          word_id wins_score_pct wins_hits_pct   win #>   <chr>    <chr>          <chr>            <dbl>         <dbl> <dbl> #> 1 1        unsere steuern 1                  0.5             0  0.25 #> 2 2        steuerzahler   2                  0               0  0"},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter by ntile. — filter_ntile","title":"Filter by ntile. — filter_ntile","text":"Filter data frame top quantile specified variable.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter by ntile. — filter_ntile","text":"","code":"filter_ntile(df, var_field, probs)"},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter by ntile. — filter_ntile","text":"df data.frame. var_field character string indicating name variable used filtering. probs numerical value 0 1 (.e. proportion) indicating quantile threshold used filtering.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter by ntile. — filter_ntile","text":"data.frame, containing rows specified variable greater equal specified threshold.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter by ntile. — filter_ntile","text":"","code":"df <- data.frame(a = rnorm(10), b = sample(letters, 10, replace = TRUE)) filter_ntile(df, \"a\", .75) #>           a b #> 4 0.7383247 y #> 5 0.5757814 l #> 7 1.5117812 a"},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":null,"dir":"Reference","previous_headings":"","what":"Find distinctive keywords — find_distinctive","title":"Find distinctive keywords — find_distinctive","text":"Compares word-vector representations words representations annotated data.frame texts. Aims detect words distinctively characterize concept.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find distinctive keywords — find_distinctive","text":"","code":"find_distinctive(   df,   concept_field,   text_field = \"text\",   word_df,   word_field = \"words\",   model )"},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find distinctive keywords — find_distinctive","text":"df data.frame containing one annotated document per row. concept_field character. Name column contains binary, (hand-coded) indicator presence absence concept. text_field character. Name column df contains text documents. Default \"text\". word_df data.frame containing column words multi-word expressions. word_field character. name column word_df contains words. model fastText model, loaded fastrtext::load_model().","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find distinctive keywords — find_distinctive","text":"Takes annotated data.frame df texts input. varialbe specified concept_field df indicates presence absence theoretical concept text. Two average word-vector representations computed, using fastText model: One texts contain concept, one . second data.frame, word_df, contains one (multi-)word per row 'word_field'. Three new columns word_df created: first, ending '_possim', indicates cosine similarity word positive concept corpus. second '_negsim', indicates similarity remaining corpus. third, ending '_distinctive' difference two.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find distinctive keywords — find_distinctive","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") word_df <- data.frame(words = c(\"skandal\", \"deutschland\", \"wundervoll\")) find_distinctive(tw_annot,                  \"pop\",                   word_df = word_df,                   model = model) #>         words pop_possim pop_negsim pop_distinctive #> 1     skandal  0.5809499  0.5490425      0.03190745 #> 2 deutschland  0.7616619  0.7402692      0.02139269 #> 3  wundervoll  0.6919564  0.7408727     -0.04891631"},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Find index of unique id in df. — find_unique_id","title":"Find index of unique id in df. — find_unique_id","text":"Searches data.frame column includes 'id' name unique identifier. Returns index first column data.frame meets conditions.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find index of unique id in df. — find_unique_id","text":"","code":"find_unique_id(df)"},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find index of unique id in df. — find_unique_id","text":"df data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find index of unique id in df. — find_unique_id","text":"numeric, indicating index unique id column.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find index of unique id in df. — find_unique_id","text":"","code":"df <- data.frame(id_not_unique = rep(1, 10), unique_id = 1:10, also_id = 21:30, other = letters[1:10]) find_unique_id(df) #> [1] 2"},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"Returns Accuracy, Recall, Precision, F1 scores binary reference variable binary prediction variable.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"","code":"get_ARPF(reference, prediction)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"reference binary reference vector. prediction binary prediction vector.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"data.frame columns 'accuracy', 'recall', 'precision', 'F1'.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"values reference prediction must 0 1. variables can stored data.frame vectors equal length. Can type numeric factor.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"","code":"mtcars$pred <- get_prediction(mtcars, mtcars$am, mtcars$drat) get_ARPF(mtcars$am, mtcars$pred) #>   accuracy    recall precision        F1 #> 1  0.84375 0.8461538 0.7857143 0.8148148"},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":null,"dir":"Reference","previous_headings":"","what":"Get F1 score for a DDR measure — get_F1","title":"Get F1 score for a DDR measure — get_F1","text":"Returns F1 score DDR measurement predicting binary reference (.e. manually annotated variable).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get F1 score for a DDR measure — get_F1","text":"","code":"get_F1(   df,   dictionary,   model,   reference,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get F1 score for a DDR measure — get_F1","text":"df data.frame containing one annotated document sentence per row. dictionary character vector containing keywords dictionary, passed cossim2dict. model fasttext model loaded load_model. reference Name binary reference column df. text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs DDR measurement. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get F1 score for a DDR measure — get_F1","text":"gradual DDR measurement passed get_prediction() obtain binary prediction logistic regression. F1 scores indicate performance words/dictionaries predicting binary coding, used DDR method. F1 score harmonic mean Recall Precision (1).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get F1 score for a DDR measure — get_F1","text":"(1) Chinchor, N. (1992). MUC-4 evaluation metrics. Proceedings 4th Conference Message Understanding, 22–29. https://doi.org/10.3115/1072064.1072067","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get F1 score for a DDR measure — get_F1","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") dict <- c(\"skandal\", \"deutschland\", \"steuerzahler\") get_F1(tw_annot, dict, model, 'pop') #> [1] 0.2897527"},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":null,"dir":"Reference","previous_headings":"","what":"Get combinations of keywords — get_combis","title":"Get combinations of keywords — get_combis","text":"Returns combinations keywords.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get combinations of keywords — get_combis","text":"","code":"get_combis(   word_df,   word_field = \"words\",   dims = NULL,   min_per_dim = 1,   max_overall,   limit = NULL,   seed = 1,   save_settings = T,   save_input = F )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get combinations of keywords — get_combis","text":"word_df data.frame containing column words multi-word expressions. word_field character. Default 'words'. name column word_df contains words. dims character. Default NULL. name column word_df groups words dimensions. Can ignored. min_per_dim numerical. Default 1. Minimum number words (per dimension) returned. replaced 1 < 1. max_overall numerical. Maximum number words per returned combination. limit numerical. Default NULL. Limits number combinations equal length per dimension randomization. seed numerical. Default 1. Input make randomization reproducible. save_settings logical. Default TRUE. Saves randomization settings df make reproducible. save_input logical. Default FALSE. Saves input words dims df character string.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get combinations of keywords — get_combis","text":"data.frame combinations. combinations stored list character vectors combs_split. Use column pass get_many_F1s get_many_RPFs. data.frame include string variables words combinations called combs dimension. use columns passing dictionaries get_many_F1s, result faulty average representation, caused representations multi-word expressions queried. Additionally, data.frame includes rowid, count variable number words overall (sum_nterms), counts dimension, can used remove imbalanced dictionaries. requested, settings stores randomization settings, input words dimensions used input.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get combinations of keywords — get_combis","text":"Takes data.frame word_df character column specified words input. default, return combinations various lengths words. Additionally, function can account conceptual dimensions, identified categorical (character, numerical, factor) column word_df specified dims. dimensions specified, function find combinations dimension, return combinations combinations. CAUTION: can lead quickly extremely large number combinations. number combinations can limited several ways: Firstly, minimum number words returned per dimension specified min_per_dim, maximum number words overall set max_overall. Secondly, function allows random sampling combinations. recommended, drastically reduces number returned combinations, reduces computational load, speeds process. (course, comes cost completeness). Random sampling implemented using comboSample function. Setting limit cap number combinations equal length dimension. E.g., limit = 5, min_per_dim = 2, max_overall = 6 set word_df containing two dimensions b, function pick max. five combinations length 2 , five length 2 b, five length 3 , five length 3 b, return combinations combinations.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get combinations of keywords — get_combis","text":"","code":"test_df <- data.frame(words = letters[1:8],                       dim = rep(paste0(\"c_\", 1:2), 4)) t0 <- get_combis(test_df,                  word_field = \"words\",                  dims = \"dim\",                  max_overall = 5) #> Joining, by = \"rowid\" t1 <- get_combis(test_df,                  word_field = \"words\",                  dims = \"dim\",                  max_overall = 5,                  limit = 5) #> Joining, by = \"rowid\""},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":null,"dir":"Reference","previous_headings":"","what":"Vector representation of a corpus. — get_corpus_representation","title":"Vector representation of a corpus. — get_corpus_representation","text":"Returns average word-vector representation text column data frame, using fastText model.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vector representation of a corpus. — get_corpus_representation","text":"","code":"get_corpus_representation(df, model, text_field = \"text\", normalize = T)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vector representation of a corpus. — get_corpus_representation","text":"df data.frame column containing text identified text_field. model fastText model, loaded fastrtext::load_model(). text_field character string indicating name text column df. normalize Logical. Default TRUE. Normalize vectors Euclidean norm?","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vector representation of a corpus. — get_corpus_representation","text":"single-row sparse matrix class dgCMatrix returned Matrix.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Vector representation of a corpus. — get_corpus_representation","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                            \"tw_demo_model_sml.bin\",                                             package = \"dictvectoR\")) tw_annot <- tw_annot %>% head(15) %>% clean_text(text_field = \"full_text\") corpus_rep <- get_corpus_representation(tw_annot, model)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get occurrence frequency of words. — get_hits","title":"Get occurrence frequency of words. — get_hits","text":"Adds number occurrences word multi-word expression quanteda tokens object data.frame. default, checks 'hits' counted fills missing values. Works GLOB-style wildcards.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get occurrence frequency of words. — get_hits","text":"","code":"get_hits(word_df, tokens, word_field = \"words\", replace = F)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get occurrence frequency of words. — get_hits","text":"word_df data.frame containing words. tokens word-tokens object, returned quanteda::tokens. word_field Character. Default \"words\". Name column word_df contains words. replace Logical. Default FALSE. FALSE checks fills 'hits' missing observations. TRUE counts 'hits' words word_df.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get occurrence frequency of words. — get_hits","text":"data.frame column 'hits' indicating frequency term tokens. Arranged descending number hits.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get occurrence frequency of words. — get_hits","text":"","code":"tw_data %<>% head(100) %>% clean_text(text_field = 'full_text') toks <- quanteda::tokens(tw_data$text) word_df <- data.frame(words = c(\"der deutschen\", \"steuer*\", \"xyz\")) %>% get_hits(tokens = toks) #> [1] \"Counting word occurrences...\""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":null,"dir":"Reference","previous_headings":"","what":"Get F1 scores for many words or dictionaries. — get_many_F1s","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"Efficiently computes F1 scores elements vector containing keywords, list containing dictionaries, used DDR method.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"","code":"get_many_F1s(   words,   model,   df,   reference,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"words character vector containing keywords, list character vectors containing dictionaries. model fastText model, loaded fastrtext::load_model(). df data.frame containing one annotated document per row. reference Name binary reference column df (character). text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs DDR measurement. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"numerical F1 score returned element (.e. word dictionary) vector list. F1 scores indicate performance words/dictionaries predicting binary coding, used DDR method. resulting gradual measure DDR measure passed logistic regression, binary coding dependent variable. Binary predictions calculated logistic model compared binary coding. F1 score harmonic mean Recall Precision (1).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"(1) Chinchor, N. (1992). MUC-4 evaluation metrics. Proceedings 4th Conference Message Understanding, 22–29. https://doi.org/10.3115/1072064.1072067","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") dict_df <- data.frame(id = 1:3) dict_df$combis <- list(c(\"mehrheit deutschen\", \"merkel\", \"skandal\"),                       c(\"steuerzahler\", \"bundesregierung\",                       \"komplett gescheitert\"),                       c( \"arbeitnehmer\", \"groko\", \"wahnsinn\")) dict_df$F1 <- get_many_F1s(dict_df$combis,                            model = model,                            df = tw_annot,                            reference = \"pop\")"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"Efficiently computes F1 scores character vector keywords stored data.frame, list dictionaries stored data.frame - reference data.frame grouped group_field. ADD DETAILS....","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"","code":"get_many_F1s_by_group(   keyword_df,   keyword_field = \"words\",   id = \"id\",   model,   text_df,   group_field,   reference,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"keyword_df data.frame, containing column character vector words, list dictionaries. keyword_field character. name column keyword_df either character vector single keywords, list dictionaries, stored separate character vectors one element per word. id unique identifier keyword. model fastText model, loaded load_model. text_df data.frame containing one annotated document per row. group_field character. Name categorical grouping variable df. reference character. Name binary reference column df. text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs DDR measurement. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"#'@seealso cossim2dict, get_prediction, get_F1, get_many_RPFs, confusionMatrix","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") dict_df <- data.frame(id = 1:3) dict_df$combis <- list(c(\"mehrheit deutschen\", \"merkel\", \"skandal\"),                       c(\"steuerzahler\", \"bundesregierung\",                       \"komplett gescheitert\"),                       c( \"arbeitnehmer\", \"groko\", \"wahnsinn\")) get_many_F1s_by_group(keyword_df = dict_df,                      keyword_field = \"combis\",                      id = \"id\",                      model = model,                      text_df = tw_annot,                      group_field = \"party\",                      reference = 'pop') #> Joining, by = \"id\" #>   id                                              combis    F1_AfD F1_B90Grune #> 1  1                 mehrheit deutschen, merkel, skandal 0.6447368           0 #> 2  2 steuerzahler, bundesregierung, komplett gescheitert 0.6666667           0 #> 3  3                       arbeitnehmer, groko, wahnsinn 0.6953642           0 #>   F1_CDU F1_CSU F1_FDP  F1_Linke F1_SPD #> 1      0      0      0 0.0000000      0 #> 2      0      0      0 0.4210526      0 #> 3      0      0      0 0.3548387      0"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Recall, Precision, F1 for many — get_many_RPFs","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"Efficiently computes Recall, Precision, F1 scores character vector keywords stored data.frame, list dictionaries stored data.frame. Adds Recall, Precision, F1 data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"","code":"get_many_RPFs(   keyword_df,   keyword_field = \"words\",   model,   text_df,   reference,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"keyword_df data.frame, containing column character vector words, list dictionaries. keyword_field character. name column keyword_df either character vector single keywords, list dictionaries, stored separate character vectors one element per word. model fastText model, loaded load_model. text_df data.frame containing one annotated document per row. reference Name binary reference column df (character). text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs DDR measurement. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"element (.e. word dictionary) character vector list data.frame . F1 scores indicate performance words/dictionaries predicting binary coding, used DDR method. resulting gradual measure DDR measure passed logistic regression, binary coding dependent variable. Binary predictions calculated logistic model compared binary coding. F1 score harmonic mean Recall Precision (Chinchor, 1992).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"Chinchor, N. (1992). MUC-4 evaluation metrics. Proceedings 4th Conference Message Understanding, 22–29. https://doi.org/10.3115/1072064.1072067","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") dict_df <- data.frame(id = 1:3) dict_df$combis <- list(c(\"mehrheit deutschen\", \"merkel\", \"skandal\"),                       c(\"steuerzahler\", \"bundesregierung\",                       \"komplett gescheitert\"),                       c( \"arbeitnehmer\", \"groko\", \"wahnsinn\")) get_many_RPFs(keyword_df = dict_df,        keyword_field = \"combis\",        model = model,        text_df = tw_annot, reference = \"pop\") #> Joining, by = \"rowid\" #>   id                                              combis     recall precision #> 1  1                 mehrheit deutschen, merkel, skandal 0.08796296 0.5428571 #> 2  2 steuerzahler, bundesregierung, komplett gescheitert 0.16666667 0.6101695 #> 3  3                       arbeitnehmer, groko, wahnsinn 0.05092593 0.6111111 #>           F1 #> 1 0.15139442 #> 2 0.26181818 #> 3 0.09401709"},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Get binary prediction — get_prediction","title":"Get binary prediction — get_prediction","text":"Returns prediction binary variable gradual measurement using logistic regression.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get binary prediction — get_prediction","text":"","code":"get_prediction(data, dv, iv)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get binary prediction — get_prediction","text":"data data frame. dv binary variable data frame. iv gradual variable data frame used predicting dv.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get binary prediction — get_prediction","text":"factor levels 0, 1.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get binary prediction — get_prediction","text":"Predictions considered '1' predicted probability >= .5.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get binary prediction — get_prediction","text":"","code":"data(mtcars) mtcars$pred <- get_prediction(mtcars, mtcars$am, mtcars$drat)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":null,"dir":"Reference","previous_headings":"","what":"Get word representations. — get_word_representations","title":"Get word representations. — get_word_representations","text":"Wrapper around get_sentence_representation return fastText word-vector representation words multiwords, stored column data frame 'word_df.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get word representations. — get_word_representations","text":"","code":"get_word_representations(word_df, model, word_field = \"words\", normalize = T)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get word representations. — get_word_representations","text":"word_df data.frame containing column words multiword expressions. model fastText model, loaded fastrtext::load_model(). word_field character string indicating name column word_df contains words. normalize Logical. Default TRUE. Normalize vectors Euclidean norm?","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get word representations. — get_word_representations","text":"sparse matrix class dgCMatrix returned Matrix. number rows word_df number columns dimensions model.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get word representations. — get_word_representations","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                            \"tw_demo_model_sml.bin\",                                             package = \"dictvectoR\")) word_df <- data.frame(words = c(\"das ist\", \"ein\", \"test\")) word_rep <- get_word_representations(word_df, model)"},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize a vector — normalize","title":"Normalize a vector — normalize","text":"Normalizes (matrix) vector Euclidean norm, 'L2'-norm.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize a vector — normalize","text":"","code":"normalize(x)"},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize a vector — normalize","text":"x numeric vector, matrix vectors.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Normalize a vector — normalize","text":"norm defined $$||x|| = \\sqrt{\\sum(x^2)}$$ Euclidean norm can interpreted length connection zero point specified vector. normalized vector, , computed : $$x' = x/||x||$$","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize a vector — normalize","text":"","code":"v <- rnorm(10) normalize(v) == v/sqrt(sum(v^2)) #>  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE"},{"path":"https://thieled.github.io/dictvectoR/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://thieled.github.io/dictvectoR/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare text for fastText-model-training — prepare_train_data","title":"Prepare text for fastText-model-training — prepare_train_data","text":"Helper function prepare text training fastText model. (Caution: Tailored texts German language).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare text for fastText-model-training — prepare_train_data","text":"","code":"prepare_train_data(df, text_field = \"text\", seed = 1)"},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare text for fastText-model-training — prepare_train_data","text":"df data frame. text_field Name column df contains text cleaned. Default 'text'. seed Used random shuffling. Default 1.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare text for fastText-model-training — prepare_train_data","text":"character vector can directly used train fasttext model build_vectors.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare text for fastText-model-training — prepare_train_data","text":"Takes data.frame containing text. First checks length texts specified text_field using nsentence. Texts 3 sentences tokenized tokens sentences. texts passed clean_text fixed settings: tolower = T remove_punct = T replace_emojis = T replace_numbers = T remove_stopwords = F store_uncleaned = F count = T cleaned, short texts shuffled returned character vector.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare text for fastText-model-training — prepare_train_data","text":"","code":"texts <- prepare_train_data(head(tw_data, 10), text_field = 'full_text')"},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove too similar terms — remove_similar_words","title":"Remove too similar terms — remove_similar_words","text":"Removes similar terms data.frame word_df.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove too similar terms — remove_similar_words","text":"","code":"remove_similar_words(   word_df,   model,   word_field = \"words\",   compare_by = NULL,   compare_hits = T,   min_simil = 0.7,   win_threshold = 0.5 )"},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove too similar terms — remove_similar_words","text":"word_df data.frame containing column words multi-word expressions. model fastText model, loaded load_model. word_field character. name column word_df contains words. compare_by character. Default NULL. name column compared. compare_hits logical. Default TRUE. true counts often one word 'beats' similar regard occurrences. min_simil Numerical (0-1). Default .7. Similarity threshold. Word pairs threshold considered dissimilar. win_threshold Numerical (0-1). Default .5. Determines threshold drop words, defined proportion won pairwise comparisons; resp., compare_by compare_hit set, mean proportional wins.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove too similar terms — remove_similar_words","text":"data.frame. Containing pairwise similarity table similar words.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove too similar terms — remove_similar_words","text":"Detects similar words multiword expressions word_df, using fastText model. cosine similarity threshold set min_simil. requested, similar words compared along variable specified compare_by, /number occurrences stored variable named hits. threshold dropping terms set win_threshold. indicates proportion many pairwise comparisons 'won' word question. compare_hits = T compare_by set, mean proportions. word 'wins' comparisions regarding frequency (wins_hits1 == 1), loses comparisions regaring set score ('wins_score1' == 0), value .5. Forces 'word_df' contain unique identifier called 'word_id'; re-uses first variable named 'id' unique identifier.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove too similar terms — remove_similar_words","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                           \"tw_demo_model_sml.bin\",                                           package = \"dictvectoR\")) set.seed(1) word_df <- data.frame(words = c(\"unsere steuern\",                                \"steuerzahler\",                                \"unsere\",                                \"steuern\"),                      hits = c(2, 3, 15, 4),                      score = rnorm(4)) remove_similar_words(word_df,                     model,                     compare_by = \"score\",                     compare_hits = FALSE,                     win_threshold = .4) #>            words hits      score word_id #> 1 unsere steuern    2 -0.6264538       1 #> 2        steuern    4  1.5952808       4"},{"path":"https://thieled.github.io/dictvectoR/reference/repl_na.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace missing values. — repl_na","title":"Replace missing values. — repl_na","text":"Helper replace missing values. Default repalce NAs 'mean' - 1 'sd'.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/repl_na.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace missing values. — repl_na","text":"","code":"repl_na(x, replace_na = c(\"mean-sd\", \"min\", 0, F))"},{"path":"https://thieled.github.io/dictvectoR/reference/repl_na.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace missing values. — repl_na","text":"x numerical vector. replace_na Specifies value used replace NAs. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/repl_na.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Replace missing values. — repl_na","text":"","code":"a <- c(rnorm(7), NA, NA, NA) repl_na(a) == repl_na(a, 'mean-sd') #>  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE repl_na(a, 'min') #>  [1]  0.3295078 -0.8204684  0.4874291  0.7383247  0.5757814 -0.3053884 #>  [7]  1.5117812 -0.8204684 -0.8204684 -0.8204684 repl_na(a, 0) #>  [1]  0.3295078 -0.8204684  0.4874291  0.7383247  0.5757814 -0.3053884 #>  [7]  1.5117812  0.0000000  0.0000000  0.0000000 repl_na(a, FALSE) #>  [1]  0.3295078 -0.8204684  0.4874291  0.7383247  0.5757814 -0.3053884 #>  [7]  1.5117812         NA         NA         NA"},{"path":"https://thieled.github.io/dictvectoR/reference/simil_words2rep.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine similarity between words and a given vector. — simil_words2rep","title":"Cosine similarity between words and a given vector. — simil_words2rep","text":"Returns cosine similarity word data frame given vector representation.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/simil_words2rep.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine similarity between words and a given vector. — simil_words2rep","text":"","code":"simil_words2rep(word_df, word_field = \"words\", rep, model)"},{"path":"https://thieled.github.io/dictvectoR/reference/simil_words2rep.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine similarity between words and a given vector. — simil_words2rep","text":"word_df data.frame containing column words multiword expressions. word_field character string indicating name column word_df contains words. rep given word-vector representation, stored numerical vector, matrix, sparse matrix object. length, resp. ncol rep must equal dimensions used fastText model. model fastText model, loaded fastrtext::load_model().","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/simil_words2rep.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cosine similarity between words and a given vector. — simil_words2rep","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) pop_rep <- tw_annot %>%            dplyr::filter(pop == 1) %>%            clean_text(text_field = \"full_text\") %>%            get_corpus_representation(model = model) words_df <- data.frame(words = c(\"coronadeutschland\", \"skandal\")) words_df$popsimil <- simil_words2rep(words_df,                                      word_field = \"words\",                                      rep = pop_rep,                                      model)"},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":null,"dir":"Reference","previous_headings":"","what":"Annotated Tweets from German politicians. — tw_annot","title":"Annotated Tweets from German politicians. — tw_annot","text":"dataset containing 1,000 hand-coded Tweets, drawn [tw_data].","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Annotated Tweets from German politicians. — tw_annot","text":"","code":"tw_annot"},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Annotated Tweets from German politicians. — tw_annot","text":"data frame 20838 rows 8 variables: status_id Tweet ID ane Binary hand-coding anti-elitism ppc Binary hand-coding people-centrism pop Binary hand-coding populism. 1 either ane ppc 1 coder Coder identifier. B. AB Tweet parallel code. case, coding 1 least one coder decided code 1. user_id Twitter user ID twitter_handle Twitter handle party Political party followers_count number followers Aug 2022 created_at Date time Tweet created full_text Uncleaned Tweet rel_test Indicates Tweet parallel-coded reliability test ane_A Binary hand-coding anti-elitism coder ane_B Binary hand-coding anti-elitism coder B ppc_A Binary hand-coding people-centrism coder ppc_B Binary hand-coding people-centrism coder B","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Annotated Tweets from German politicians. — tw_annot","text":"dataset contains 1,000 Tweets drawn stratified random sample population data [tw_data]. political party represented (least) 120 Tweets. populist parties AfD (250 Tweets) Die Linke (150 Tweets) oversampled, anticipated populist content parties. TWo expert coders, one author package, hand-coded Tweets along two binary categories populist communication: Anti-elitism people-centrism. coding followed instructions documented online supplementary files Thiele (2022). 90 Tweets parallel-coded reliability testing, resulting Krippendorff's Alphas .86 anti-elitism, .71 people-centrism, documented variables ane_A, ane_B, ppc_A, ppc_B.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Annotated Tweets from German politicians. — tw_annot","text":"Thiele, D. (2022). Pandemic Populism? Covid-19 Triggered Populist Facebook User Comments Germany Austria. Politics Governance, 10(1), 185–196. https://doi.org/10.17645/pag.v10i1.4712","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Tweets from German politicians. — tw_data","title":"Tweets from German politicians. — tw_data","text":"dataset containing 20,838 Tweets 32 German politicians, posted 2020-03-11 2021-09-25.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tweets from German politicians. — tw_data","text":"","code":"tw_data"},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Tweets from German politicians. — tw_data","text":"data frame 20838 rows 8 variables: user_id Twitter user ID twitter_handle Twitter handle party Political party followers_count number followers Aug 2022 status_id Tweet ID created_at Date time Tweet created full_text Uncleaned text Tweet poppa_populism Mean populism score POPPA expert survey","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Tweets from German politicians. — tw_data","text":"Twitter, EPINet Twitter Dataset, POPPA","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tweets from German politicians. — tw_data","text":"Twitter accounts selected EPINetz Twitter Politicians Dataset 2021 (König et al., 2022). seven political parties represented German Parliament, five popular Twitter accounts politicians active federal level party representatives selected. accounts 30,000 followers selected. timeframe starts beginning Covid-19 pandemic ends one day German general elections 2021. account, maximum number (3,200) Tweets returned API v2 downloaded August 2022, using rtweet (Kearney, 2022). Quotes, re-tweets, Tweets outside timeframe excluded. Completeness dataset guaranteed. dataset also includes variable extracted POPPA Populism Political Parties Expert Survey, indicating mean expert rating populism per political party.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Tweets from German politicians. — tw_data","text":"Kearney, M. W., Sancho, L. R., Wickham, H., Heiss, ., Briatte, F., & Sidi, J. (2022). rtweet: Collecting Twitter Data. Retrieved https://CRAN.R-project.org/package=rtweet König, T., Schünemann, W. J., Brand, ., Freyberg, J., & Gertz, M. (2022). EPINetz Twitter Politicians Dataset 2021. New Resource Study German Twittersphere Application 2021 Federal Elections. Politische Vierteljahresschrift. https://doi.org/10.1007/s11615-022-00405-7 Meijers, M., & Zaslove, . (2020). Populism Political Parties Expert Survey 2018 (POPPA) (Data set). Harvard Dataverse. https://doi.org/10.7910/DVN/8NEL7B","code":""},{"path":"https://thieled.github.io/dictvectoR/news/index.html","id":"dictvector-0009000","dir":"Changelog","previous_headings":"","what":"dictvectoR 0.0.0.9000","title":"dictvectoR 0.0.0.9000","text":"Added NEWS.md file track changes package.","code":""}]
