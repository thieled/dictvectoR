[{"path":[]},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement [INSERT CONTACT METHOD]. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://thieled.github.io/dictvectoR/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.0, available https://www.contributor-covenant.org/version/2/0/ code_of_conduct.html. Community Impact Guidelines inspired Mozilla’s code conduct enforcement ladder. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https:// www.contributor-covenant.org/translations.","code":""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"from_text_to_measurement","text":"vignette guides workflow described Thiele (2022) applying ‘Distributed Dictionary Representation’ (DDR) Method (Garten et al., 2018) introduces functions dictvectoR package. DDR method provides continuous measurement concept dataset documents. measurement obtained calculating average word vector representation concept dictionary, representations document, calculating cosine similarity two vectors. detailed description, see Garten et al. (2018) Thiele (2022).","code":""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"from_text_to_measurement","text":"workflow described starts scratch, using textual data input, guides steps finding inductively list keywords, demonstrates evaluation application. requires population dataset containing textual data hand-coded sample drawn dataset, annotated presence theoretical concept. use built-dataset tw_data population data tw_annot annotated sample. vignette follows steps: Pre-process data Train fastText model Finding keywords Add multi-words Get F1 scores Drop similar terms Narrowing hand Get combinations terms. Evaluate performance combinations. Apply best performing dictionary. Face validity.","code":""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"pre-process-data","dir":"Articles","previous_headings":"","what":"Pre-process data","title":"from_text_to_measurement","text":"First load pre-process built textual data. clean_text cleans text tailored German-speaking texts social media. using text languages, may want adapt helper-function needs. prepare_train_data prepares textual data training fastText model. tokenizes longer documents sentences, shuffles , calls clean_text fixed settings. first prepare texts character vector training fastText model, clean text tw_data tw_annot later analyses.","code":"# Prepare text data texts <- prepare_train_data(tw_data, text_field = \"full_text\", seed = 42)  # Clean text in tw_data tw_data %<>% clean_text(remove_stopwords = T,                         text_field = \"full_text\")  # Clean text in tw_annot tw_annot %<>% clean_text(remove_stopwords = T,                          text_field = \"full_text\")"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"train-fasttext-model","dir":"Articles","previous_headings":"","what":"Train fastText model","title":"from_text_to_measurement","text":"Now, train fastText model using fastrtext::build_vectors. customized word vector model advantage maps words contexts actually appear studied material. important , use vocabulary model starting point conceptual dictionary. downside training model expensive regarding memory computation. used machine CPU @ 1.80GHz processor 8 cores 16 GB RAM. run memory limitations, consider decreasing dim, number dimensions. Decreasing number epochs speed learing process, decreasing bucket size reduce size resulting model. details, see Bojanowski et al. (2017). obtain model better quality, also consider increasing size textual data used training. example limited size data can feasibly shipped R-package. Alternatively, may consider using pre-trained model . code creates local folder ft_model user home directory, saves model two files: ft_model.bin ft_model.vec. (Note: code runs model file exists. training algorithm create perfectly identical word vector models, can cause problems re-running vignette.) Let’s load model use nearest-neighbor query check works properly. ‘Spahn’ name former German minister health:","code":"# Create local folder and set name for model dir.create(\"~/ft_model\", showWarnings = FALSE) model_file <- path.expand(\"~/ft_model/ft_model\")  # Train a fasttext model using the twitter data (if model does not yet exist)  if (!file.exists(paste0(model_file, \".bin\"))) {    fastrtext::build_vectors(texts, model_file, modeltype = c(\"skipgram\"),                               dim = 150, epoch = 10, bucket = 1e+6,  lr = 0.05,                               lrUpdateRate = 100, maxn = 7,  minn = 4, minCount = 3,                               t = 1e-04, thread = 8, ws= 6)   } #> writing tempfile at ... /tmp/RtmpLz35xX/file343734d9bf99 #> Starting training vectors with following commands:  #> $ skipgram -input /tmp/RtmpLz35xX/file343734d9bf99 -output /home/runner/ft_model/ft_model -bucket 1000000 -dim 150 -epoch 10 -label __label__ -loss ns -lr 0.05 -lrUpdateRate 100 -maxn 7 -minCount 3 -minn 4 -neg 5 -t 0.0001 -thread 8 -verbose 2 -wordNgrams 1 -ws 6 #>  Read 0M words #> Number of words:  13363 #> Number of labels: 0 #>  Progress:   0.2% words/sec/thread:   11417 lr:  0.049923 avg.loss:  4.148207 ETA:   0h 1m 4s Progress:   0.3% words/sec/thread:   11602 lr:  0.049843 avg.loss:  4.138947 ETA:   0h 1m 3s Progress:   0.5% words/sec/thread:   11592 lr:  0.049765 avg.loss:  4.132806 ETA:   0h 1m 3s Progress:   0.6% words/sec/thread:   11546 lr:  0.049688 avg.loss:  4.128905 ETA:   0h 1m 3s Progress:   0.8% words/sec/thread:   11566 lr:  0.049609 avg.loss:  4.124319 ETA:   0h 1m 3s Progress:   0.9% words/sec/thread:   11588 lr:  0.049530 avg.loss:  4.103191 ETA:   0h 1m 3s Progress:   1.1% words/sec/thread:   11593 lr:  0.049452 avg.loss:  4.034355 ETA:   0h 1m 3s Progress:   1.3% words/sec/thread:   11620 lr:  0.049372 avg.loss:  3.928089 ETA:   0h 1m 2s Progress:   1.4% words/sec/thread:   11639 lr:  0.049292 avg.loss:  3.807195 ETA:   0h 1m 2s Progress:   1.6% words/sec/thread:   11598 lr:  0.049216 avg.loss:  3.715662 ETA:   0h 1m 2s Progress:   1.7% words/sec/thread:   11645 lr:  0.049134 avg.loss:  3.600158 ETA:   0h 1m 2s Progress:   1.9% words/sec/thread:   11666 lr:  0.049054 avg.loss:  3.512057 ETA:   0h 1m 2s Progress:   2.1% words/sec/thread:   11677 lr:  0.048974 avg.loss:  3.424441 ETA:   0h 1m 2s Progress:   2.2% words/sec/thread:   11628 lr:  0.048900 avg.loss:  3.358880 ETA:   0h 1m 2s Progress:   2.4% words/sec/thread:   11636 lr:  0.048821 avg.loss:  3.290859 ETA:   0h 1m 2s Progress:   2.5% words/sec/thread:   11627 lr:  0.048743 avg.loss:  3.236943 ETA:   0h 1m 2s Progress:   2.7% words/sec/thread:   11605 lr:  0.048667 avg.loss:  3.191783 ETA:   0h 1m 2s Progress:   2.8% words/sec/thread:   11622 lr:  0.048586 avg.loss:  3.142794 ETA:   0h 1m 1s Progress:   3.0% words/sec/thread:   11645 lr:  0.048505 avg.loss:  3.096490 ETA:   0h 1m 1s Progress:   3.1% words/sec/thread:   11642 lr:  0.048427 avg.loss:  3.062898 ETA:   0h 1m 1s Progress:   3.3% words/sec/thread:   11666 lr:  0.048345 avg.loss:  3.033285 ETA:   0h 1m 1s Progress:   3.5% words/sec/thread:   11670 lr:  0.048265 avg.loss:  3.008168 ETA:   0h 1m 1s Progress:   3.6% words/sec/thread:   11685 lr:  0.048184 avg.loss:  2.984518 ETA:   0h 1m 1s Progress:   3.8% words/sec/thread:   11672 lr:  0.048107 avg.loss:  2.960892 ETA:   0h 1m 1s Progress:   3.9% words/sec/thread:   11680 lr:  0.048027 avg.loss:  2.938986 ETA:   0h 1m 0s Progress:   4.1% words/sec/thread:   11668 lr:  0.047950 avg.loss:  2.915595 ETA:   0h 1m 0s Progress:   4.2% words/sec/thread:   11643 lr:  0.047876 avg.loss:  2.897715 ETA:   0h 1m 0s Progress:   4.4% words/sec/thread:   11637 lr:  0.047798 avg.loss:  2.881764 ETA:   0h 1m 0s Progress:   4.6% words/sec/thread:   11652 lr:  0.047717 avg.loss:  2.869252 ETA:   0h 1m 0s Progress:   4.7% words/sec/thread:   11655 lr:  0.047637 avg.loss:  2.853956 ETA:   0h 1m 0s Progress:   4.9% words/sec/thread:   11659 lr:  0.047558 avg.loss:  2.841913 ETA:   0h 1m 0s Progress:   5.0% words/sec/thread:   11675 lr:  0.047475 avg.loss:  2.827499 ETA:   0h 1m 0s Progress:   5.2% words/sec/thread:   11676 lr:  0.047396 avg.loss:  2.817055 ETA:   0h 1m 0s Progress:   5.4% words/sec/thread:   11672 lr:  0.047318 avg.loss:  2.806159 ETA:   0h 1m 0s Progress:   5.5% words/sec/thread:   11673 lr:  0.047239 avg.loss:  2.795080 ETA:   0h 0m59s Progress:   5.7% words/sec/thread:   11679 lr:  0.047159 avg.loss:  2.785211 ETA:   0h 0m59s Progress:   5.8% words/sec/thread:   11684 lr:  0.047079 avg.loss:  2.773924 ETA:   0h 0m59s Progress:   6.0% words/sec/thread:   11691 lr:  0.046998 avg.loss:  2.763246 ETA:   0h 0m59s Progress:   6.2% words/sec/thread:   11693 lr:  0.046919 avg.loss:  2.755245 ETA:   0h 0m59s Progress:   6.3% words/sec/thread:   11686 lr:  0.046841 avg.loss:  2.748116 ETA:   0h 0m59s Progress:   6.5% words/sec/thread:   11691 lr:  0.046761 avg.loss:  2.739743 ETA:   0h 0m59s Progress:   6.6% words/sec/thread:   11704 lr:  0.046678 avg.loss:  2.732208 ETA:   0h 0m59s Progress:   6.8% words/sec/thread:   11709 lr:  0.046598 avg.loss:  2.724396 ETA:   0h 0m58s Progress:   7.0% words/sec/thread:   11699 lr:  0.046522 avg.loss:  2.717621 ETA:   0h 0m58s Progress:   7.1% words/sec/thread:   11695 lr:  0.046444 avg.loss:  2.710935 ETA:   0h 0m58s Progress:   7.3% words/sec/thread:   11685 lr:  0.046368 avg.loss:  2.703604 ETA:   0h 0m58s Progress:   7.4% words/sec/thread:   11693 lr:  0.046286 avg.loss:  2.698422 ETA:   0h 0m58s Progress:   7.6% words/sec/thread:   11685 lr:  0.046210 avg.loss:  2.692960 ETA:   0h 0m58s Progress:   7.7% words/sec/thread:   11681 lr:  0.046132 avg.loss:  2.687257 ETA:   0h 0m58s Progress:   7.9% words/sec/thread:   11677 lr:  0.046055 avg.loss:  2.683226 ETA:   0h 0m58s Progress:   8.0% words/sec/thread:   11674 lr:  0.045977 avg.loss:  2.677567 ETA:   0h 0m58s Progress:   8.2% words/sec/thread:   11675 lr:  0.045898 avg.loss:  2.671612 ETA:   0h 0m58s Progress:   8.4% words/sec/thread:   11683 lr:  0.045816 avg.loss:  2.668395 ETA:   0h 0m58s Progress:   8.5% words/sec/thread:   11682 lr:  0.045737 avg.loss:  2.662606 ETA:   0h 0m58s Progress:   8.7% words/sec/thread:   11682 lr:  0.045658 avg.loss:  2.658024 ETA:   0h 0m57s Progress:   8.8% words/sec/thread:   11685 lr:  0.045578 avg.loss:  2.654539 ETA:   0h 0m57s Progress:   9.0% words/sec/thread:   11688 lr:  0.045498 avg.loss:  2.650916 ETA:   0h 0m57s Progress:   9.2% words/sec/thread:   11687 lr:  0.045419 avg.loss:  2.646786 ETA:   0h 0m57s Progress:   9.3% words/sec/thread:   11684 lr:  0.045342 avg.loss:  2.642103 ETA:   0h 0m57s Progress:   9.5% words/sec/thread:   11686 lr:  0.045262 avg.loss:  2.639386 ETA:   0h 0m57s Progress:   9.6% words/sec/thread:   11692 lr:  0.045180 avg.loss:  2.635831 ETA:   0h 0m57s Progress:   9.8% words/sec/thread:   11695 lr:  0.045100 avg.loss:  2.632724 ETA:   0h 0m57s Progress:  10.0% words/sec/thread:   11696 lr:  0.045021 avg.loss:  2.629042 ETA:   0h 0m57s Progress:  10.1% words/sec/thread:   11689 lr:  0.044945 avg.loss:  2.624908 ETA:   0h 0m56s Progress:  10.3% words/sec/thread:   11691 lr:  0.044865 avg.loss:  2.621637 ETA:   0h 0m56s Progress:  10.4% words/sec/thread:   11689 lr:  0.044787 avg.loss:  2.618962 ETA:   0h 0m56s Progress:  10.6% words/sec/thread:   11688 lr:  0.044708 avg.loss:  2.615649 ETA:   0h 0m56s Progress:  10.7% words/sec/thread:   11690 lr:  0.044628 avg.loss:  2.611859 ETA:   0h 0m56s Progress:  10.9% words/sec/thread:   11686 lr:  0.044551 avg.loss:  2.610415 ETA:   0h 0m56s Progress:  11.1% words/sec/thread:   11685 lr:  0.044473 avg.loss:  2.608501 ETA:   0h 0m56s Progress:  11.2% words/sec/thread:   11678 lr:  0.044397 avg.loss:  2.605021 ETA:   0h 0m56s Progress:  11.4% words/sec/thread:   11683 lr:  0.044316 avg.loss:  2.601467 ETA:   0h 0m56s Progress:  11.5% words/sec/thread:   11681 lr:  0.044238 avg.loss:  2.599252 ETA:   0h 0m56s Progress:  11.7% words/sec/thread:   11681 lr:  0.044159 avg.loss:  2.596998 ETA:   0h 0m56s Progress:  11.8% words/sec/thread:   11676 lr:  0.044082 avg.loss:  2.595684 ETA:   0h 0m55s Progress:  12.0% words/sec/thread:   11676 lr:  0.044003 avg.loss:  2.592942 ETA:   0h 0m55s Progress:  12.1% words/sec/thread:   11673 lr:  0.043926 avg.loss:  2.590724 ETA:   0h 0m55s Progress:  12.3% words/sec/thread:   11674 lr:  0.043847 avg.loss:  2.589341 ETA:   0h 0m55s Progress:  12.5% words/sec/thread:   11674 lr:  0.043768 avg.loss:  2.587662 ETA:   0h 0m55s Progress:  12.6% words/sec/thread:   11675 lr:  0.043688 avg.loss:  2.586101 ETA:   0h 0m55s Progress:  12.8% words/sec/thread:   11678 lr:  0.043608 avg.loss:  2.584117 ETA:   0h 0m55s Progress:  12.9% words/sec/thread:   11676 lr:  0.043530 avg.loss:  2.582321 ETA:   0h 0m55s Progress:  13.1% words/sec/thread:   11674 lr:  0.043452 avg.loss:  2.580685 ETA:   0h 0m55s Progress:  13.3% words/sec/thread:   11676 lr:  0.043372 avg.loss:  2.578837 ETA:   0h 0m55s Progress:  13.4% words/sec/thread:   11676 lr:  0.043293 avg.loss:  2.576187 ETA:   0h 0m54s Progress:  13.6% words/sec/thread:   11669 lr:  0.043218 avg.loss:  2.574883 ETA:   0h 0m54s Progress:  13.7% words/sec/thread:   11668 lr:  0.043140 avg.loss:  2.573403 ETA:   0h 0m54s Progress:  13.9% words/sec/thread:   11665 lr:  0.043063 avg.loss:  2.571688 ETA:   0h 0m54s Progress:  14.0% words/sec/thread:   11665 lr:  0.042984 avg.loss:  2.570195 ETA:   0h 0m54s Progress:  14.2% words/sec/thread:   11665 lr:  0.042906 avg.loss:  2.568170 ETA:   0h 0m54s Progress:  14.3% words/sec/thread:   11667 lr:  0.042825 avg.loss:  2.566173 ETA:   0h 0m54s Progress:  14.5% words/sec/thread:   11667 lr:  0.042747 avg.loss:  2.564306 ETA:   0h 0m54s Progress:  14.7% words/sec/thread:   11665 lr:  0.042669 avg.loss:  2.562393 ETA:   0h 0m54s Progress:  14.8% words/sec/thread:   11666 lr:  0.042589 avg.loss:  2.560665 ETA:   0h 0m54s Progress:  15.0% words/sec/thread:   11663 lr:  0.042513 avg.loss:  2.559465 ETA:   0h 0m54s Progress:  15.1% words/sec/thread:   11663 lr:  0.042433 avg.loss:  2.557489 ETA:   0h 0m53s Progress:  15.3% words/sec/thread:   11664 lr:  0.042354 avg.loss:  2.556180 ETA:   0h 0m53s Progress:  15.5% words/sec/thread:   11666 lr:  0.042274 avg.loss:  2.555029 ETA:   0h 0m53s Progress:  15.6% words/sec/thread:   11666 lr:  0.042195 avg.loss:  2.553491 ETA:   0h 0m53s Progress:  15.8% words/sec/thread:   11665 lr:  0.042117 avg.loss:  2.551163 ETA:   0h 0m53s Progress:  15.9% words/sec/thread:   11663 lr:  0.042039 avg.loss:  2.550023 ETA:   0h 0m53s Progress:  16.1% words/sec/thread:   11668 lr:  0.041957 avg.loss:  2.548742 ETA:   0h 0m53s Progress:  16.2% words/sec/thread:   11670 lr:  0.041877 avg.loss:  2.547161 ETA:   0h 0m53s Progress:  16.4% words/sec/thread:   11669 lr:  0.041799 avg.loss:  2.545447 ETA:   0h 0m53s Progress:  16.6% words/sec/thread:   11669 lr:  0.041720 avg.loss:  2.544039 ETA:   0h 0m52s Progress:  16.7% words/sec/thread:   11665 lr:  0.041644 avg.loss:  2.542323 ETA:   0h 0m52s Progress:  16.9% words/sec/thread:   11668 lr:  0.041563 avg.loss:  2.540563 ETA:   0h 0m52s Progress:  17.0% words/sec/thread:   11671 lr:  0.041482 avg.loss:  2.539449 ETA:   0h 0m52s Progress:  17.2% words/sec/thread:   11671 lr:  0.041403 avg.loss:  2.537909 ETA:   0h 0m52s Progress:  17.4% words/sec/thread:   11673 lr:  0.041323 avg.loss:  2.537135 ETA:   0h 0m52s Progress:  17.5% words/sec/thread:   11667 lr:  0.041248 avg.loss:  2.535231 ETA:   0h 0m52s Progress:  17.7% words/sec/thread:   11671 lr:  0.041166 avg.loss:  2.533569 ETA:   0h 0m52s Progress:  17.8% words/sec/thread:   11671 lr:  0.041087 avg.loss:  2.531563 ETA:   0h 0m52s Progress:  18.0% words/sec/thread:   11675 lr:  0.041005 avg.loss:  2.530539 ETA:   0h 0m52s Progress:  18.1% words/sec/thread:   11671 lr:  0.040930 avg.loss:  2.529170 ETA:   0h 0m51s Progress:  18.3% words/sec/thread:   11676 lr:  0.040847 avg.loss:  2.527537 ETA:   0h 0m51s Progress:  18.7% words/sec/thread:   11653 lr:  0.040667 avg.loss:  2.524049 ETA:   0h 0m51s Progress:  18.8% words/sec/thread:   11652 lr:  0.040589 avg.loss:  2.522699 ETA:   0h 0m51s Progress:  19.0% words/sec/thread:   11657 lr:  0.040506 avg.loss:  2.522100 ETA:   0h 0m51s Progress:  19.1% words/sec/thread:   11660 lr:  0.040426 avg.loss:  2.520333 ETA:   0h 0m51s Progress:  19.3% words/sec/thread:   11660 lr:  0.040346 avg.loss:  2.518851 ETA:   0h 0m51s Progress:  19.5% words/sec/thread:   11663 lr:  0.040264 avg.loss:  2.517357 ETA:   0h 0m51s Progress:  19.6% words/sec/thread:   11663 lr:  0.040186 avg.loss:  2.515914 ETA:   0h 0m51s Progress:  19.8% words/sec/thread:   11659 lr:  0.040110 avg.loss:  2.514943 ETA:   0h 0m50s Progress:  19.9% words/sec/thread:   11662 lr:  0.040029 avg.loss:  2.513802 ETA:   0h 0m50s Progress:  20.1% words/sec/thread:   11661 lr:  0.039951 avg.loss:  2.512832 ETA:   0h 0m50s Progress:  20.3% words/sec/thread:   11664 lr:  0.039869 avg.loss:  2.511794 ETA:   0h 0m50s Progress:  21.9% words/sec/thread:   11552 lr:  0.039067 avg.loss:  2.501673 ETA:   0h 0m50s Progress:  22.0% words/sec/thread:   11556 lr:  0.038985 avg.loss:  2.500475 ETA:   0h 0m49s Progress:  22.2% words/sec/thread:   11557 lr:  0.038906 avg.loss:  2.499280 ETA:   0h 0m49s Progress:  22.3% words/sec/thread:   11559 lr:  0.038826 avg.loss:  2.498032 ETA:   0h 0m49s Progress:  22.5% words/sec/thread:   11561 lr:  0.038746 avg.loss:  2.496774 ETA:   0h 0m49s Progress:  22.7% words/sec/thread:   11562 lr:  0.038666 avg.loss:  2.496048 ETA:   0h 0m49s Progress:  22.8% words/sec/thread:   11561 lr:  0.038589 avg.loss:  2.494878 ETA:   0h 0m49s Progress:  23.0% words/sec/thread:   11561 lr:  0.038512 avg.loss:  2.494061 ETA:   0h 0m49s Progress:  23.1% words/sec/thread:   11561 lr:  0.038433 avg.loss:  2.492626 ETA:   0h 0m49s Progress:  23.3% words/sec/thread:   11559 lr:  0.038357 avg.loss:  2.491516 ETA:   0h 0m49s Progress:  23.4% words/sec/thread:   11562 lr:  0.038275 avg.loss:  2.490223 ETA:   0h 0m49s Progress:  23.6% words/sec/thread:   11562 lr:  0.038197 avg.loss:  2.489326 ETA:   0h 0m48s Progress:  23.8% words/sec/thread:   11563 lr:  0.038118 avg.loss:  2.488573 ETA:   0h 0m48s Progress:  23.9% words/sec/thread:   11567 lr:  0.038036 avg.loss:  2.487423 ETA:   0h 0m48s Progress:  24.1% words/sec/thread:   11567 lr:  0.037958 avg.loss:  2.486847 ETA:   0h 0m48s Progress:  24.2% words/sec/thread:   11565 lr:  0.037882 avg.loss:  2.485647 ETA:   0h 0m48s Progress:  24.4% words/sec/thread:   11567 lr:  0.037801 avg.loss:  2.484862 ETA:   0h 0m48s Progress:  24.6% words/sec/thread:   11568 lr:  0.037722 avg.loss:  2.484018 ETA:   0h 0m48s Progress:  24.7% words/sec/thread:   11570 lr:  0.037642 avg.loss:  2.483040 ETA:   0h 0m48s Progress:  24.9% words/sec/thread:   11574 lr:  0.037559 avg.loss:  2.481891 ETA:   0h 0m48s Progress:  25.0% words/sec/thread:   11576 lr:  0.037479 avg.loss:  2.480851 ETA:   0h 0m47s Progress:  25.2% words/sec/thread:   11576 lr:  0.037400 avg.loss:  2.480004 ETA:   0h 0m47s Progress:  25.4% words/sec/thread:   11577 lr:  0.037322 avg.loss:  2.478904 ETA:   0h 0m47s Progress:  25.5% words/sec/thread:   11577 lr:  0.037242 avg.loss:  2.478709 ETA:   0h 0m47s Progress:  25.7% words/sec/thread:   11582 lr:  0.037159 avg.loss:  2.477926 ETA:   0h 0m47s Progress:  25.8% words/sec/thread:   11580 lr:  0.037083 avg.loss:  2.477276 ETA:   0h 0m47s Progress:  26.0% words/sec/thread:   11581 lr:  0.037003 avg.loss:  2.476375 ETA:   0h 0m47s Progress:  26.2% words/sec/thread:   11581 lr:  0.036925 avg.loss:  2.475888 ETA:   0h 0m47s Progress:  26.3% words/sec/thread:   11584 lr:  0.036844 avg.loss:  2.474653 ETA:   0h 0m47s Progress:  26.5% words/sec/thread:   11581 lr:  0.036769 avg.loss:  2.473866 ETA:   0h 0m47s Progress:  26.6% words/sec/thread:   11582 lr:  0.036689 avg.loss:  2.473208 ETA:   0h 0m46s Progress:  26.8% words/sec/thread:   11583 lr:  0.036610 avg.loss:  2.472140 ETA:   0h 0m46s Progress:  26.9% words/sec/thread:   11581 lr:  0.036533 avg.loss:  2.471287 ETA:   0h 0m46s Progress:  27.1% words/sec/thread:   11584 lr:  0.036453 avg.loss:  2.469549 ETA:   0h 0m46s Progress:  27.3% words/sec/thread:   11583 lr:  0.036375 avg.loss:  2.468862 ETA:   0h 0m46s Progress:  27.4% words/sec/thread:   11586 lr:  0.036293 avg.loss:  2.468148 ETA:   0h 0m46s Progress:  27.6% words/sec/thread:   11586 lr:  0.036214 avg.loss:  2.466985 ETA:   0h 0m46s Progress:  27.7% words/sec/thread:   11584 lr:  0.036138 avg.loss:  2.466226 ETA:   0h 0m46s Progress:  27.9% words/sec/thread:   11585 lr:  0.036058 avg.loss:  2.464888 ETA:   0h 0m46s Progress:  28.0% words/sec/thread:   11585 lr:  0.035980 avg.loss:  2.464134 ETA:   0h 0m46s Progress:  28.2% words/sec/thread:   11585 lr:  0.035903 avg.loss:  2.463318 ETA:   0h 0m45s Progress:  28.4% words/sec/thread:   11589 lr:  0.035820 avg.loss:  2.462332 ETA:   0h 0m45s Progress:  28.5% words/sec/thread:   11589 lr:  0.035741 avg.loss:  2.461544 ETA:   0h 0m45s Progress:  28.7% words/sec/thread:   11592 lr:  0.035659 avg.loss:  2.460376 ETA:   0h 0m45s Progress:  28.8% words/sec/thread:   11591 lr:  0.035582 avg.loss:  2.459713 ETA:   0h 0m45s Progress:  29.0% words/sec/thread:   11592 lr:  0.035503 avg.loss:  2.458671 ETA:   0h 0m45s Progress:  29.2% words/sec/thread:   11592 lr:  0.035423 avg.loss:  2.457646 ETA:   0h 0m45s Progress:  29.3% words/sec/thread:   11592 lr:  0.035345 avg.loss:  2.456954 ETA:   0h 0m45s Progress:  29.5% words/sec/thread:   11594 lr:  0.035265 avg.loss:  2.456023 ETA:   0h 0m45s Progress:  29.6% words/sec/thread:   11596 lr:  0.035183 avg.loss:  2.455044 ETA:   0h 0m44s Progress:  29.8% words/sec/thread:   11595 lr:  0.035106 avg.loss:  2.454440 ETA:   0h 0m44s Progress:  29.9% words/sec/thread:   11597 lr:  0.035025 avg.loss:  2.454068 ETA:   0h 0m44s Progress:  30.1% words/sec/thread:   11598 lr:  0.034945 avg.loss:  2.453410 ETA:   0h 0m44s Progress:  30.3% words/sec/thread:   11600 lr:  0.034865 avg.loss:  2.452362 ETA:   0h 0m44s Progress:  30.4% words/sec/thread:   11601 lr:  0.034784 avg.loss:  2.451540 ETA:   0h 0m44s Progress:  30.6% words/sec/thread:   11603 lr:  0.034704 avg.loss:  2.450540 ETA:   0h 0m44s Progress:  30.7% words/sec/thread:   11599 lr:  0.034630 avg.loss:  2.449925 ETA:   0h 0m44s Progress:  30.9% words/sec/thread:   11600 lr:  0.034550 avg.loss:  2.449084 ETA:   0h 0m44s Progress:  31.1% words/sec/thread:   11599 lr:  0.034473 avg.loss:  2.448267 ETA:   0h 0m44s Progress:  31.2% words/sec/thread:   11600 lr:  0.034394 avg.loss:  2.447348 ETA:   0h 0m43s Progress:  31.4% words/sec/thread:   11600 lr:  0.034315 avg.loss:  2.446705 ETA:   0h 0m43s Progress:  31.5% words/sec/thread:   11603 lr:  0.034233 avg.loss:  2.446197 ETA:   0h 0m43s Progress:  31.7% words/sec/thread:   11605 lr:  0.034152 avg.loss:  2.445236 ETA:   0h 0m43s Progress:  31.9% words/sec/thread:   11604 lr:  0.034075 avg.loss:  2.444760 ETA:   0h 0m43s Progress:  32.0% words/sec/thread:   11603 lr:  0.033997 avg.loss:  2.444034 ETA:   0h 0m43s Progress:  32.2% words/sec/thread:   11604 lr:  0.033918 avg.loss:  2.443183 ETA:   0h 0m43s Progress:  32.3% words/sec/thread:   11603 lr:  0.033841 avg.loss:  2.442644 ETA:   0h 0m43s Progress:  32.5% words/sec/thread:   11603 lr:  0.033762 avg.loss:  2.442464 ETA:   0h 0m43s Progress:  32.6% words/sec/thread:   11604 lr:  0.033682 avg.loss:  2.441884 ETA:   0h 0m43s Progress:  32.8% words/sec/thread:   11601 lr:  0.033608 avg.loss:  2.441026 ETA:   0h 0m42s Progress:  32.9% words/sec/thread:   11602 lr:  0.033528 avg.loss:  2.440443 ETA:   0h 0m42s Progress:  33.1% words/sec/thread:   11601 lr:  0.033451 avg.loss:  2.439870 ETA:   0h 0m42s Progress:  33.3% words/sec/thread:   11602 lr:  0.033371 avg.loss:  2.438977 ETA:   0h 0m42s Progress:  33.4% words/sec/thread:   11603 lr:  0.033292 avg.loss:  2.438367 ETA:   0h 0m42s Progress:  33.6% words/sec/thread:   11604 lr:  0.033212 avg.loss:  2.437501 ETA:   0h 0m42s Progress:  33.7% words/sec/thread:   11603 lr:  0.033134 avg.loss:  2.436999 ETA:   0h 0m42s Progress:  33.9% words/sec/thread:   11600 lr:  0.033060 avg.loss:  2.436385 ETA:   0h 0m42s Progress:  34.0% words/sec/thread:   11603 lr:  0.032977 avg.loss:  2.435786 ETA:   0h 0m42s Progress:  34.2% words/sec/thread:   11605 lr:  0.032897 avg.loss:  2.435354 ETA:   0h 0m41s Progress:  34.4% words/sec/thread:   11606 lr:  0.032817 avg.loss:  2.434412 ETA:   0h 0m41s Progress:  34.5% words/sec/thread:   11605 lr:  0.032740 avg.loss:  2.433808 ETA:   0h 0m41s Progress:  34.7% words/sec/thread:   11605 lr:  0.032661 avg.loss:  2.433263 ETA:   0h 0m41s Progress:  34.8% words/sec/thread:   11605 lr:  0.032582 avg.loss:  2.432514 ETA:   0h 0m41s Progress:  35.0% words/sec/thread:   11608 lr:  0.032500 avg.loss:  2.431742 ETA:   0h 0m41s Progress:  35.2% words/sec/thread:   11608 lr:  0.032421 avg.loss:  2.431052 ETA:   0h 0m41s Progress:  35.3% words/sec/thread:   11608 lr:  0.032343 avg.loss:  2.430612 ETA:   0h 0m41s Progress:  35.5% words/sec/thread:   11605 lr:  0.032252 avg.loss:  2.429905 ETA:   0h 0m41s Progress:  35.7% words/sec/thread:   11605 lr:  0.032174 avg.loss:  2.429090 ETA:   0h 0m41s Progress:  35.8% words/sec/thread:   11604 lr:  0.032098 avg.loss:  2.428367 ETA:   0h 0m40s Progress:  36.0% words/sec/thread:   11604 lr:  0.032018 avg.loss:  2.427775 ETA:   0h 0m40s Progress:  36.1% words/sec/thread:   11603 lr:  0.031941 avg.loss:  2.427227 ETA:   0h 0m40s Progress:  36.3% words/sec/thread:   11601 lr:  0.031866 avg.loss:  2.426281 ETA:   0h 0m40s Progress:  36.4% words/sec/thread:   11603 lr:  0.031784 avg.loss:  2.425573 ETA:   0h 0m40s Progress:  36.6% words/sec/thread:   11603 lr:  0.031706 avg.loss:  2.425067 ETA:   0h 0m40s Progress:  36.7% words/sec/thread:   11603 lr:  0.031627 avg.loss:  2.424577 ETA:   0h 0m40s Progress:  36.9% words/sec/thread:   11602 lr:  0.031550 avg.loss:  2.423882 ETA:   0h 0m40s Progress:  37.1% words/sec/thread:   11602 lr:  0.031473 avg.loss:  2.423189 ETA:   0h 0m40s Progress:  37.2% words/sec/thread:   11599 lr:  0.031399 avg.loss:  2.422474 ETA:   0h 0m40s Progress:  37.4% words/sec/thread:   11600 lr:  0.031318 avg.loss:  2.422013 ETA:   0h 0m39s Progress:  37.5% words/sec/thread:   11600 lr:  0.031240 avg.loss:  2.421575 ETA:   0h 0m39s Progress:  37.7% words/sec/thread:   11601 lr:  0.031159 avg.loss:  2.420885 ETA:   0h 0m39s Progress:  37.8% words/sec/thread:   11603 lr:  0.031078 avg.loss:  2.420496 ETA:   0h 0m39s Progress:  38.0% words/sec/thread:   11604 lr:  0.030999 avg.loss:  2.420166 ETA:   0h 0m39s Progress:  38.2% words/sec/thread:   11604 lr:  0.030920 avg.loss:  2.419574 ETA:   0h 0m39s Progress:  38.3% words/sec/thread:   11599 lr:  0.030850 avg.loss:  2.418922 ETA:   0h 0m39s Progress:  38.5% words/sec/thread:   11597 lr:  0.030774 avg.loss:  2.417815 ETA:   0h 0m39s Progress:  38.6% words/sec/thread:   11595 lr:  0.030699 avg.loss:  2.416949 ETA:   0h 0m39s Progress:  38.7% words/sec/thread:   11591 lr:  0.030628 avg.loss:  2.416268 ETA:   0h 0m39s Progress:  38.9% words/sec/thread:   11587 lr:  0.030556 avg.loss:  2.415262 ETA:   0h 0m39s Progress:  39.1% words/sec/thread:   11590 lr:  0.030473 avg.loss:  2.414850 ETA:   0h 0m38s Progress:  39.2% words/sec/thread:   11591 lr:  0.030393 avg.loss:  2.414383 ETA:   0h 0m38s Progress:  39.4% words/sec/thread:   11589 lr:  0.030316 avg.loss:  2.413512 ETA:   0h 0m38s Progress:  39.5% words/sec/thread:   11590 lr:  0.030238 avg.loss:  2.412713 ETA:   0h 0m38s Progress:  39.7% words/sec/thread:   11592 lr:  0.030156 avg.loss:  2.411845 ETA:   0h 0m38s Progress:  39.8% words/sec/thread:   11592 lr:  0.030078 avg.loss:  2.411386 ETA:   0h 0m38s Progress:  40.0% words/sec/thread:   11591 lr:  0.030001 avg.loss:  2.410648 ETA:   0h 0m38s Progress:  40.2% words/sec/thread:   11592 lr:  0.029921 avg.loss:  2.410250 ETA:   0h 0m38s Progress:  40.3% words/sec/thread:   11592 lr:  0.029842 avg.loss:  2.409362 ETA:   0h 0m38s Progress:  40.5% words/sec/thread:   11591 lr:  0.029765 avg.loss:  2.408762 ETA:   0h 0m38s Progress:  40.6% words/sec/thread:   11591 lr:  0.029686 avg.loss:  2.408229 ETA:   0h 0m37s Progress:  40.8% words/sec/thread:   11594 lr:  0.029604 avg.loss:  2.407345 ETA:   0h 0m37s Progress:  41.0% words/sec/thread:   11596 lr:  0.029522 avg.loss:  2.406600 ETA:   0h 0m37s Progress:  41.1% words/sec/thread:   11596 lr:  0.029442 avg.loss:  2.405972 ETA:   0h 0m37s Progress:  41.3% words/sec/thread:   11598 lr:  0.029361 avg.loss:  2.404980 ETA:   0h 0m37s Progress:  41.4% words/sec/thread:   11599 lr:  0.029282 avg.loss:  2.404539 ETA:   0h 0m37s Progress:  41.6% words/sec/thread:   11598 lr:  0.029204 avg.loss:  2.403872 ETA:   0h 0m37s Progress:  41.8% words/sec/thread:   11599 lr:  0.029125 avg.loss:  2.403206 ETA:   0h 0m37s Progress:  41.9% words/sec/thread:   11599 lr:  0.029046 avg.loss:  2.402234 ETA:   0h 0m37s Progress:  42.1% words/sec/thread:   11600 lr:  0.028965 avg.loss:  2.401410 ETA:   0h 0m36s Progress:  42.2% words/sec/thread:   11600 lr:  0.028886 avg.loss:  2.400858 ETA:   0h 0m36s Progress:  42.4% words/sec/thread:   11600 lr:  0.028808 avg.loss:  2.400359 ETA:   0h 0m36s Progress:  42.5% words/sec/thread:   11601 lr:  0.028729 avg.loss:  2.399639 ETA:   0h 0m36s Progress:  42.7% words/sec/thread:   11601 lr:  0.028650 avg.loss:  2.398954 ETA:   0h 0m36s Progress:  42.9% words/sec/thread:   11602 lr:  0.028570 avg.loss:  2.398447 ETA:   0h 0m36s Progress:  43.0% words/sec/thread:   11602 lr:  0.028491 avg.loss:  2.397986 ETA:   0h 0m36s Progress:  43.2% words/sec/thread:   11602 lr:  0.028413 avg.loss:  2.397301 ETA:   0h 0m36s Progress:  43.3% words/sec/thread:   11601 lr:  0.028336 avg.loss:  2.396459 ETA:   0h 0m36s Progress:  43.5% words/sec/thread:   11603 lr:  0.028253 avg.loss:  2.396053 ETA:   0h 0m36s Progress:  43.7% words/sec/thread:   11604 lr:  0.028173 avg.loss:  2.395314 ETA:   0h 0m35s Progress:  43.8% words/sec/thread:   11605 lr:  0.028093 avg.loss:  2.394939 ETA:   0h 0m35s Progress:  44.0% words/sec/thread:   11604 lr:  0.028015 avg.loss:  2.394566 ETA:   0h 0m35s Progress:  44.1% words/sec/thread:   11606 lr:  0.027935 avg.loss:  2.393739 ETA:   0h 0m35s Progress:  44.3% words/sec/thread:   11607 lr:  0.027854 avg.loss:  2.393044 ETA:   0h 0m35s Progress:  44.5% words/sec/thread:   11609 lr:  0.027772 avg.loss:  2.392341 ETA:   0h 0m35s Progress:  44.6% words/sec/thread:   11608 lr:  0.027695 avg.loss:  2.391828 ETA:   0h 0m35s Progress:  44.8% words/sec/thread:   11608 lr:  0.027615 avg.loss:  2.391294 ETA:   0h 0m35s Progress:  44.9% words/sec/thread:   11608 lr:  0.027538 avg.loss:  2.390531 ETA:   0h 0m35s Progress:  45.1% words/sec/thread:   11607 lr:  0.027445 avg.loss:  2.389572 ETA:   0h 0m35s Progress:  45.3% words/sec/thread:   11607 lr:  0.027368 avg.loss:  2.388834 ETA:   0h 0m34s Progress:  45.4% words/sec/thread:   11608 lr:  0.027287 avg.loss:  2.387903 ETA:   0h 0m34s Progress:  45.6% words/sec/thread:   11608 lr:  0.027208 avg.loss:  2.387166 ETA:   0h 0m34s Progress:  45.7% words/sec/thread:   11608 lr:  0.027130 avg.loss:  2.386431 ETA:   0h 0m34s Progress:  45.9% words/sec/thread:   11608 lr:  0.027051 avg.loss:  2.385872 ETA:   0h 0m34s Progress:  46.1% words/sec/thread:   11608 lr:  0.026971 avg.loss:  2.385462 ETA:   0h 0m34s Progress:  46.2% words/sec/thread:   11610 lr:  0.026890 avg.loss:  2.384700 ETA:   0h 0m34s Progress:  46.4% words/sec/thread:   11610 lr:  0.026811 avg.loss:  2.384201 ETA:   0h 0m34s Progress:  46.5% words/sec/thread:   11610 lr:  0.026732 avg.loss:  2.383607 ETA:   0h 0m34s Progress:  46.7% words/sec/thread:   11609 lr:  0.026656 avg.loss:  2.383234 ETA:   0h 0m34s Progress:  46.8% words/sec/thread:   11610 lr:  0.026576 avg.loss:  2.383051 ETA:   0h 0m33s Progress:  47.0% words/sec/thread:   11610 lr:  0.026496 avg.loss:  2.382574 ETA:   0h 0m33s Progress:  47.2% words/sec/thread:   11611 lr:  0.026417 avg.loss:  2.381939 ETA:   0h 0m33s Progress:  47.3% words/sec/thread:   11611 lr:  0.026338 avg.loss:  2.381045 ETA:   0h 0m33s Progress:  47.5% words/sec/thread:   11611 lr:  0.026259 avg.loss:  2.380386 ETA:   0h 0m33s Progress:  47.6% words/sec/thread:   11612 lr:  0.026179 avg.loss:  2.379830 ETA:   0h 0m33s Progress:  47.8% words/sec/thread:   11610 lr:  0.026104 avg.loss:  2.379401 ETA:   0h 0m33s Progress:  47.9% words/sec/thread:   11609 lr:  0.026027 avg.loss:  2.378855 ETA:   0h 0m33s Progress:  48.1% words/sec/thread:   11611 lr:  0.025946 avg.loss:  2.378447 ETA:   0h 0m33s Progress:  48.3% words/sec/thread:   11611 lr:  0.025867 avg.loss:  2.377434 ETA:   0h 0m33s Progress:  48.4% words/sec/thread:   11613 lr:  0.025784 avg.loss:  2.376773 ETA:   0h 0m32s Progress:  48.6% words/sec/thread:   11613 lr:  0.025706 avg.loss:  2.376147 ETA:   0h 0m32s Progress:  48.8% words/sec/thread:   11615 lr:  0.025624 avg.loss:  2.375597 ETA:   0h 0m32s Progress:  48.9% words/sec/thread:   11614 lr:  0.025546 avg.loss:  2.374886 ETA:   0h 0m32s Progress:  49.1% words/sec/thread:   11615 lr:  0.025467 avg.loss:  2.374326 ETA:   0h 0m32s Progress:  49.2% words/sec/thread:   11616 lr:  0.025386 avg.loss:  2.373783 ETA:   0h 0m32s Progress:  49.4% words/sec/thread:   11618 lr:  0.025303 avg.loss:  2.373076 ETA:   0h 0m32s Progress:  49.5% words/sec/thread:   11617 lr:  0.025225 avg.loss:  2.372544 ETA:   0h 0m32s Progress:  49.7% words/sec/thread:   11617 lr:  0.025147 avg.loss:  2.371689 ETA:   0h 0m32s Progress:  49.9% words/sec/thread:   11618 lr:  0.025067 avg.loss:  2.370970 ETA:   0h 0m31s Progress:  50.0% words/sec/thread:   11619 lr:  0.024987 avg.loss:  2.370458 ETA:   0h 0m31s Progress:  50.2% words/sec/thread:   11620 lr:  0.024905 avg.loss:  2.369923 ETA:   0h 0m31s Progress:  50.4% words/sec/thread:   11622 lr:  0.024821 avg.loss:  2.369140 ETA:   0h 0m31s Progress:  50.5% words/sec/thread:   11622 lr:  0.024743 avg.loss:  2.368979 ETA:   0h 0m31s Progress:  50.7% words/sec/thread:   11623 lr:  0.024663 avg.loss:  2.368332 ETA:   0h 0m31s Progress:  50.8% words/sec/thread:   11624 lr:  0.024583 avg.loss:  2.367782 ETA:   0h 0m31s Progress:  51.0% words/sec/thread:   11624 lr:  0.024503 avg.loss:  2.367207 ETA:   0h 0m31s Progress:  51.1% words/sec/thread:   11624 lr:  0.024425 avg.loss:  2.366981 ETA:   0h 0m31s Progress:  51.3% words/sec/thread:   11623 lr:  0.024348 avg.loss:  2.366646 ETA:   0h 0m31s Progress:  51.5% words/sec/thread:   11622 lr:  0.024272 avg.loss:  2.366201 ETA:   0h 0m30s Progress:  51.6% words/sec/thread:   11623 lr:  0.024192 avg.loss:  2.365878 ETA:   0h 0m30s Progress:  51.8% words/sec/thread:   11623 lr:  0.024113 avg.loss:  2.365175 ETA:   0h 0m30s Progress:  51.9% words/sec/thread:   11624 lr:  0.024032 avg.loss:  2.364876 ETA:   0h 0m30s Progress:  52.1% words/sec/thread:   11623 lr:  0.023954 avg.loss:  2.364211 ETA:   0h 0m30s Progress:  52.2% words/sec/thread:   11624 lr:  0.023875 avg.loss:  2.363674 ETA:   0h 0m30s Progress:  52.4% words/sec/thread:   11625 lr:  0.023793 avg.loss:  2.363267 ETA:   0h 0m30s Progress:  52.6% words/sec/thread:   11624 lr:  0.023716 avg.loss:  2.362787 ETA:   0h 0m30s Progress:  52.7% words/sec/thread:   11625 lr:  0.023636 avg.loss:  2.362106 ETA:   0h 0m30s Progress:  52.9% words/sec/thread:   11624 lr:  0.023554 avg.loss:  2.361494 ETA:   0h 0m30s Progress:  53.1% words/sec/thread:   11624 lr:  0.023475 avg.loss:  2.361082 ETA:   0h 0m29s Progress:  53.2% words/sec/thread:   11624 lr:  0.023396 avg.loss:  2.360641 ETA:   0h 0m29s Progress:  53.4% words/sec/thread:   11626 lr:  0.023314 avg.loss:  2.360121 ETA:   0h 0m29s Progress:  53.5% words/sec/thread:   11626 lr:  0.023235 avg.loss:  2.359610 ETA:   0h 0m29s Progress:  53.7% words/sec/thread:   11627 lr:  0.023154 avg.loss:  2.359126 ETA:   0h 0m29s Progress:  53.9% words/sec/thread:   11628 lr:  0.023073 avg.loss:  2.358563 ETA:   0h 0m29s Progress:  54.0% words/sec/thread:   11628 lr:  0.022993 avg.loss:  2.357982 ETA:   0h 0m29s Progress:  54.2% words/sec/thread:   11629 lr:  0.022912 avg.loss:  2.357215 ETA:   0h 0m29s Progress:  54.3% words/sec/thread:   11628 lr:  0.022837 avg.loss:  2.356744 ETA:   0h 0m29s Progress:  54.5% words/sec/thread:   11629 lr:  0.022756 avg.loss:  2.356143 ETA:   0h 0m28s Progress:  54.6% words/sec/thread:   11628 lr:  0.022679 avg.loss:  2.355710 ETA:   0h 0m28s Progress:  54.8% words/sec/thread:   11629 lr:  0.022597 avg.loss:  2.355229 ETA:   0h 0m28s Progress:  55.0% words/sec/thread:   11630 lr:  0.022518 avg.loss:  2.354713 ETA:   0h 0m28s Progress:  55.1% words/sec/thread:   11629 lr:  0.022440 avg.loss:  2.354267 ETA:   0h 0m28s Progress:  55.3% words/sec/thread:   11629 lr:  0.022364 avg.loss:  2.353825 ETA:   0h 0m28s Progress:  55.4% words/sec/thread:   11629 lr:  0.022285 avg.loss:  2.353316 ETA:   0h 0m28s Progress:  55.6% words/sec/thread:   11629 lr:  0.022205 avg.loss:  2.352619 ETA:   0h 0m28s Progress:  55.8% words/sec/thread:   11631 lr:  0.022122 avg.loss:  2.352029 ETA:   0h 0m28s Progress:  55.9% words/sec/thread:   11630 lr:  0.022046 avg.loss:  2.351584 ETA:   0h 0m28s Progress:  56.1% words/sec/thread:   11631 lr:  0.021965 avg.loss:  2.351020 ETA:   0h 0m27s Progress:  56.2% words/sec/thread:   11630 lr:  0.021887 avg.loss:  2.350657 ETA:   0h 0m27s Progress:  56.4% words/sec/thread:   11630 lr:  0.021809 avg.loss:  2.350268 ETA:   0h 0m27s Progress:  56.5% words/sec/thread:   11630 lr:  0.021730 avg.loss:  2.349701 ETA:   0h 0m27s Progress:  56.7% words/sec/thread:   11632 lr:  0.021648 avg.loss:  2.349410 ETA:   0h 0m27s Progress:  56.9% words/sec/thread:   11633 lr:  0.021566 avg.loss:  2.348817 ETA:   0h 0m27s Progress:  57.0% words/sec/thread:   11632 lr:  0.021490 avg.loss:  2.348437 ETA:   0h 0m27s Progress:  57.2% words/sec/thread:   11632 lr:  0.021412 avg.loss:  2.348018 ETA:   0h 0m27s Progress:  57.3% words/sec/thread:   11632 lr:  0.021333 avg.loss:  2.347426 ETA:   0h 0m27s Progress:  57.5% words/sec/thread:   11632 lr:  0.021253 avg.loss:  2.347232 ETA:   0h 0m27s Progress:  57.7% words/sec/thread:   11633 lr:  0.021172 avg.loss:  2.346699 ETA:   0h 0m26s Progress:  57.8% words/sec/thread:   11634 lr:  0.021093 avg.loss:  2.346245 ETA:   0h 0m26s Progress:  58.0% words/sec/thread:   11633 lr:  0.021014 avg.loss:  2.345599 ETA:   0h 0m26s Progress:  58.1% words/sec/thread:   11632 lr:  0.020939 avg.loss:  2.344738 ETA:   0h 0m26s Progress:  58.3% words/sec/thread:   11634 lr:  0.020856 avg.loss:  2.344502 ETA:   0h 0m26s Progress:  58.4% words/sec/thread:   11634 lr:  0.020776 avg.loss:  2.344160 ETA:   0h 0m26s Progress:  58.6% words/sec/thread:   11634 lr:  0.020698 avg.loss:  2.343595 ETA:   0h 0m26s Progress:  58.8% words/sec/thread:   11635 lr:  0.020616 avg.loss:  2.343081 ETA:   0h 0m26s Progress:  58.9% words/sec/thread:   11635 lr:  0.020537 avg.loss:  2.342532 ETA:   0h 0m26s Progress:  59.1% words/sec/thread:   11637 lr:  0.020455 avg.loss:  2.342156 ETA:   0h 0m26s Progress:  59.2% words/sec/thread:   11637 lr:  0.020375 avg.loss:  2.341850 ETA:   0h 0m25s Progress:  59.4% words/sec/thread:   11637 lr:  0.020298 avg.loss:  2.341303 ETA:   0h 0m25s Progress:  59.6% words/sec/thread:   11637 lr:  0.020218 avg.loss:  2.340841 ETA:   0h 0m25s Progress:  59.7% words/sec/thread:   11637 lr:  0.020132 avg.loss:  2.340550 ETA:   0h 0m25s Progress:  59.9% words/sec/thread:   11637 lr:  0.020054 avg.loss:  2.340226 ETA:   0h 0m25s Progress:  60.1% words/sec/thread:   11638 lr:  0.019974 avg.loss:  2.339577 ETA:   0h 0m25s Progress:  60.2% words/sec/thread:   11638 lr:  0.019894 avg.loss:  2.339117 ETA:   0h 0m25s Progress:  60.4% words/sec/thread:   11639 lr:  0.019813 avg.loss:  2.338922 ETA:   0h 0m25s Progress:  60.5% words/sec/thread:   11639 lr:  0.019733 avg.loss:  2.338490 ETA:   0h 0m25s Progress:  60.7% words/sec/thread:   11639 lr:  0.019655 avg.loss:  2.338187 ETA:   0h 0m25s Progress:  60.8% words/sec/thread:   11639 lr:  0.019576 avg.loss:  2.337526 ETA:   0h 0m24s Progress:  61.0% words/sec/thread:   11638 lr:  0.019500 avg.loss:  2.337049 ETA:   0h 0m24s Progress:  61.2% words/sec/thread:   11638 lr:  0.019422 avg.loss:  2.336602 ETA:   0h 0m24s Progress:  61.3% words/sec/thread:   11639 lr:  0.019341 avg.loss:  2.336069 ETA:   0h 0m24s Progress:  61.5% words/sec/thread:   11640 lr:  0.019260 avg.loss:  2.335845 ETA:   0h 0m24s Progress:  61.6% words/sec/thread:   11640 lr:  0.019180 avg.loss:  2.335431 ETA:   0h 0m24s Progress:  61.8% words/sec/thread:   11640 lr:  0.019102 avg.loss:  2.335017 ETA:   0h 0m24s Progress:  62.0% words/sec/thread:   11640 lr:  0.019023 avg.loss:  2.334554 ETA:   0h 0m24s Progress:  62.1% words/sec/thread:   11640 lr:  0.018944 avg.loss:  2.334130 ETA:   0h 0m24s Progress:  62.3% words/sec/thread:   11641 lr:  0.018863 avg.loss:  2.333798 ETA:   0h 0m24s Progress:  62.4% words/sec/thread:   11641 lr:  0.018783 avg.loss:  2.333322 ETA:   0h 0m23s Progress:  62.6% words/sec/thread:   11644 lr:  0.018698 avg.loss:  2.333021 ETA:   0h 0m23s Progress:  62.8% words/sec/thread:   11643 lr:  0.018622 avg.loss:  2.332547 ETA:   0h 0m23s Progress:  62.9% words/sec/thread:   11642 lr:  0.018545 avg.loss:  2.331916 ETA:   0h 0m23s Progress:  63.1% words/sec/thread:   11644 lr:  0.018462 avg.loss:  2.331480 ETA:   0h 0m23s Progress:  63.2% words/sec/thread:   11643 lr:  0.018385 avg.loss:  2.331003 ETA:   0h 0m23s Progress:  63.4% words/sec/thread:   11644 lr:  0.018304 avg.loss:  2.330558 ETA:   0h 0m23s Progress:  63.6% words/sec/thread:   11644 lr:  0.018223 avg.loss:  2.330102 ETA:   0h 0m23s Progress:  63.7% words/sec/thread:   11644 lr:  0.018146 avg.loss:  2.329626 ETA:   0h 0m23s Progress:  63.9% words/sec/thread:   11645 lr:  0.018066 avg.loss:  2.329133 ETA:   0h 0m22s Progress:  64.0% words/sec/thread:   11643 lr:  0.017990 avg.loss:  2.328452 ETA:   0h 0m22s Progress:  64.2% words/sec/thread:   11643 lr:  0.017912 avg.loss:  2.327962 ETA:   0h 0m22s Progress:  64.3% words/sec/thread:   11644 lr:  0.017831 avg.loss:  2.327657 ETA:   0h 0m22s Progress:  64.5% words/sec/thread:   11643 lr:  0.017756 avg.loss:  2.327385 ETA:   0h 0m22s Progress:  64.6% words/sec/thread:   11642 lr:  0.017678 avg.loss:  2.326898 ETA:   0h 0m22s Progress:  64.8% words/sec/thread:   11643 lr:  0.017598 avg.loss:  2.326378 ETA:   0h 0m22s Progress:  65.0% words/sec/thread:   11643 lr:  0.017518 avg.loss:  2.325950 ETA:   0h 0m22s Progress:  65.1% words/sec/thread:   11643 lr:  0.017441 avg.loss:  2.325373 ETA:   0h 0m22s Progress:  65.3% words/sec/thread:   11644 lr:  0.017357 avg.loss:  2.324916 ETA:   0h 0m22s Progress:  65.4% words/sec/thread:   11644 lr:  0.017281 avg.loss:  2.324518 ETA:   0h 0m21s Progress:  65.6% words/sec/thread:   11643 lr:  0.017203 avg.loss:  2.323971 ETA:   0h 0m21s Progress:  65.8% words/sec/thread:   11642 lr:  0.017124 avg.loss:  2.323538 ETA:   0h 0m21s Progress:  65.9% words/sec/thread:   11643 lr:  0.017034 avg.loss:  2.322948 ETA:   0h 0m21s Progress:  66.1% words/sec/thread:   11643 lr:  0.016954 avg.loss:  2.322596 ETA:   0h 0m21s Progress:  66.2% words/sec/thread:   11643 lr:  0.016877 avg.loss:  2.322443 ETA:   0h 0m21s Progress:  66.4% words/sec/thread:   11642 lr:  0.016800 avg.loss:  2.321992 ETA:   0h 0m21s Progress:  66.6% words/sec/thread:   11643 lr:  0.016718 avg.loss:  2.321217 ETA:   0h 0m21s Progress:  66.7% words/sec/thread:   11642 lr:  0.016641 avg.loss:  2.320733 ETA:   0h 0m21s Progress:  66.9% words/sec/thread:   11643 lr:  0.016561 avg.loss:  2.320330 ETA:   0h 0m21s Progress:  67.0% words/sec/thread:   11644 lr:  0.016479 avg.loss:  2.319771 ETA:   0h 0m20s Progress:  67.2% words/sec/thread:   11644 lr:  0.016402 avg.loss:  2.319230 ETA:   0h 0m20s Progress:  67.4% words/sec/thread:   11644 lr:  0.016322 avg.loss:  2.318753 ETA:   0h 0m20s Progress:  67.5% words/sec/thread:   11645 lr:  0.016241 avg.loss:  2.318244 ETA:   0h 0m20s Progress:  67.7% words/sec/thread:   11645 lr:  0.016161 avg.loss:  2.317935 ETA:   0h 0m20s Progress:  67.8% words/sec/thread:   11640 lr:  0.016098 avg.loss:  2.317580 ETA:   0h 0m20s Progress:  68.0% words/sec/thread:   11640 lr:  0.016019 avg.loss:  2.317100 ETA:   0h 0m20s Progress:  68.1% words/sec/thread:   11640 lr:  0.015941 avg.loss:  2.316610 ETA:   0h 0m20s Progress:  68.3% words/sec/thread:   11641 lr:  0.015858 avg.loss:  2.316208 ETA:   0h 0m20s Progress:  68.4% words/sec/thread:   11640 lr:  0.015781 avg.loss:  2.315801 ETA:   0h 0m20s Progress:  68.6% words/sec/thread:   11641 lr:  0.015702 avg.loss:  2.315236 ETA:   0h 0m19s Progress:  68.8% words/sec/thread:   11641 lr:  0.015623 avg.loss:  2.314827 ETA:   0h 0m19s Progress:  68.9% words/sec/thread:   11641 lr:  0.015544 avg.loss:  2.314425 ETA:   0h 0m19s Progress:  69.1% words/sec/thread:   11641 lr:  0.015463 avg.loss:  2.314010 ETA:   0h 0m19s Progress:  69.2% words/sec/thread:   11642 lr:  0.015383 avg.loss:  2.313357 ETA:   0h 0m19s Progress:  69.4% words/sec/thread:   11643 lr:  0.015302 avg.loss:  2.313132 ETA:   0h 0m19s Progress:  69.5% words/sec/thread:   11641 lr:  0.015228 avg.loss:  2.312726 ETA:   0h 0m19s Progress:  69.7% words/sec/thread:   11642 lr:  0.015147 avg.loss:  2.312280 ETA:   0h 0m19s Progress:  69.9% words/sec/thread:   11642 lr:  0.015069 avg.loss:  2.311963 ETA:   0h 0m19s Progress:  70.0% words/sec/thread:   11641 lr:  0.014990 avg.loss:  2.311473 ETA:   0h 0m19s Progress:  70.2% words/sec/thread:   11642 lr:  0.014909 avg.loss:  2.311067 ETA:   0h 0m18s Progress:  70.3% words/sec/thread:   11643 lr:  0.014829 avg.loss:  2.310694 ETA:   0h 0m18s Progress:  70.5% words/sec/thread:   11642 lr:  0.014752 avg.loss:  2.310179 ETA:   0h 0m18s Progress:  70.7% words/sec/thread:   11643 lr:  0.014672 avg.loss:  2.309730 ETA:   0h 0m18s Progress:  70.8% words/sec/thread:   11642 lr:  0.014595 avg.loss:  2.309414 ETA:   0h 0m18s Progress:  71.0% words/sec/thread:   11642 lr:  0.014516 avg.loss:  2.309111 ETA:   0h 0m18s Progress:  71.1% words/sec/thread:   11642 lr:  0.014438 avg.loss:  2.308533 ETA:   0h 0m18s Progress:  71.3% words/sec/thread:   11641 lr:  0.014356 avg.loss:  2.308166 ETA:   0h 0m18s Progress:  71.5% words/sec/thread:   11642 lr:  0.014275 avg.loss:  2.307759 ETA:   0h 0m18s Progress:  71.6% words/sec/thread:   11642 lr:  0.014196 avg.loss:  2.307462 ETA:   0h 0m18s Progress:  71.8% words/sec/thread:   11643 lr:  0.014114 avg.loss:  2.307142 ETA:   0h 0m17s Progress:  71.9% words/sec/thread:   11644 lr:  0.014032 avg.loss:  2.306728 ETA:   0h 0m17s Progress:  72.1% words/sec/thread:   11643 lr:  0.013956 avg.loss:  2.306397 ETA:   0h 0m17s Progress:  72.3% words/sec/thread:   11644 lr:  0.013875 avg.loss:  2.306126 ETA:   0h 0m17s Progress:  72.4% words/sec/thread:   11643 lr:  0.013798 avg.loss:  2.305632 ETA:   0h 0m17s Progress:  72.6% words/sec/thread:   11643 lr:  0.013719 avg.loss:  2.305255 ETA:   0h 0m17s Progress:  72.7% words/sec/thread:   11643 lr:  0.013640 avg.loss:  2.304875 ETA:   0h 0m17s Progress:  72.9% words/sec/thread:   11643 lr:  0.013562 avg.loss:  2.304406 ETA:   0h 0m17s Progress:  73.0% words/sec/thread:   11643 lr:  0.013482 avg.loss:  2.303897 ETA:   0h 0m17s Progress:  73.2% words/sec/thread:   11643 lr:  0.013403 avg.loss:  2.303625 ETA:   0h 0m17s Progress:  73.3% words/sec/thread:   11643 lr:  0.013325 avg.loss:  2.303211 ETA:   0h 0m16s Progress:  73.5% words/sec/thread:   11643 lr:  0.013246 avg.loss:  2.302781 ETA:   0h 0m16s Progress:  73.7% words/sec/thread:   11643 lr:  0.013167 avg.loss:  2.302429 ETA:   0h 0m16s Progress:  73.8% words/sec/thread:   11643 lr:  0.013090 avg.loss:  2.302019 ETA:   0h 0m16s Progress:  74.0% words/sec/thread:   11643 lr:  0.013009 avg.loss:  2.301583 ETA:   0h 0m16s Progress:  74.1% words/sec/thread:   11643 lr:  0.012932 avg.loss:  2.301134 ETA:   0h 0m16s Progress:  74.3% words/sec/thread:   11644 lr:  0.012852 avg.loss:  2.300781 ETA:   0h 0m16s Progress:  74.5% words/sec/thread:   11643 lr:  0.012773 avg.loss:  2.300367 ETA:   0h 0m16s Progress:  74.6% words/sec/thread:   11644 lr:  0.012693 avg.loss:  2.299901 ETA:   0h 0m16s Progress:  74.8% words/sec/thread:   11644 lr:  0.012613 avg.loss:  2.299645 ETA:   0h 0m16s Progress:  74.9% words/sec/thread:   11645 lr:  0.012533 avg.loss:  2.299198 ETA:   0h 0m15s Progress:  75.1% words/sec/thread:   11645 lr:  0.012452 avg.loss:  2.298900 ETA:   0h 0m15s Progress:  75.3% words/sec/thread:   11645 lr:  0.012374 avg.loss:  2.298580 ETA:   0h 0m15s Progress:  75.4% words/sec/thread:   11645 lr:  0.012296 avg.loss:  2.298225 ETA:   0h 0m15s Progress:  75.6% words/sec/thread:   11642 lr:  0.012225 avg.loss:  2.297856 ETA:   0h 0m15s Progress:  75.7% words/sec/thread:   11639 lr:  0.012155 avg.loss:  2.297392 ETA:   0h 0m15s Progress:  75.8% words/sec/thread:   11638 lr:  0.012082 avg.loss:  2.296871 ETA:   0h 0m15s Progress:  76.0% words/sec/thread:   11635 lr:  0.012013 avg.loss:  2.296561 ETA:   0h 0m15s Progress:  76.1% words/sec/thread:   11633 lr:  0.011939 avg.loss:  2.296169 ETA:   0h 0m15s Progress:  76.3% words/sec/thread:   11632 lr:  0.011864 avg.loss:  2.295740 ETA:   0h 0m15s Progress:  76.4% words/sec/thread:   11631 lr:  0.011783 avg.loss:  2.295393 ETA:   0h 0m15s Progress:  76.6% words/sec/thread:   11631 lr:  0.011703 avg.loss:  2.295002 ETA:   0h 0m14s Progress:  76.8% words/sec/thread:   11632 lr:  0.011621 avg.loss:  2.294554 ETA:   0h 0m14s Progress:  76.9% words/sec/thread:   11633 lr:  0.011542 avg.loss:  2.294139 ETA:   0h 0m14s Progress:  77.1% words/sec/thread:   11631 lr:  0.011467 avg.loss:  2.293628 ETA:   0h 0m14s Progress:  77.2% words/sec/thread:   11632 lr:  0.011388 avg.loss:  2.293364 ETA:   0h 0m14s Progress:  77.4% words/sec/thread:   11633 lr:  0.011305 avg.loss:  2.292841 ETA:   0h 0m14s Progress:  77.5% words/sec/thread:   11632 lr:  0.011228 avg.loss:  2.292347 ETA:   0h 0m14s Progress:  77.7% words/sec/thread:   11633 lr:  0.011146 avg.loss:  2.291829 ETA:   0h 0m14s Progress:  77.9% words/sec/thread:   11633 lr:  0.011067 avg.loss:  2.291723 ETA:   0h 0m14s Progress:  78.0% words/sec/thread:   11633 lr:  0.010988 avg.loss:  2.291246 ETA:   0h 0m13s Progress:  78.2% words/sec/thread:   11634 lr:  0.010907 avg.loss:  2.290827 ETA:   0h 0m13s Progress:  78.3% words/sec/thread:   11633 lr:  0.010831 avg.loss:  2.290365 ETA:   0h 0m13s Progress:  78.5% words/sec/thread:   11633 lr:  0.010752 avg.loss:  2.290137 ETA:   0h 0m13s Progress:  78.7% words/sec/thread:   11634 lr:  0.010671 avg.loss:  2.289822 ETA:   0h 0m13s Progress:  78.8% words/sec/thread:   11634 lr:  0.010593 avg.loss:  2.289605 ETA:   0h 0m13s Progress:  79.0% words/sec/thread:   11635 lr:  0.010511 avg.loss:  2.289069 ETA:   0h 0m13s Progress:  79.1% words/sec/thread:   11634 lr:  0.010434 avg.loss:  2.288778 ETA:   0h 0m13s Progress:  79.3% words/sec/thread:   11634 lr:  0.010355 avg.loss:  2.288458 ETA:   0h 0m13s Progress:  79.4% words/sec/thread:   11635 lr:  0.010275 avg.loss:  2.288126 ETA:   0h 0m13s Progress:  79.6% words/sec/thread:   11635 lr:  0.010197 avg.loss:  2.287593 ETA:   0h 0m12s Progress:  79.8% words/sec/thread:   11635 lr:  0.010118 avg.loss:  2.287219 ETA:   0h 0m12s Progress:  79.9% words/sec/thread:   11635 lr:  0.010038 avg.loss:  2.286843 ETA:   0h 0m12s Progress:  80.1% words/sec/thread:   11636 lr:  0.009956 avg.loss:  2.286455 ETA:   0h 0m12s Progress:  80.2% words/sec/thread:   11635 lr:  0.009880 avg.loss:  2.286139 ETA:   0h 0m12s Progress:  80.4% words/sec/thread:   11635 lr:  0.009802 avg.loss:  2.285899 ETA:   0h 0m12s Progress:  80.6% words/sec/thread:   11635 lr:  0.009723 avg.loss:  2.285597 ETA:   0h 0m12s Progress:  80.7% words/sec/thread:   11636 lr:  0.009642 avg.loss:  2.285136 ETA:   0h 0m12s Progress:  80.9% words/sec/thread:   11636 lr:  0.009562 avg.loss:  2.284831 ETA:   0h 0m12s Progress:  81.0% words/sec/thread:   11637 lr:  0.009480 avg.loss:  2.284459 ETA:   0h 0m12s Progress:  81.2% words/sec/thread:   11636 lr:  0.009397 avg.loss:  2.284080 ETA:   0h 0m11s Progress:  81.4% words/sec/thread:   11637 lr:  0.009316 avg.loss:  2.283630 ETA:   0h 0m11s Progress:  81.5% words/sec/thread:   11637 lr:  0.009237 avg.loss:  2.283267 ETA:   0h 0m11s Progress:  81.7% words/sec/thread:   11637 lr:  0.009160 avg.loss:  2.282857 ETA:   0h 0m11s Progress:  81.8% words/sec/thread:   11637 lr:  0.009079 avg.loss:  2.282354 ETA:   0h 0m11s Progress:  82.0% words/sec/thread:   11638 lr:  0.008999 avg.loss:  2.281881 ETA:   0h 0m11s Progress:  82.2% words/sec/thread:   11638 lr:  0.008920 avg.loss:  2.281588 ETA:   0h 0m11s Progress:  82.3% words/sec/thread:   11638 lr:  0.008841 avg.loss:  2.281205 ETA:   0h 0m11s Progress:  82.5% words/sec/thread:   11638 lr:  0.008762 avg.loss:  2.280823 ETA:   0h 0m11s Progress:  82.6% words/sec/thread:   11637 lr:  0.008686 avg.loss:  2.280423 ETA:   0h 0m11s Progress:  82.8% words/sec/thread:   11638 lr:  0.008605 avg.loss:  2.280053 ETA:   0h 0m10s Progress:  82.9% words/sec/thread:   11637 lr:  0.008527 avg.loss:  2.279663 ETA:   0h 0m10s Progress:  83.1% words/sec/thread:   11638 lr:  0.008447 avg.loss:  2.279469 ETA:   0h 0m10s Progress:  83.3% words/sec/thread:   11638 lr:  0.008369 avg.loss:  2.279133 ETA:   0h 0m10s Progress:  83.4% words/sec/thread:   11638 lr:  0.008290 avg.loss:  2.278605 ETA:   0h 0m10s Progress:  83.6% words/sec/thread:   11637 lr:  0.008215 avg.loss:  2.278302 ETA:   0h 0m10s Progress:  83.7% words/sec/thread:   11637 lr:  0.008133 avg.loss:  2.277950 ETA:   0h 0m10s Progress:  83.9% words/sec/thread:   11637 lr:  0.008055 avg.loss:  2.277585 ETA:   0h 0m10s Progress:  84.0% words/sec/thread:   11637 lr:  0.007978 avg.loss:  2.277176 ETA:   0h 0m10s Progress:  84.2% words/sec/thread:   11638 lr:  0.007895 avg.loss:  2.276831 ETA:   0h 0m10s Progress:  84.4% words/sec/thread:   11637 lr:  0.007821 avg.loss:  2.276447 ETA:   0h 0m 9s Progress:  84.5% words/sec/thread:   11637 lr:  0.007741 avg.loss:  2.276182 ETA:   0h 0m 9s Progress:  84.7% words/sec/thread:   11637 lr:  0.007662 avg.loss:  2.275805 ETA:   0h 0m 9s Progress:  84.8% words/sec/thread:   11637 lr:  0.007584 avg.loss:  2.275522 ETA:   0h 0m 9s Progress:  85.0% words/sec/thread:   11637 lr:  0.007503 avg.loss:  2.275218 ETA:   0h 0m 9s Progress:  85.2% words/sec/thread:   11638 lr:  0.007424 avg.loss:  2.274864 ETA:   0h 0m 9s Progress:  85.3% words/sec/thread:   11638 lr:  0.007345 avg.loss:  2.274352 ETA:   0h 0m 9s Progress:  85.5% words/sec/thread:   11637 lr:  0.007270 avg.loss:  2.274056 ETA:   0h 0m 9s Progress:  85.6% words/sec/thread:   11638 lr:  0.007186 avg.loss:  2.273693 ETA:   0h 0m 9s Progress:  85.8% words/sec/thread:   11637 lr:  0.007106 avg.loss:  2.273321 ETA:   0h 0m 9s Progress:  85.9% words/sec/thread:   11637 lr:  0.007027 avg.loss:  2.272880 ETA:   0h 0m 8s Progress:  86.1% words/sec/thread:   11637 lr:  0.006947 avg.loss:  2.272416 ETA:   0h 0m 8s Progress:  86.3% words/sec/thread:   11637 lr:  0.006869 avg.loss:  2.272174 ETA:   0h 0m 8s Progress:  86.4% words/sec/thread:   11637 lr:  0.006790 avg.loss:  2.271837 ETA:   0h 0m 8s Progress:  86.6% words/sec/thread:   11637 lr:  0.006711 avg.loss:  2.271294 ETA:   0h 0m 8s Progress:  86.7% words/sec/thread:   11637 lr:  0.006631 avg.loss:  2.270923 ETA:   0h 0m 8s Progress:  86.9% words/sec/thread:   11638 lr:  0.006550 avg.loss:  2.270526 ETA:   0h 0m 8s Progress:  87.0% words/sec/thread:   11637 lr:  0.006476 avg.loss:  2.270242 ETA:   0h 0m 8s Progress:  87.2% words/sec/thread:   11637 lr:  0.006395 avg.loss:  2.269969 ETA:   0h 0m 8s Progress:  87.4% words/sec/thread:   11637 lr:  0.006317 avg.loss:  2.269632 ETA:   0h 0m 8s Progress:  87.5% words/sec/thread:   11637 lr:  0.006238 avg.loss:  2.269302 ETA:   0h 0m 7s Progress:  87.7% words/sec/thread:   11637 lr:  0.006160 avg.loss:  2.268926 ETA:   0h 0m 7s Progress:  87.8% words/sec/thread:   11638 lr:  0.006077 avg.loss:  2.268494 ETA:   0h 0m 7s Progress:  88.0% words/sec/thread:   11639 lr:  0.005996 avg.loss:  2.268268 ETA:   0h 0m 7s Progress:  88.2% words/sec/thread:   11639 lr:  0.005917 avg.loss:  2.267974 ETA:   0h 0m 7s Progress:  88.3% words/sec/thread:   11639 lr:  0.005839 avg.loss:  2.267790 ETA:   0h 0m 7s Progress:  88.5% words/sec/thread:   11639 lr:  0.005758 avg.loss:  2.267513 ETA:   0h 0m 7s Progress:  88.6% words/sec/thread:   11639 lr:  0.005679 avg.loss:  2.267137 ETA:   0h 0m 7s Progress:  88.8% words/sec/thread:   11639 lr:  0.005600 avg.loss:  2.266660 ETA:   0h 0m 7s Progress:  89.0% words/sec/thread:   11640 lr:  0.005519 avg.loss:  2.266394 ETA:   0h 0m 7s Progress:  89.1% words/sec/thread:   11639 lr:  0.005442 avg.loss:  2.266056 ETA:   0h 0m 6s Progress:  89.3% words/sec/thread:   11639 lr:  0.005365 avg.loss:  2.265722 ETA:   0h 0m 6s Progress:  89.4% words/sec/thread:   11638 lr:  0.005288 avg.loss:  2.265373 ETA:   0h 0m 6s Progress:  89.6% words/sec/thread:   11639 lr:  0.005207 avg.loss:  2.265044 ETA:   0h 0m 6s Progress:  89.7% words/sec/thread:   11639 lr:  0.005129 avg.loss:  2.264596 ETA:   0h 0m 6s Progress:  89.9% words/sec/thread:   11640 lr:  0.005046 avg.loss:  2.264287 ETA:   0h 0m 6s Progress:  90.1% words/sec/thread:   11637 lr:  0.004964 avg.loss:  2.263812 ETA:   0h 0m 6s Progress:  90.2% words/sec/thread:   11638 lr:  0.004885 avg.loss:  2.263468 ETA:   0h 0m 6s Progress:  90.4% words/sec/thread:   11637 lr:  0.004807 avg.loss:  2.263178 ETA:   0h 0m 6s Progress:  90.5% words/sec/thread:   11638 lr:  0.004726 avg.loss:  2.262953 ETA:   0h 0m 6s Progress:  90.7% words/sec/thread:   11639 lr:  0.004644 avg.loss:  2.262643 ETA:   0h 0m 5s Progress:  90.9% words/sec/thread:   11639 lr:  0.004565 avg.loss:  2.262247 ETA:   0h 0m 5s Progress:  91.0% words/sec/thread:   11638 lr:  0.004488 avg.loss:  2.261992 ETA:   0h 0m 5s Progress:  91.2% words/sec/thread:   11639 lr:  0.004408 avg.loss:  2.261586 ETA:   0h 0m 5s Progress:  91.3% words/sec/thread:   11639 lr:  0.004329 avg.loss:  2.261321 ETA:   0h 0m 5s Progress:  91.5% words/sec/thread:   11639 lr:  0.004250 avg.loss:  2.261120 ETA:   0h 0m 5s Progress:  91.7% words/sec/thread:   11639 lr:  0.004170 avg.loss:  2.260664 ETA:   0h 0m 5s Progress:  91.8% words/sec/thread:   11639 lr:  0.004092 avg.loss:  2.260381 ETA:   0h 0m 5s Progress:  92.0% words/sec/thread:   11639 lr:  0.004013 avg.loss:  2.260020 ETA:   0h 0m 5s Progress:  92.1% words/sec/thread:   11639 lr:  0.003934 avg.loss:  2.259672 ETA:   0h 0m 5s Progress:  92.3% words/sec/thread:   11639 lr:  0.003853 avg.loss:  2.259430 ETA:   0h 0m 4s Progress:  92.4% words/sec/thread:   11639 lr:  0.003777 avg.loss:  2.259138 ETA:   0h 0m 4s Progress:  92.6% words/sec/thread:   11638 lr:  0.003700 avg.loss:  2.258854 ETA:   0h 0m 4s Progress:  92.8% words/sec/thread:   11638 lr:  0.003622 avg.loss:  2.258410 ETA:   0h 0m 4s Progress:  92.9% words/sec/thread:   11638 lr:  0.003543 avg.loss:  2.258100 ETA:   0h 0m 4s Progress:  93.1% words/sec/thread:   11638 lr:  0.003465 avg.loss:  2.257631 ETA:   0h 0m 4s Progress:  93.2% words/sec/thread:   11638 lr:  0.003387 avg.loss:  2.257370 ETA:   0h 0m 4s Progress:  93.4% words/sec/thread:   11639 lr:  0.003305 avg.loss:  2.257128 ETA:   0h 0m 4s Progress:  93.5% words/sec/thread:   11639 lr:  0.003226 avg.loss:  2.256884 ETA:   0h 0m 4s Progress:  93.7% words/sec/thread:   11638 lr:  0.003149 avg.loss:  2.256515 ETA:   0h 0m 4s Progress:  93.9% words/sec/thread:   11639 lr:  0.003068 avg.loss:  2.256264 ETA:   0h 0m 3s Progress:  94.0% words/sec/thread:   11639 lr:  0.002988 avg.loss:  2.255782 ETA:   0h 0m 3s Progress:  94.2% words/sec/thread:   11638 lr:  0.002897 avg.loss:  2.255469 ETA:   0h 0m 3s Progress:  94.4% words/sec/thread:   11639 lr:  0.002816 avg.loss:  2.255162 ETA:   0h 0m 3s Progress:  94.5% words/sec/thread:   11638 lr:  0.002738 avg.loss:  2.254817 ETA:   0h 0m 3s Progress:  94.7% words/sec/thread:   11638 lr:  0.002660 avg.loss:  2.254430 ETA:   0h 0m 3s Progress:  94.8% words/sec/thread:   11637 lr:  0.002586 avg.loss:  2.254177 ETA:   0h 0m 3s Progress:  95.0% words/sec/thread:   11638 lr:  0.002502 avg.loss:  2.253835 ETA:   0h 0m 3s Progress:  95.2% words/sec/thread:   11638 lr:  0.002424 avg.loss:  2.253528 ETA:   0h 0m 3s Progress:  95.3% words/sec/thread:   11638 lr:  0.002346 avg.loss:  2.253304 ETA:   0h 0m 2s Progress:  95.5% words/sec/thread:   11638 lr:  0.002265 avg.loss:  2.252957 ETA:   0h 0m 2s Progress:  95.6% words/sec/thread:   11639 lr:  0.002185 avg.loss:  2.252613 ETA:   0h 0m 2s Progress:  95.8% words/sec/thread:   11640 lr:  0.002103 avg.loss:  2.252301 ETA:   0h 0m 2s Progress:  95.9% words/sec/thread:   11639 lr:  0.002026 avg.loss:  2.252012 ETA:   0h 0m 2s Progress:  96.1% words/sec/thread:   11640 lr:  0.001944 avg.loss:  2.251654 ETA:   0h 0m 2s Progress:  96.3% words/sec/thread:   11640 lr:  0.001865 avg.loss:  2.251439 ETA:   0h 0m 2s Progress:  96.4% words/sec/thread:   11640 lr:  0.001785 avg.loss:  2.250980 ETA:   0h 0m 2s Progress:  96.6% words/sec/thread:   11641 lr:  0.001705 avg.loss:  2.250777 ETA:   0h 0m 2s Progress:  96.7% words/sec/thread:   11640 lr:  0.001628 avg.loss:  2.250449 ETA:   0h 0m 2s Progress:  96.9% words/sec/thread:   11639 lr:  0.001552 avg.loss:  2.250193 ETA:   0h 0m 1s Progress:  97.0% words/sec/thread:   11639 lr:  0.001475 avg.loss:  2.249878 ETA:   0h 0m 1s Progress:  97.2% words/sec/thread:   11639 lr:  0.001395 avg.loss:  2.249489 ETA:   0h 0m 1s Progress:  97.4% words/sec/thread:   11640 lr:  0.001314 avg.loss:  2.249270 ETA:   0h 0m 1s Progress:  97.5% words/sec/thread:   11640 lr:  0.001234 avg.loss:  2.248913 ETA:   0h 0m 1s Progress:  97.7% words/sec/thread:   11640 lr:  0.001155 avg.loss:  2.248508 ETA:   0h 0m 1s Progress:  97.8% words/sec/thread:   11640 lr:  0.001077 avg.loss:  2.248136 ETA:   0h 0m 1s Progress:  98.0% words/sec/thread:   11640 lr:  0.000994 avg.loss:  2.247876 ETA:   0h 0m 1s Progress:  98.2% words/sec/thread:   11640 lr:  0.000914 avg.loss:  2.247468 ETA:   0h 0m 1s Progress:  98.3% words/sec/thread:   11641 lr:  0.000832 avg.loss:  2.247215 ETA:   0h 0m 1s Progress:  98.5% words/sec/thread:   11641 lr:  0.000753 avg.loss:  2.246857 ETA:   0h 0m 0s Progress:  98.6% words/sec/thread:   11640 lr:  0.000677 avg.loss:  2.246508 ETA:   0h 0m 0s Progress:  98.8% words/sec/thread:   11640 lr:  0.000599 avg.loss:  2.246182 ETA:   0h 0m 0s Progress:  99.0% words/sec/thread:   11640 lr:  0.000520 avg.loss:  2.245917 ETA:   0h 0m 0s Progress:  99.1% words/sec/thread:   11640 lr:  0.000442 avg.loss:  2.245666 ETA:   0h 0m 0s Progress:  99.3% words/sec/thread:   11639 lr:  0.000365 avg.loss:  2.245393 ETA:   0h 0m 0s Progress:  99.4% words/sec/thread:   11639 lr:  0.000286 avg.loss:  2.245160 ETA:   0h 0m 0s Progress:  99.6% words/sec/thread:   11640 lr:  0.000205 avg.loss:  2.244834 ETA:   0h 0m 0s Progress:  99.7% words/sec/thread:   11640 lr:  0.000126 avg.loss:  2.244520 ETA:   0h 0m 0s Progress:  99.9% words/sec/thread:   11640 lr:  0.000048 avg.loss:  2.244270 ETA:   0h 0m 0s Progress: 100.0% words/sec/thread:   11633 lr: -0.000003 avg.loss:  2.243985 ETA:   0h 0m 0s Progress: 100.0% words/sec/thread:   11633 lr:  0.000000 avg.loss:  2.243985 ETA:   0h 0m 0s #> [1] \"/home/runner/ft_model/ft_model.bin\" # Load model: model_path <- path.expand(\"~/ft_model/ft_model.bin\") model <- fastrtext::load_model(paste0(model_path))  # Nearest-neigbor query: fastrtext::get_nn(model, \"spahn\", k=8) #>           spahns    spahnversagen             jens  spahnruecktritt  #>        0.9208406        0.7816812        0.7632975        0.7401152  #>    maskenskandal     maskenaffäre personalnotstand          tönnies  #>        0.7122628        0.6860786        0.6611746        0.6524869"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"finding-keywords","dir":"Articles","previous_headings":"","what":"Finding keywords","title":"from_text_to_measurement","text":"Next, want use fastText model manually annotated dataset find short dictionary populist communication. First, split annotated data train test sample, using caret::createDataPartition. starting point dictionary, use vocabulary fastText model. add ID vocabulary use clean_text remove stopwords, drop words less 3 characters list: Next, want narrow list. identify words similar hand-coded corpus populist Tweets dissimilar non-populist Tweets df_train using find_distinctive. find_distinctive computes average representation subset annotated corpus populism coded present, well representation negative counterpart, .e. non-populist corpus. computes cosine similarities word dataframe two corpora, calculates difference two similarity scores. provides us quick computationally inexpensive shortcut find keywords capture specific concept, nothing else. However, method narrowing list keywords bit crude informative assessing well single word perform used DDR method. Hence, want get actual F1 scores single word, used one-word-dicitonary DDR method. Note DDR method returns cosine similarity continuous measure, theoretically ranging -1 +1, manually annotated dataset coded binary fashion (populist non-populist). solve problem, continuous measure used independent variable logistic regression, predicting manual binary coding. obtain Recall, Precision F1 scores, dictvectoR compares binary predictions resulting regression manual coding. direct gold standard test circumvents problem producing reliable granular codes manually (Grimmer & Stewart, 2013, p. 275). Recall evaluation metric indicates well automated measure captures true positives, precision indicates well captures true positives, F1 harmonic mean (Chinchor 1992). get_many_F1s function efficiently returns F1 scores list words dictionaries, used dictionary DDR method. ’re interested best-performing quartile, extract using helper-function filter_ntile:","code":"# set seed set.seed(42)  # get index for splitting train_id <- caret::createDataPartition(tw_annot$pop,                                 times = 1,                                 p = .7,                                 list = F) # split annotated data df_train <- tw_annot %>% slice(train_id)  df_test <- tw_annot %>% slice(-train_id) # # Get vocabulary vocab_df <- fastrtext::get_dictionary(model) %>% data.frame(words = .)  # Get vocabulary ID vocab_df$word_id <- fastrtext::get_word_ids(model, vocab_df$words)  # remove stopwords vocab_df <- clean_text(vocab_df,                         text_field = \"words\",                        remove_stopwords = T) %>%             filter(n_words == 1,                    n_chars > 2) # Get distinctive word scores vocab_df <- find_distinctive(df_train,                               concept_field = \"pop\",                              text_field = \"text\",                              word_df = vocab_df,                              word_field = \"words\",                              model = model)  vocab_df %<>% mutate(pop_distinctiveXpossim = pop_distinctive*pop_possim) %>%                filter_ntile(\"pop_distinctiveXpossim\", .75) vocab_df$popF1 <- get_many_F1s(vocab_df$words,                               model = model,                               df = df_train,                               reference = \"pop\") top <- vocab_df %>%               filter_ntile(\"popF1\", .75) %>%              filter(popF1 > 0)"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"add-multi-words","dir":"Articles","previous_headings":"","what":"Add multi-words","title":"from_text_to_measurement","text":"concepts expressed rather multi-word expressions single words. case, suspect people-centrism, one core dimension populism, may involve multiword expressions, example constructing -group references ‘’, e.g., ‘taxpayers’, ‘land’ etc. find common multiword expressions, use population data (75 percent speed process bit), tokenize using quanteda::tokens. add_multiwords adds multiword expressions dataframe single words counts occurences. look multiwords 2-word-window (level = 1), can increased 3-words window (level = 2). process might take two three minutes. want narrow list , filtering terms occur . add unique word_id add_word_id.","code":"tw_data %<>% filter(n_words >= 2)  # Use 75% sample set.seed(42) tw_split <- tw_data %>% slice_sample(prop = .75)  # tokenize toks <- quanteda::tokens(tw_split$text) top <- add_multiwords(top,                        tokens = toks,                        min_hits = 1,                       word_field = \"words\",                       levels = 1) #> [1] \"Finding multi-word expressions in window = 1...\" #> Joining, by = \"from\" #> [1] \"Adding missing count of original words:\" #> [1] \"Counting word occurrences...\" top %<>% filter(hits > 1)  top %<>% add_word_id()"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"get-f1-scores","dir":"Articles","previous_headings":"","what":"Get F1 scores","title":"from_text_to_measurement","text":"Now, also want know F1 scores new multiword terms, hence just run get_many_F1s .","code":"top$popF1 <- get_many_F1s(top$words,                           model = model,                           df = df_train,                           reference = \"pop\")"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"drop-similar-terms","dir":"Articles","previous_headings":"","what":"Drop similar terms","title":"from_text_to_measurement","text":"resulting list words quite long includes lot redundancy. Note unlike traditional dictionary approaches, DDR method performs better input dictionary short clear-cut. redundancy problem traditional dictionaries, may distort performance dictionary DDR (Garten et al., 2018). remove_similar_words helps us detect similar terms - computing pairwise cosine similarity words representations datafram. can use two score input decide words drop: use F1 score calculated popF1, number occurences (compare_hits = T). set function compare terms reach smiliarity 60% (min_simil = .6). win_threshold defines share comparisons term must win order remain dataframe. Additionally, drop terms originate uni-word afd, German populist party, since want populism measure rather non-partisan:","code":"## narrow down top_subs <- top %>% remove_similar_words(model,                                          compare_by = \"popF1\",                                         compare_hits = T,                                          min_simil = .6,                                          win_threshold = .65) %>%                      filter(!from == \"afd\")"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"narrowing-down-by-hand","dir":"Articles","previous_headings":"","what":"Narrowing down by hand","title":"from_text_to_measurement","text":"Let’s look terms found far. (won’t print 400 terms top 50) list keywors still quite long. includes many terms seem theoretically plausible, also many words obviously either specific, seem place. Since next steps involve combinatorics finding good combination keywords, want narrow list words drastically possible. theoretical reasons, decided group found words three categories reflect different aspects populist communication: Terms reflect ‘elites’, terms reflect ‘people’, words relate two groups. 400 terms, hand-picked five distinctive terms categories. mode selection theory-driven different inductive logic used far. may want opt different modes selection, depending task quality inductively generated keywords. words picked processing: use hand-picked list words create shortlist add variable indicates category:","code":"top_subs %>% arrange(desc(popF1)) %>% pull(words) %>% head(50) #>  [1] \"panzer\"                      \"wahn\"                        #>  [3] \"deutschen steuerzahler\"      \"wand\"                        #>  [5] \"verrechnet\"                  \"gaga\"                        #>  [7] \"steuergeldverschwendung\"     \"gender gaga\"                 #>  [9] \"skandalösen\"                 \"teures\"                      #> [11] \"rücken wand\"                 \"maskenskandal\"               #> [13] \"deutschen mittelstand\"       \"mdb nimmt\"                   #> [15] \"steuerzahler\"                \"verrückt\"                    #> [17] \"igmetall\"                    \"skandalöse\"                  #> [19] \"mdb groko\"                   \"irrsinnige\"                  #> [21] \"banken\"                      \"autofahrer\"                  #> [23] \"betreibt\"                    \"deutschland groko\"           #> [25] \"merkels groko\"               \"daimler vw\"                  #> [27] \"schamlos\"                    \"existenzen vernichtet\"       #> [29] \"staatshaushalt\"              \"kurzstreckenflüge verbieten\" #> [31] \"irrweg\"                      \"maut desaster\"               #> [33] \"skandalös\"                   \"ganzer linie\"                #> [35] \"deckt\"                       \"knickt\"                      #> [37] \"gendergaga\"                  \"vernichtet\"                  #> [39] \"mittels\"                     \"skandale\"                    #> [41] \"spahn warnt\"                 \"steuergeldern\"               #> [43] \"suizid\"                      \"diesel benziner\"             #> [45] \"betreiber\"                   \"diesel fahrverbote\"          #> [47] \"rechnet\"                     \"kette\"                       #> [49] \"monetäre staatsfinanzierung\" \"vernichten\" elites <- c( \"altparteien\",  \"lobbyisten\",  \"merkel co\",  \"plänen bundesregierung\",  \"zwangsgebühr\"  )  people <- c( \"arbeitnehmern\",   \"existenzen\",  \"deutschen\",  \"deutschen mittelstand\",  \"deutschen steuerzahler\"  )  relation <- c( \"entlarven\",  \"schamlos\",  \"verrückt\", \"wahn\",  \"steuergeldverschwendung\"  ) top_elites <- top_subs %>% filter(words %in% elites) %>% mutate(cat = \"elites\") top_people <- top_subs %>% filter(words %in% people) %>% mutate(cat = \"people\") top_relation <- top_subs %>% filter(words %in% relation) %>% mutate(cat = \"relation\")  shortlist <- bind_rows(top_elites, top_people, top_relation)  shortlist$cat %<>% factor()  shortlist$cat %>% summary() #>   elites   people relation  #>        5        5        5"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"get-combinations","dir":"Articles","previous_headings":"","what":"Get combinations","title":"from_text_to_measurement","text":"now use shortlist get possible combinations various length per category, possible combinations combinations. get_combis helps us finding combinations, provides useful random sampling mechanism limits number returned combinations: Setting limit, limits number combinations given length returned per dimension. example, consider shortlist words 2 categories B, 10 words per category, want find combinations words include least 3 words per category (min_per_dim = 3) maximum 10 words overall (max_overall = 10), .e. max. 5 words per dimension. 120 possible combinations length 3 category B: However, also get 210 possible combinations length 4, 252 combinations length 5 per category: want get possible combinations combinations, number increases really quickly, can see . first two lines return possible cominations B varying length 3 5. expand_grid returns combinations combinations: Setting limit , let’s say 30, get_combis radomly draws 30 possible combinations one length per category. case, limit really necessary, 5 words per category want lengths 3 4. maximum number combinations per length & category 10: Although unnecessary case, specify limit (won’t drop combinations) - just demonstration. Setting seed makes results reproducible, default 1. get_cobmis returns data.frame includes column ’re settings stored, case want come back draw different round combinations. , let’s get combinations list words:","code":"A <- c(1:10) B <- c(11:20)  choose(10, 3) #> [1] 120 choose(10, 4) #> [1] 210 choose(10, 5) #> [1] 252 A_c <- do.call(\"c\", lapply(3:5, function(i) combn(A, i, FUN = list))) B_c <- do.call(\"c\", lapply(3:5, function(i) combn(B, i, FUN = list))) expand.grid(A_c, B_c)  %>% nrow() #> [1] 338724 max(   choose(5, 3),   choose(5, 4) ) #> [1] 10 combis_rd <- get_combis(shortlist,                         dims = \"cat\",                         min_per_dim = 3,                         max_overall = 12,                         limit = 10,                         seed = 42) #> Joining, by = \"rowid\""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"evaluate-combinations","dir":"Articles","previous_headings":"","what":"Evaluate combinations","title":"from_text_to_measurement","text":"Now want get Recall, Precision, F1 scores 3,375 combinations. get_many_RPFs made task: Now, let’s pick best performing short dictionary: 10 words cover various aspects populist communication highly context-specific. However, place dive theoretical discussions. Let’s check performance dictionary instead. training sample, dictionary reaches F1 .56, great, OK. Let’s check F1 score applied test sample: short dictionary reaches F1 .55 test sample, almost par result train data. Drawing evaluation, reason assume avoided overfitting dictionary training data. may therefore assume measurement can least approximately capture populist communication population dataset. mean short dictionary perform equally well different contexts, e.g., user-generated material, timeframes. special applications, might intereste DDR measurement perform different subsets data, e.g., using translated corpus originating various languages. F1 scores grouped subsets text dataframe, can obtained get_many_F1s_by_group. case, might interested dictionaries perform different political parties. , demonstrate function using top 5% short dictionaries. Since populist communication annotated sample indeed mainly concentrated parties AfD Die Linke, calculate 3rd root product overall F1 score two party-specific F1-scores pick best performing, balanced dictionary: Let’s check performance ‘balanced’ dictionary test data: decreased performance test data, stick dictionary picked .","code":"combis_df <- get_many_RPFs(keyword_df = combis_rd,                              keyword_field = \"combs_split\",                              model = model,                               text_df = df_train,                              reference = \"pop\",                               text_field = \"text\") #> Joining, by = \"rowid\" # Pick dictionary that maximizes F1: dict  <- combis_df %>%            filter(F1 == max(F1)) %>%            pull(combs_split) %>%           unlist()  # Let's see: dict #>  [1] \"deutschen\"               \"deutschen mittelstand\"   #>  [3] \"entlarven\"               \"existenzen\"              #>  [5] \"merkel co\"               \"plänen bundesregierung\"  #>  [7] \"verrückt\"                \"wahn\"                    #>  [9] \"zwangsgebühr\"            \"altparteien\"             #> [11] \"arbeitnehmern\"           \"deutschen\"               #> [13] \"deutschen mittelstand\"   \"deutschen steuerzahler\"  #> [15] \"entlarven\"               \"merkel co\"               #> [17] \"plänen bundesregierung\"  \"steuergeldverschwendung\" #> [19] \"verrückt\"                \"wahn\" combis_df$F1 %>% max()   #> [1] 0.5568627 # Performance for test_df get_F1(df_test, dict, model, 'pop') #> [1] 0.5252525 combis_subset <- combis_df %>% filter_ntile(\"F1\", .95)  combis_df_by_group <-  get_many_F1s_by_group(keyword_df = combis_subset,                                              keyword_field = \"combs_split\",                                              id = \"id\",                                              model = model,                                              text_df = df_train,                                              group_field = \"party\",                                              reference = 'pop') #> Joining, by = \"id\" # Compute 3rd root product of 3 F1s: combis_df_by_group %<>% mutate(F1_balanced = (F1*F1_AfD*F1_Linke)^(1/3))  dict_bal  <- combis_df_by_group %>%                          filter(F1_balanced == max(F1_balanced)) %>%                          pull(combs_split) %>%                         unlist()  dict_bal #> [1] \"arbeitnehmern\"           \"deutschen steuerzahler\"  #> [3] \"existenzen\"              \"lobbyisten\"              #> [5] \"merkel co\"               \"plänen bundesregierung\"  #> [7] \"steuergeldverschwendung\" \"verrückt\"                #> [9] \"wahn\" get_F1(df_test, dict_bal, model, 'pop') #> [1] 0.54"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"apply-ddr","dir":"Articles","previous_headings":"","what":"Apply DDR","title":"from_text_to_measurement","text":"Now, want apply best performing dictionary datasets. apply , population data tw_data annotated data tw_annot, using core function package cossim2dict. fill missing values, occure sometimes text cleaning empties text field mean minus 1 SD:","code":"tw_annot$pop_ddr <- cossim2dict(df = tw_annot,                                dictionary = dict,                                model = model,                                replace_na = 'mean-sd')  tw_data$pop_ddr <- cossim2dict(df = tw_data,                                dictionary = dict,                                model = model,                                replace_na = 'mean-sd')"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"face-validity","dir":"Articles","previous_headings":"","what":"Face validity","title":"from_text_to_measurement","text":"Now, lets inspect top 3 populist Tweets population data: examples include Tweets AfD directed ‘genderism’ leftist ideology, directed EU, directed climate politics. Judging face validity Tweets clearly populist. Let’s inspect lower end spectrum: least populist Tweets - according measurement - include quite harmless “get well soon” wishes.","code":"tw_data %>% arrange(desc(pop_ddr)) %>% pull(full_text) %>% head(3) #> [1] \"#AfD-Fraktionsvize @PeterFelser hat den linksideologischen Hype um neue Geschlechter als „teures #Gendergaga“ kritisiert: \\\"Riesen Aufwand für Nichts! Gender-Gaga auf Kosten der #Steuerzahler ersatzlos streichen!\\\" #Bundestag https://t.co/paIVvtTkfg\"                              #> [2] \"+++EU auf den Müllhaufen der Wirtschaftsgeschichte werfen!+++\\n#AfD-MdB Hansjörg Müller @MdB_Mueller_AfD erklärt: \\\"Brüssel stranguliert mit sinnloser Bürokratie deutsche Unternehmen zu Tode!\\\" #Bundestag\\nhttps://t.co/m4WKyMweog\"                                                  #> [3] \"Zur Ablehnung des #AfD-Antrags Absatzeinbrüchen im Neuwagen-Verkauf durch eine sofortige Senkung des Umsatz- bzw. Mehrwertsteuersatzes von 19% auf 5% zu begegnen, erklärt @DirkSpaniel: \\\"#GroKo lässt 🇩🇪 Autobauer &amp; Arbeitnehmer im Stich!\\\" #Bundestag https://t.co/2bn41PAHJE\" tw_data %>% arrange(desc(pop_ddr)) %>% pull(full_text) %>% tail(3) #> [1] \"@KristinaFassler @KenzaAbbou @KaiDiekmann @katha_krentz @lukizzl @SophiaRoediger @TijenOnaran @cihansugur @anja_hendel @esinekalos @froileinmueller @biancareitz_ @wohlrapp @pmk997 @swissmadame @ChrisBoesenberg Gute Besserung ❤️‍\\U{01fa79}\" #> [2] \"@ralphruthe @marga_owski Gute Besserung!\"                                                                                                                                                                                                     #> [3] \"@grischdjane @doktordab @MarkusBlume So ein Käse Käse ;-)\""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"plots","dir":"Articles","previous_headings":"","what":"Plots","title":"from_text_to_measurement","text":"Let’s use DDR measurements plots. start plotting gradual populism measurement human, binary coding populism, using complete annotated dataset:  see non-populist Tweets score higher , mean populism score groups clearly different - indicated boxplots. can also use parallel-coded 90 Tweets get visual evaluation gradual measurement returned DDR method. ‘Populism’ annotated dataset coded two distinct categories, anti-elitism people-centrism. obtain gradual score, just calculate sum coding two categories two coders, resulting score ranges 0 4. plot DDR score:  two (semi-)continuous scores Pearson’s r correlation .53: Finally, assess external validity measurement, compare aggregated level POPPA expert rating political parties (Meijers Zaslove, 2020). expert survey provides gradual rating populism political parties. score already merged tw_data.  aggregate level, expert ratings populism political parties align well DDR score populist communication. Indeed, scores strongly correlated:","code":"tw_annot$pop %<>% factor()   ggplot(tw_annot, aes(x = pop, y = pop_ddr, color = pop)) +    geom_boxplot()+   geom_jitter(width = .2)+   theme_bw()+   labs(y = \"Populism (DDR)\",         x = \"Populism (Human coding)\")+   theme(axis.title.x = element_text(hjust = 0),         legend.position=\"none\") parallel_df <- tw_annot %>%   filter(rel_test == 1) %>%   mutate(pop_cum = ppc_A + ppc_B + ane_A + ane_B)   ggplot(parallel_df, aes(x = pop_ddr, y = pop_cum))+   geom_jitter(height = .1,               width = 0)+   geom_smooth(method = lm, se = T)+   coord_cartesian(ylim = c(0,4)) cor(parallel_df$pop_cum, parallel_df$pop_ddr) #> [1] 0.5283765 # aggregate on party level tw_party <- tw_data %>%                group_by(party) %>%               summarise(pop_ddr = mean(pop_ddr),                         poppa_populism = mean(poppa_populism))    ggplot(tw_party, aes(x = pop_ddr, y = poppa_populism, label=party))+   geom_point(na.rm = T)+   geom_smooth(method = lm)+   geom_text(hjust=0, vjust=0) cor(tw_party$pop_ddr, tw_party$poppa_populism) #> [1] 0.8704711"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"simple-visual-analysis","dir":"Articles","previous_headings":"","what":"Simple visual analysis","title":"from_text_to_measurement","text":"resulting DDR scores used, e.g. investigate temporal shifts strategies political parties. plot mean populism score per party time. see ‘AfD’ clearly populist party, followed ‘Die Linke’, theoretically absolutely plausible. sudden decrease populist communication ‘Die Linke’ parties August 2020, driven growth populist coronasceptic movement - opposed parties ‘AfD’. end timeframe, shortly German General Elections September 2021, level populism rises notably ‘AfD’.","code":"tw_data %>%     mutate(day = as.Date(created_at)) %>%      group_by(party, day) %>%      summarise(pop_ddr = mean(pop_ddr)) %>%      ggplot(aes(x = day, y = pop_ddr, color = party))+     geom_smooth(method = 'loess', span = 0.15, se = F)"},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"from_text_to_measurement","text":"Summing , dictionary measurement produced vignette good job differentiating levels populism communication political parties, aggregate level. also produces somehow plausible results compared cumulative coding two coders. performs OK compared directly binary hand-codings Tweets. However, much room improvement. possible ways improve measurement: Improve fastText model using much larger training dataset. Pre-trained models trained millions sentences, example. However, unlike generic models, may want stick material specific context (: communication political elites). Improve fastText model parameter tuning. E.g., donsider increasing number dimensions (dim, 100-300 popular choices), increasing number epochs (epoch), play around different ngram lengths (minn, maxn), play around window size context (ws). can use get_nn face validity plausibility checks, use formal test (e.g., analogy tests) determine quality fastText model. Use larger annotated sample finding good keywords evaluating quality. larger sample, lower risk overfitting short dictionary specific context. Play around different thresholds remove_similar_terms. function can quite dramatic effect words included excluded. Think theory-driven, explicit coding-scheme hand-picking words narrowed subset. Allow combinations per length dimension (limit) get_combis. Good luck!","code":""},{"path":"https://thieled.github.io/dictvectoR/articles/from_text_to_measurement.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"from_text_to_measurement","text":"Bojanowski, P., Grave, E., Joulin, ., & Mikolov, T. (2017). Enriching Word Vectors Subword Information. ArXiv:1607.04606 [Cs]. Retrieved http://arxiv.org/abs/1607.04606 Chinchor, N. (1992). MUC-4 evaluation metrics. Proceedings 4th Conference Message Understanding, 22–29. USA: Association Computational Linguistics. https://doi.org/10.3115/1072064.1072067 Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018). Dictionaries distributions: Combining expert knowledge large scale textual data content analysis. Behavior Research Methods, 50(1), 344–361. https://doi.org/10.3758/s13428-017-0875-9 Grimmer, J., & Stewart, B. M. (2013). Text data: promise pitfalls automatic content analysis methods political texts. Political Analysis, 21(3), 267–297. https://doi.org/10.1093/pan/mps028 Meijers, M., & Zaslove, . (2020). Populism Political Parties Expert Survey 2018 (POPPA) (Data set). Harvard Dataverse. https://doi.org/10.7910/DVN/8NEL7B Thiele, D. (2022, June 27). “Don’t believe media’s pandemic propaganda!!” Covid-19 affected populist Facebook user comments seven European countries. Presented ICA Regional Conference 2022. Computational Communication Research Central Eastern Europe, Helsinki, Finland. Retrieved https://ucloud.univie.ac./index.php/s/PzGzChXroLCXrtt","code":""},{"path":"https://thieled.github.io/dictvectoR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Daniel Thiele. Maintainer, author.","code":""},{"path":"https://thieled.github.io/dictvectoR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Thiele D (2022). dictvectoR:  Word vectors dictionaries . https://github.com/thieled, https://thieled.github.io/dictvectoR/.","code":"@Manual{,   title = {dictvectoR: {{ Word vectors for dictionaries }}},   author = {Daniel Thiele},   year = {2022},   note = {https://github.com/thieled, https://thieled.github.io/dictvectoR/}, }"},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"dictvector-","dir":"","previous_headings":"","what":"{{ Word vectors for dictionaries }}","title":"{{ Word vectors for dictionaries }}","text":"dictvectoR implements “Distributed Dictionary Representation” (DDR) Method (Garten et al., 2018). DDR uses word vector representation dictionary, .e. list words reflect theoretical concept, calculate “continuous measure similarity concept piece text” [1]. Word vectors, also called word embeddings, aim represent semantic proximity words vector space [2]. package uses fastText word vectors [3]. Average vector representations computed concept dictionary document. DDR score cosine similarity vectors [1]. addition core function, package comes range functions help find keywords assess performance words dictionaries. can learn workflow training fastText model, finding good dictionary, evaluating performance vignette(\"from_text_to_measurement\"). workflow described greater detail [4]. demonstrate usage, package comes corpus 20,838 Tweets 32 Twitter accounts German politics, published 2020-03-11 2021-09-25, hand-coded sample 1,000 Tweets corpus, coded along two binary variables populist communication: Anti-elitism people-centrism. package also includes minimal fastText model, serves provide working examples functions package recommended used actual analysis.","code":""},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"{{ Word vectors for dictionaries }}","text":"can install development version dictvectoR GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"thieled/dictvectoR\")"},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"{{ Word vectors for dictionaries }}","text":"basic example, starting scratch, using built-Twitter data German politicians Covid-crisis. get started, need fastText word vector model, train pre-processed data, cleaned, split, shuffled [prepare_train_data]. demonstration purposes, keep model simple small. actual application, might want consider using much textual data, epochs, dimensions larger bucket size. might also consider using (task-specific) pre-trained model . (Note: code create fastText model user home directory deleted end readme.) text data loaded Tweets German politicians, posted beginning Covid pandemic German General Elections September 2021. First, clean textual data, using helper function [clean_text], tailored German social media texts. Now, lets apply DDR method. Let’s say want measure degree populism Tweets. first define ad-hoc short dictionary consider reflect important dimensions populist communication apply DDR method using short dictionary [cossim2dict]. Let’s look top 3 populist Tweets, according ad-hoc measurement: speak German, can judge face-validity top results appear quite plausible. Let’s quick look measurement can re-create scores POPPA expert survey, rated parties along contiuous score populism. score already included tw_data:  aggregate level, expert survey rating ad-hoc DDR measurement highly correlated:","code":"library(dictvectoR) library(dplyr)  # Prepare text data. Cleans, splits, and shuffles textual data texts <- prepare_train_data(tw_data, text_field = \"full_text\", seed = 42)  # Create local folder, set model file name dir.create(\"~/ft_model_readme\", showWarnings = FALSE) model_file <- path.expand(\"~/ft_model_readme/ft_model_demo\")  # Train a fasttext model using the twitter data fastrtext::build_vectors(texts,                           model_file,                           modeltype = c(\"skipgram\"),                          dim = 70, epoch = 5, bucket = 5e+4, lr = 0.1,                          maxn = 6,  minn = 4, minCount = 4,                          verbose = 1, ws= 5)  # Load model: model_path <- path.expand(\"~/ft_model_readme/ft_model_demo.bin\") model <- fastrtext::load_model(paste0(model_path)) tw_data %<>% clean_text(text_field = \"full_text\", remove_stopwords = T) # Define a short dictionary pop_dict <- c(\"merkel\",               \"irrsinn\",               \"diktatur\",               \"lobbyismus\",               \"arbeitende menschen\",               \"deutschland\",               \"unsere steuergelder\")  # Get the DDR score tw_data$pop_ddr <- cossim2dict(tw_data, pop_dict, model, replace_na = 'mean-sd') tw_data %>%   dplyr::arrange(desc(pop_ddr)) %>%   head(3) %>%   dplyr::pull(full_text) #> [1] \"Ob Deutschkurs oder Job - die Bilanz von \\\"geflüchteten\\\" #Frauen stellt sich nach fünf Jahren in #Deutschland ernüchternd dar. Merkels Fachkräfte-Märchen zerschellt krachend an der Realität - - der deutsche #Sozialstaat machts möglich! #AfD #Migration https://t.co/oTrP2ewkpv https://t.co/kr4PPh2a1z\" #> [2] \"#UNO finanziert #Islamisten-#Hass mit unserem #Steuergeld!\\n\\nSeit Jahrzehnten haben die Vereinten Nationen ein massives Problem mit #Islamismus und islamischem #Antisemitismus in den eigenen Reihen – und es wird nicht besser.\\n\\nhttps://t.co/G9MMaQrqIY https://t.co/fzjQ8Ty2qq\"                        #> [3] \"Seit es das #Grundgesetz gibt, hatte #Deutschland keinen Regierungschef, der deutsche Interessen so schlecht vertritt und den Ausverkauf des deutschen Volksvermögens und der Staatsfinanzen so schamlos betreibt, wie Angela #Merkel! #AfD #Corona #Covid19 https://t.co/qGKYUgqcSe\" # aggregate tw_party <- tw_data %>%                group_by(party) %>%               summarise(pop_ddr = mean(pop_ddr),                         poppa_populism = mean(poppa_populism))   # plot library(ggplot2) ggplot(tw_party, aes(x = pop_ddr, y = poppa_populism, label=party))+   geom_point(na.rm = T)+   geom_smooth(method = lm)+   geom_text(hjust=0, vjust=0) cor(tw_party$pop_ddr, tw_party$poppa_populism) #> [1] 0.8342808"},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"finding-keywords","dir":"","previous_headings":"","what":"Finding keywords","title":"{{ Word vectors for dictionaries }}","text":"dictvectoR provides variety functions inductively find good keywords efficiently assess performance many . can learn complete workflow vignette(\"from_text_to_measurement\"). fun! (first, let’s delete demo model:)","code":"unlink(\"~/ft_model_readme\", recursive = TRUE)"},{"path":"https://thieled.github.io/dictvectoR/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"{{ Word vectors for dictionaries }}","text":"[1] Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018). Dictionaries distributions: Combining expert knowledge large scale textual data content analysis. Behavior Research Methods, 50(1), 344–361. https://doi.org/10.3758/s13428-017-0875-9 [2] Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation Word Representations Vector Space. ArXiv:1301.3781 [Cs]. http://arxiv.org/abs/1301.3781 [3] Bojanowski, P., Grave, E., Joulin, ., & Mikolov, T. (2017). Enriching Word Vectors Subword Information. ArXiv:1607.04606 [Cs]. Retrieved http://arxiv.org/abs/1607.04606 [4] Thiele, D. (2022, June 27). “Don’t believe media’s pandemic propaganda!!” Covid-19 affected populist Facebook user comments seven European countries. Presented ICA Regional Conference 2022. Computational Communication Research Central Eastern Europe, Helsinki, Finland. Retrieved https://ucloud.univie.ac./index.php/s/PzGzChXroLCXrtt [5] Meijers, M., & Zaslove, . (2020). Populism Political Parties Expert Survey 2018 (POPPA) [Data set]. Harvard Dataverse. https://doi.org/10.7910/DVN/8NEL7B","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Add id — add_id","title":"Add id — add_id","text":"Function add 'id' data.frame already .","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add id — add_id","text":"","code":"add_id(df)"},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add id — add_id","text":"df data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add id — add_id","text":"data.frame includes column called 'id' type 'character' contains unique identifiers rows data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add id — add_id","text":"Searches data.frame variables include 'id' name. Checks unique identifier. , variable renamed 'word_id' forced type 'character'. , new column named 'word_id' type 'character' created, containing string numbers uniquely identify rows data.frame.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/add_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add id — add_id","text":"","code":"t1 <- data.frame(id = 1:10, a = sample(letters, 10)) t2 <- data.frame(id = rep(1, 10),                 word_id = sprintf(\"%02d\", 1:10),                 a = sample(letters, 10)) t3 <- data.frame(a = sample(letters, 10)) add_id(t1) #>    id a #> 1   1 n #> 2   2 e #> 3   3 s #> 4   4 u #> 5   5 k #> 6   6 q #> 7   7 j #> 8   8 v #> 9   9 w #> 10 10 m add_id(t2) #>    id a #> 1  01 j #> 2  02 o #> 3  03 v #> 4  04 f #> 5  05 l #> 6  06 q #> 7  07 w #> 8  08 d #> 9  09 a #> 10 10 h add_id(t3) #>    a id #> 1  u 01 #> 2  e 02 #> 3  h 03 #> 4  p 04 #> 5  o 05 #> 6  z 06 #> 7  k 07 #> 8  b 08 #> 9  l 09 #> 10 n 10"},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":null,"dir":"Reference","previous_headings":"","what":"Find multi-word expressions. — add_multiwords","title":"Find multi-word expressions. — add_multiwords","text":"Adds multi-word expressions found quanteda tokens object data.frame words.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find multi-word expressions. — add_multiwords","text":"","code":"add_multiwords(   word_df,   tokens,   min_hits = 3,   word_field = \"words\",   levels = c(1, 2) )"},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find multi-word expressions. — add_multiwords","text":"word_df data.frame containing words. tokens word-tokens object, returned quanteda::tokens. min_hits Numerical. Default 3. Minimum occurrence found multi-word expressions. word_field Character. Default \"words\". Name column word_df contains words. levels Numerical (1 2). window size multi-word expressions.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find multi-word expressions. — add_multiwords","text":"data.frame information word_df, added multi-word expressions rows. Additionally, returned data.frame contains columns... 'orig_id' unique identifier original word, re-used word_df created new present '' indicating word origin 'word_id (character) unique identifier words multi-words 'hits indicating number occurrences word multi-word","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_multiwords.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find multi-word expressions. — add_multiwords","text":"","code":"tw_data %<>% head(100) %>% clean_text(text_field = 'full_text') toks <- quanteda::tokens(tw_data$text) data.frame(words = c(\"deutschen\", \"millionen\")) %>%             add_multiwords(tokens = toks,             min_hits = 1,             levels = 1) #> [1] \"Finding multi-word expressions in window = 1...\" #> Joining, by = \"from\" #> [1] \"Adding missing count of original words:\" #> [1] \"Counting word occurrences...\" #>                               words      from word_id orig_id hits #> 3                     der deutschen deutschen   1_001       1    3 #> 1                         deutschen deutschen       1       1    6 #> 7             deutschen autokonzern deutschen   1_005       1    1 #> 9               deutschen bundestag deutschen   1_007       1    1 #> 11 deutschen kommissionspräsidentin deutschen   1_009       1    1 #> 12                deutschen politik deutschen   1_010       1    1 #> 10            deutschen unternehmen deutschen   1_008       1    1 #> 8              deutschen wirtschaft deutschen   1_006       1    1 #> 6                   einer deutschen deutschen   1_004       1    1 #> 5                      im deutschen deutschen   1_003       1    1 #> 4                  keinen deutschen deutschen   1_002       1    1 #> 2                         millionen millionen       2       2    3 #> 17                   millionen euro millionen   2_005       2    1 #> 16                millionen pendler millionen   2_004       2    1 #> 15                    millionen von millionen   2_003       2    1 #> 13                   sind millionen millionen   2_001       2    1 #> 14                vierzig millionen millionen   2_002       2    1"},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Add word_id. — add_word_id","title":"Add word_id. — add_word_id","text":"Function add word_id data.frame already .","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add word_id. — add_word_id","text":"","code":"add_word_id(df)"},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add word_id. — add_word_id","text":"df data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add word_id. — add_word_id","text":"data.frame includes column called 'word_id' type 'character' contains unique identifiers rows data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add word_id. — add_word_id","text":"Searches data.frame variables include 'id' name. Checks unique identifier. , variable renamed 'word_id' forced type 'character'. , new column named 'word_id' type 'character' created, containing string numbers uniquely identify rows data.frame.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/add_word_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add word_id. — add_word_id","text":"","code":"t1 <- data.frame(wid = 1:10, a = sample(letters, 10)) t2 <- data.frame(id = rep(1, 10),                 word_id = sprintf(\"%02d\", 1:10),                 a = sample(letters, 10)) t3 <- data.frame(a = sample(letters, 10)) add_word_id(t1) #>    a word_id #> 1  j       1 #> 2  p       2 #> 3  z       3 #> 4  x       4 #> 5  u       5 #> 6  q       6 #> 7  d       7 #> 8  g       8 #> 9  t       9 #> 10 b      10 add_word_id(t2) #>    id word_id a #> 1   1      01 m #> 2   1      02 r #> 3   1      03 y #> 4   1      04 d #> 5   1      05 h #> 6   1      06 v #> 7   1      07 w #> 8   1      08 l #> 9   1      09 s #> 10  1      10 q add_word_id(t3) #>    a word_id #> 1  b      01 #> 2  q      02 #> 3  l      03 #> 4  c      04 #> 5  y      05 #> 6  g      06 #> 7  t      07 #> 8  z      08 #> 9  h      09 #> 10 m      10"},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean text — clean_text","title":"Clean text — clean_text","text":"Cleans text stored data frame. Function tailored German texts.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean text — clean_text","text":"","code":"clean_text(   df,   text_field = \"text\",   clean_field = \"text\",   tolower = T,   remove_punct = T,   simplify_punct = F,   replace_emojis = T,   replace_numbers = T,   remove_stopwords = F,   store_uncleaned = T,   count = T )"},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean text — clean_text","text":"df data frame. text_field character. Default 'text'. Name column df contains text cleaned. Default 'text'. clean_field character. Default 'text'. Name given new column df contain cleaned text. tolower logical. Lowercase text? Default TRUE. remove_punct logical. Remove punctuation?  Default TRUE. simplify_punct logical. Replace !, :, ?, ... single . Default FALSE. replace_emojis logical. Replace list emojis German words certain emotions ('wut', 'angst', 'freude'). Default TRUE. replace_numbers logical. Replace numbers German words numbers. Default TRUE. remove_stopwords logical. Remove German stopwords 'nltk' list exception words defined author. Default FALSE. store_uncleaned logical. Save uncleaned text 'uncleaned_text' column? Default TRUE. count logical. Count number words strings cleaning? create columns 'n_words' 'n_chars'. Default TRUE.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean text — clean_text","text":"data.frame columns df. cleaned text column specified text_field , specified 'text.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/clean_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean text — clean_text","text":"","code":"tw_data %<>% head(10) %>% clean_text(text_field = 'full_text') tw_data$text #>  [1] \"es geht bei dieser richtungswahl um die frage bleiben wir auf einem kurs der mitte und der stabilität oder bekommen wir ein rot grün rotes abenteuer mit unabsehbaren konsequenzen für arbeitsplätze den wohlstand unseres landes und die zukunft unserer kinder tm merzmail\"       #>  [2] \"aus vielen veranstaltungen der letzten wochen nehme ich den eindruck mit die stimmung für cdu und csu ist besser als die umfragen darin liegt jetzt unsere chance bitte gehen sie alle morgen wählen und geben sie beide stimmen der union tm merzmail unionstärkstekraft\"          #>  [3] \"sendung verpasst sehen sie hier die diskussion zwischen friedrich merz und hubertus heil bei sandra maischberger in der ard mediathek ab ca neunzehn tm\"                                                                                                                            #>  [4] \"ich bin mir nicht sicher ob wir corona einschränkungen wie die maskenpflicht noch brauchen wir sehen in ländern die rascher gelockert haben niedrige inzidenzen und hospitalisierungsquoten und wir sehen wie viel gesellschaftlichen dissens die maßnahmen verursachen tm\"         #>  [5] \"frau nahles hatte recht der mechanismus der mindestlohnkommission ist wichtig damit der mindestlohn kein wahlkampfthema wird der vorschlag für zwölf euro mindestlohn ist rein populistisch jetzt startet ein politischer überbietungswettbewerb zwischen spd und linkspartei tm\"   #>  [6] \"ich war immer für den mindestlohn die entscheidende frage ist wer ihn in zukunft festlegt die tarifvertragsparteien oder der gesetzgeber aus meiner sicht sollte dieses thema in der mindestlohnkommission bleiben und gehört nicht in den bundestag tm merz maischberger\"          #>  [7] \"die oberen zehn der steuerpflichtigen bezahlen mehr als fünfzig des einkommenssteueraufkommens zu behaupten dass sich diese menschen nicht genug an der finanzierung des gemeinwesens beteiligen ist unseriös wir haben die höchsten steuern und abgaben in europa tm maischberger\" #>  [8] \"die vollständige abschaffung des solidaritätszuschlags ist eine frage des respekts und der fairness gegenüber all denjenigen die dreißig jahre lang die deutsche einheit finanziert haben alles andere ist ein gebrochenes versprechen des steuergesetzgebers tm maischberger merz\" #>  [9] \"die cdu hat damals als oppositionspartei der agenda tausende der spd zugestimmt wir sehen es mit bedauern dass diese reformen von den sozialdemokraten schritt für schritt wieder zurückgenommen werden wir müssen zurückkehren zum prinzip fördern und fordern tm maischberger\"    #> [10] \"von analog tausende zu digital tausende schauen sie mal was mein team und ich heute zum abschluss des wahlkampfes im hochsauerlandkreis online gestellt haben leiten sie das video auch gerne an freunde und bekannte weiter hier geht s zum download\""},{"path":"https://thieled.github.io/dictvectoR/reference/compound.html","id":null,"dir":"Reference","previous_headings":"","what":"Assignment pipe — %<>%","title":"Assignment pipe — %<>%","text":"See magrittr::%<>% details.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/compound.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assignment pipe — %<>%","text":"lhs object serves initial value target. rhs function call using magrittr semantics.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":null,"dir":"Reference","previous_headings":"","what":"Get confusion matrix — confuse","title":"Get confusion matrix — confuse","text":"Returns confusion matrix validation scores binary reference variable, binary prediction variable.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get confusion matrix — confuse","text":"","code":"confuse(reference, prediction)"},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get confusion matrix — confuse","text":"reference binary reference vector. prediction binary prediction vector.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get confusion matrix — confuse","text":"confusionMatrix object caret::confusionMatrix().","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get confusion matrix — confuse","text":"values reference prediction must 0 1. variables can stored data.frame vectors equal length. Can type numeric factor.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/confuse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get confusion matrix — confuse","text":"","code":"mtcars$pred <- get_prediction(mtcars, mtcars$am, mtcars$drat) confuse(mtcars$am, mtcars$pred) #> Confusion Matrix and Statistics #>  #>     #>      0  1 #>   0 16  2 #>   1  3 11 #>                                            #>                Accuracy : 0.8438           #>                  95% CI : (0.6721, 0.9472) #>     No Information Rate : 0.5938           #>     P-Value [Acc > NIR] : 0.002273         #>                                            #>                   Kappa : 0.68             #>                                            #>  Mcnemar's Test P-Value : 1.000000         #>                                            #>               Precision : 0.7857           #>                  Recall : 0.8462           #>                      F1 : 0.8148           #>              Prevalence : 0.4062           #>          Detection Rate : 0.3438           #>    Detection Prevalence : 0.4375           #>       Balanced Accuracy : 0.8441           #>                                            #>        'Positive' Class : 1                #>"},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":null,"dir":"Reference","previous_headings":"","what":"Similarity of documents to a dictionary — cossim2dict","title":"Similarity of documents to a dictionary — cossim2dict","text":"Computes cosinal similarity average word vector representation document data frame average word vector representation dictionary, using fasttext word vector model.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Similarity of documents to a dictionary — cossim2dict","text":"","code":"cossim2dict(   df,   dictionary,   model,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Similarity of documents to a dictionary — cossim2dict","text":"df dataframe containing one document per row. dictionary character vector containing keywords dictionary. model fasttext model loaded load_model. text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Similarity of documents to a dictionary — cossim2dict","text":"Numerical. Cosinal similarity, ranging (theoretically) -1 +1. Indicating similarity average fasttext word vector words dictionary average fasttext word vector document df.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Similarity of documents to a dictionary — cossim2dict","text":"Implements method called 'Distributed Dictionary Representation' (DDR), introduced Garten et al. (2018). average dictionary vector calculated mean vector words dictioary, stored character vector. document vectors calculated mean vectors words per observation column named 'text' dataframe. One row dataframe represents one document. , average dictionary vector document vectors L2 normalized. function returns cosinal similarity dictionary vector document dataframe.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Similarity of documents to a dictionary — cossim2dict","text":"Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018). Dictionaries distributions: Combining expert knowledge large scale textual data content analysis. Behavior Research Methods, 50(1), 344 - 361. https://doi.org/10.3758/s13428-017-0875-9","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/cossim2dict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Similarity of documents to a dictionary — cossim2dict","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\", \"tw_demo_model_sml.bin\", package = \"dictvectoR\")) tw_annot %<>% head(100) %>% clean_text(remove_stopwords = TRUE,                                        text_field = \"full_text\") dict <- c(\"skandal\", \"deutschland\", \"steuerzahler\") tw_annot$ddr <- cossim2dict(tw_annot, dict, model)"},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect similar words — detect_similar_words","title":"Detect similar words — detect_similar_words","text":"Detects similar words data.frame, using fastText model. requested, compares similar words along variable specified compare_by, number occurrences stored variable named hits. Counts often one word 'beats' similar pairwise comparison. count returned 'wins_'.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect similar words — detect_similar_words","text":"","code":"detect_similar_words(   word_df,   model,   word_field = \"words\",   compare_by = NULL,   compare_hits = T,   min_simil = 0.7 )"},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect similar words — detect_similar_words","text":"word_df data.frame containing column words multi-word expressions. model fastText model, loaded load_model. word_field Character. name column word_df contains words. compare_by Character. Default NULL. name column compared. compare_hits Logical. Default TRUE. true counts often one word 'beats' similar regard occurrences. min_simil Numerical (0-1). Default .7. Similarity threshold. Word pairs threshold considered dissimilar.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect similar words — detect_similar_words","text":"data.frame. Containing pairwise similarity table similar words.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Detect similar words — detect_similar_words","text":"Forces 'word_df' unique identifying variable called 'word_id'; re-uses variable named 'id' unique.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/detect_similar_words.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect similar words — detect_similar_words","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\", \"tw_demo_model_sml.bin\", package = \"dictvectoR\")) word_df <- data.frame(words = c(\"unsere steuern\", \"steuerzahler\",  \"unsere\",  \"steuern\"), hits = c(2, 3, 15, 4)) detect_similar_words(word_df, model) #>       simil word_id1          word1 hits1 word_id2          word2 hits2 #> 1 0.8014612        3         unsere    15        1 unsere steuern     2 #> 2 0.8014612        4        steuern     4        1 unsere steuern     2 #> 3 0.8445002        4        steuern     4        2   steuerzahler     3 #> 4 0.8014612        1 unsere steuern     2        3         unsere    15 #> 5 0.8014612        1 unsere steuern     2        4        steuern     4 #> 6 0.8445002        2   steuerzahler     3        4        steuern     4 #>   wins_hits1 #> 1          1 #> 2          1 #> 3          1 #> 4          0 #> 5          0 #> 6          0"},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine which similar terms to drop — drop_which","title":"Determine which similar terms to drop — drop_which","text":"Takes pairwise similarity table returned detect_similar_words. Returns data.frame terms dropped according specified decision rules. function can compare words similarity table along two aspects: number comparison wins regarding score specified 'compare_by' detect_similar_words, resp. remove_similar_words. variable must stored similarity table 'wins_score1. number comparison wins regarding frequency occurrences, set 'compare_hits = T' detect_similar_words, resp. remove_similar_words. variable must stored similarity table 'wins_hits1. variables can compared time. 'wins_score1' missing, function compare 'hits'.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine which similar terms to drop — drop_which","text":"","code":"drop_which(simil_table, compare_hits = T, win_threshold = 0.5)"},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine which similar terms to drop — drop_which","text":"simil_table data.frame. pairwise similarity table returned detect_similar_words. compare_hits logical. Default TRUE. TRUE, consider 'hits' comparison. win_threshold Numerical (0-1). Default .5. Determines threshold drop words, defined proportion won pairwise comparisons. hits scores compared, mean proportions. Words suggested dropping computed value smaller value set .","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine which similar terms to drop — drop_which","text":"data.frame words suggested dropping.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/drop_which.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine which similar terms to drop — drop_which","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                           \"tw_demo_model_sml.bin\",                                            package = \"dictvectoR\")) set.seed(1) word_df <- data.frame(words = c(\"unsere steuern\", \"steuerzahler\",                                \"unsere\", \"steuern\"), hits = c(2, 3, 15, 4), score = rnorm(4)) sim_t <- detect_similar_words(word_df, model, compare_by = \"score\") drop_which(sim_t, compare_hits = TRUE, win_threshold = .4) #> # A tibble: 2 × 6 #>   word_id1 words          word_id wins_score_pct wins_hits_pct   win #>   <chr>    <chr>          <chr>            <dbl>         <dbl> <dbl> #> 1 1        unsere steuern 1                  0.5             0  0.25 #> 2 2        steuerzahler   2                  0               0  0"},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter by ntile. — filter_ntile","title":"Filter by ntile. — filter_ntile","text":"Filter data frame top quantile specified variable.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter by ntile. — filter_ntile","text":"","code":"filter_ntile(df, var_field, probs)"},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter by ntile. — filter_ntile","text":"df data.frame. var_field character string indicating name variable used filtering. probs numerical value 0 1 (.e. proportion) indicating quantile threshold used filtering.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter by ntile. — filter_ntile","text":"data.frame, containing rows specified variable greater equal specified threshold.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/filter_ntile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter by ntile. — filter_ntile","text":"","code":"df <- data.frame(a = rnorm(10), b = sample(letters, 10, replace = TRUE)) filter_ntile(df, \"a\", .75) #>           a b #> 4 0.7383247 y #> 5 0.5757814 l #> 7 1.5117812 a"},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":null,"dir":"Reference","previous_headings":"","what":"Find distinctive keywords — find_distinctive","title":"Find distinctive keywords — find_distinctive","text":"Compares word-vector representations words representations annotated data.frame texts. Aims detect words distinctively characterize concept.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find distinctive keywords — find_distinctive","text":"","code":"find_distinctive(   df,   concept_field,   text_field = \"text\",   word_df,   word_field = \"words\",   model )"},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find distinctive keywords — find_distinctive","text":"df data.frame containing one annotated document per row. concept_field character. Name column contains binary, (hand-coded) indicator presence absence concept. text_field character. Name column df contains text documents. Default \"text\". word_df data.frame containing column words multi-word expressions. word_field character. name column word_df contains words. model fastText model, loaded fastrtext::load_model().","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find distinctive keywords — find_distinctive","text":"Takes annotated data.frame df texts input. varialbe specified concept_field df indicates presence absence theoretical concept text. Two average word-vector representations computed, using fastText model: One texts contain concept, one . second data.frame, word_df, contains one (multi-)word per row 'word_field'. Three new columns word_df created: first, ending '_possim', indicates cosine similarity word positive concept corpus. second '_negsim', indicates similarity remaining corpus. third, ending '_distinctive' difference two.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/find_distinctive.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find distinctive keywords — find_distinctive","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") word_df <- data.frame(words = c(\"skandal\", \"deutschland\", \"wundervoll\")) find_distinctive(tw_annot,                  \"pop\",                   word_df = word_df,                   model = model) #>         words pop_possim pop_negsim pop_distinctive #> 1     skandal  0.5809499  0.5490425      0.03190745 #> 2 deutschland  0.7616619  0.7402692      0.02139269 #> 3  wundervoll  0.6919564  0.7408727     -0.04891631"},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Find index of unique id in df. — find_unique_id","title":"Find index of unique id in df. — find_unique_id","text":"Searches data.frame column includes 'id' name unique identifier. Returns index first column data.frame meets conditions.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find index of unique id in df. — find_unique_id","text":"","code":"find_unique_id(df)"},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find index of unique id in df. — find_unique_id","text":"df data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find index of unique id in df. — find_unique_id","text":"numeric, indicating index unique id column.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/find_unique_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find index of unique id in df. — find_unique_id","text":"","code":"df <- data.frame(id_not_unique = rep(1, 10), unique_id = 1:10, also_id = 21:30, other = letters[1:10]) find_unique_id(df) #> [1] 2"},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"Returns Accuracy, Recall, Precision, F1 scores binary reference variable binary prediction variable.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"","code":"get_ARPF(reference, prediction)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"reference binary reference vector. prediction binary prediction vector.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"data.frame columns 'accuracy', 'recall', 'precision', 'F1'.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"values reference prediction must 0 1. variables can stored data.frame vectors equal length. Can type numeric factor.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_ARPF.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Accuracy, Recall, Precision, and F1 — get_ARPF","text":"","code":"mtcars$pred <- get_prediction(mtcars, mtcars$am, mtcars$drat) get_ARPF(mtcars$am, mtcars$pred) #>   accuracy    recall precision        F1 #> 1  0.84375 0.8461538 0.7857143 0.8148148"},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":null,"dir":"Reference","previous_headings":"","what":"Get F1 score for a DDR measure — get_F1","title":"Get F1 score for a DDR measure — get_F1","text":"Returns F1 score DDR measurement predicting binary reference (.e. manually annotated variable).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get F1 score for a DDR measure — get_F1","text":"","code":"get_F1(   df,   dictionary,   model,   reference,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get F1 score for a DDR measure — get_F1","text":"df data.frame containing one annotated document sentence per row. dictionary character vector containing keywords dictionary, passed cossim2dict. model fasttext model loaded load_model. reference Name binary reference column df. text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs DDR measurement. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get F1 score for a DDR measure — get_F1","text":"gradual DDR measurement passed get_prediction() obtain binary prediction logistic regression. F1 scores indicate performance words/dictionaries predicting binary coding, used DDR method. F1 score harmonic mean Recall Precision (1).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get F1 score for a DDR measure — get_F1","text":"(1) Chinchor, N. (1992). MUC-4 evaluation metrics. Proceedings 4th Conference Message Understanding, 22–29. https://doi.org/10.3115/1072064.1072067","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_F1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get F1 score for a DDR measure — get_F1","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") dict <- c(\"skandal\", \"deutschland\", \"steuerzahler\") get_F1(tw_annot, dict, model, 'pop') #> [1] 0.2897527"},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":null,"dir":"Reference","previous_headings":"","what":"Get combinations of keywords — get_combis","title":"Get combinations of keywords — get_combis","text":"Returns combinations keywords.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get combinations of keywords — get_combis","text":"","code":"get_combis(   word_df,   word_field = \"words\",   dims = NULL,   min_per_dim = 1,   max_overall,   limit = NULL,   seed = 1,   save_settings = T,   save_input = F )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get combinations of keywords — get_combis","text":"word_df data.frame containing column words multi-word expressions. word_field character. Default 'words'. name column word_df contains words. dims character. Default NULL. name column word_df groups words dimensions. Can ignored. min_per_dim numerical. Default 1. Minimum number words (per dimension) returned. replaced 1 < 1. max_overall numerical. Maximum number words per returned combination. limit numerical. Default NULL. Limits number combinations equal length per dimension randomization. seed numerical. Default 1. Input make randomization reproducible. save_settings logical. Default TRUE. Saves randomization settings df make reproducible. save_input logical. Default FALSE. Saves input words dims df character string.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get combinations of keywords — get_combis","text":"data.frame combinations. combinations stored list character vectors combs_split. Use column pass get_many_F1s get_many_RPFs. data.frame include string variables words combinations called combs dimension. use columns passing dictionaries get_many_F1s, result faulty average representation, caused representations multi-word expressions queried. Additionally, data.frame includes rowid, count variable number words overall (sum_nterms), counts dimension, can used remove imbalanced dictionaries. requested, settings stores randomization settings, input words dimensions used input.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get combinations of keywords — get_combis","text":"Takes data.frame word_df character column specified words input. default, return combinations various lengths words. Additionally, function can account conceptual dimensions, identified categorical (character, numerical, factor) column word_df specified dims. dimensions specified, function find combinations dimension, return combinations combinations. CAUTION: can lead quickly extremely large number combinations. number combinations can limited several ways: Firstly, minimum number words returned per dimension specified min_per_dim, maximum number words overall set max_overall. Secondly, function allows random sampling combinations. recommended, drastically reduces number returned combinations, reduces computational load, speeds process. (course, comes cost completeness). Random sampling implemented using comboSample function. Setting limit cap number combinations equal length dimension. E.g., limit = 5, min_per_dim = 2, max_overall = 6 set word_df containing two dimensions b, function pick max. five combinations length 2 , five length 2 b, five length 3 , five length 3 b, return combinations combinations.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_combis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get combinations of keywords — get_combis","text":"","code":"test_df <- data.frame(words = letters[1:8],                       dim = rep(paste0(\"c_\", 1:2), 4)) t0 <- get_combis(test_df,                  word_field = \"words\",                  dims = \"dim\",                  max_overall = 5) #> Joining, by = \"rowid\" t1 <- get_combis(test_df,                  word_field = \"words\",                  dims = \"dim\",                  max_overall = 5,                  limit = 5) #> Joining, by = \"rowid\""},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":null,"dir":"Reference","previous_headings":"","what":"Vector representation of a corpus. — get_corpus_representation","title":"Vector representation of a corpus. — get_corpus_representation","text":"Returns average word-vector representation text column data frame, using fastText model.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vector representation of a corpus. — get_corpus_representation","text":"","code":"get_corpus_representation(df, model, text_field = \"text\", normalize = T)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vector representation of a corpus. — get_corpus_representation","text":"df data.frame column containing text identified text_field. model fastText model, loaded fastrtext::load_model(). text_field character string indicating name text column df. normalize Logical. Default TRUE. Normalize vectors Euclidean norm?","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vector representation of a corpus. — get_corpus_representation","text":"single-row sparse matrix class dgCMatrix returned Matrix.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_corpus_representation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Vector representation of a corpus. — get_corpus_representation","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                            \"tw_demo_model_sml.bin\",                                             package = \"dictvectoR\")) tw_annot <- tw_annot %>% head(15) %>% clean_text(text_field = \"full_text\") corpus_rep <- get_corpus_representation(tw_annot, model)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get occurrence frequency of words. — get_hits","title":"Get occurrence frequency of words. — get_hits","text":"Adds number occurrences word multi-word expression quanteda tokens object data.frame. default, checks 'hits' counted fills missing values. Works GLOB-style wildcards.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get occurrence frequency of words. — get_hits","text":"","code":"get_hits(word_df, tokens, word_field = \"words\", replace = F)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get occurrence frequency of words. — get_hits","text":"word_df data.frame containing words. tokens word-tokens object, returned quanteda::tokens. word_field Character. Default \"words\". Name column word_df contains words. replace Logical. Default FALSE. FALSE checks fills 'hits' missing observations. TRUE counts 'hits' words word_df.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get occurrence frequency of words. — get_hits","text":"data.frame column 'hits' indicating frequency term tokens. Arranged descending number hits.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_hits.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get occurrence frequency of words. — get_hits","text":"","code":"tw_data %<>% head(100) %>% clean_text(text_field = 'full_text') toks <- quanteda::tokens(tw_data$text) word_df <- data.frame(words = c(\"der deutschen\", \"steuer*\", \"xyz\")) %>% get_hits(tokens = toks) #> [1] \"Counting word occurrences...\""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":null,"dir":"Reference","previous_headings":"","what":"Get F1 scores for many words or dictionaries. — get_many_F1s","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"Efficiently computes F1 scores elements vector containing keywords, list containing dictionaries, used DDR method.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"","code":"get_many_F1s(   words,   model,   df,   reference,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"words character vector containing keywords, list character vectors containing dictionaries. model fastText model, loaded fastrtext::load_model(). df data.frame containing one annotated document per row. reference Name binary reference column df (character). text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs DDR measurement. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"numerical F1 score returned element (.e. word dictionary) vector list. F1 scores indicate performance words/dictionaries predicting binary coding, used DDR method. resulting gradual measure DDR measure passed logistic regression, binary coding dependent variable. Binary predictions calculated logistic model compared binary coding. F1 score harmonic mean Recall Precision (1).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"(1) Chinchor, N. (1992). MUC-4 evaluation metrics. Proceedings 4th Conference Message Understanding, 22–29. https://doi.org/10.3115/1072064.1072067","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get F1 scores for many words or dictionaries. — get_many_F1s","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") dict_df <- data.frame(id = 1:3) dict_df$combis <- list(c(\"mehrheit deutschen\", \"merkel\", \"skandal\"),                       c(\"steuerzahler\", \"bundesregierung\",                       \"komplett gescheitert\"),                       c( \"arbeitnehmer\", \"groko\", \"wahnsinn\")) dict_df$F1 <- get_many_F1s(dict_df$combis,                            model = model,                            df = tw_annot,                            reference = \"pop\")"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"Efficiently computes F1 scores character vector keywords stored data.frame, list dictionaries stored data.frame - reference data.frame grouped group_field. ADD DETAILS....","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"","code":"get_many_F1s_by_group(   keyword_df,   keyword_field = \"words\",   id = \"id\",   model,   text_df,   group_field,   reference,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"keyword_df data.frame, containing column character vector words, list dictionaries. keyword_field character. name column keyword_df either character vector single keywords, list dictionaries, stored separate character vectors one element per word. id unique identifier keyword. model fastText model, loaded load_model. text_df data.frame containing one annotated document per row. group_field character. Name categorical grouping variable df. reference character. Name binary reference column df. text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs DDR measurement. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"#'@seealso cossim2dict, get_prediction, get_F1, get_many_RPFs, confusionMatrix","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_F1s_by_group.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get F1 for many by a grouping varialbe — get_many_F1s_by_group","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") dict_df <- data.frame(id = 1:3) dict_df$combis <- list(c(\"mehrheit deutschen\", \"merkel\", \"skandal\"),                       c(\"steuerzahler\", \"bundesregierung\",                       \"komplett gescheitert\"),                       c( \"arbeitnehmer\", \"groko\", \"wahnsinn\")) get_many_F1s_by_group(keyword_df = dict_df,                      keyword_field = \"combis\",                      id = \"id\",                      model = model,                      text_df = tw_annot,                      group_field = \"party\",                      reference = 'pop') #> Joining, by = \"id\" #>   id                                              combis    F1_AfD F1_B90Grune #> 1  1                 mehrheit deutschen, merkel, skandal 0.6447368           0 #> 2  2 steuerzahler, bundesregierung, komplett gescheitert 0.6666667           0 #> 3  3                       arbeitnehmer, groko, wahnsinn 0.6953642           0 #>   F1_CDU F1_CSU F1_FDP  F1_Linke F1_SPD #> 1      0      0      0 0.0000000      0 #> 2      0      0      0 0.4210526      0 #> 3      0      0      0 0.3548387      0"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Recall, Precision, F1 for many — get_many_RPFs","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"Efficiently computes Recall, Precision, F1 scores character vector keywords stored data.frame, list dictionaries stored data.frame. Adds Recall, Precision, F1 data.frame.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"","code":"get_many_RPFs(   keyword_df,   keyword_field = \"words\",   model,   text_df,   reference,   text_field = \"text\",   replace_na = c(\"mean-sd\", \"min\", 0, F) )"},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"keyword_df data.frame, containing column character vector words, list dictionaries. keyword_field character. name column keyword_df either character vector single keywords, list dictionaries, stored separate character vectors one element per word. model fastText model, loaded load_model. text_df data.frame containing one annotated document per row. reference Name binary reference column df (character). text_field Name column df contains text documents. Default \"text\". replace_na Specifies value used replace NAs DDR measurement. Default 'mean-sd'. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"element (.e. word dictionary) character vector list data.frame . F1 scores indicate performance words/dictionaries predicting binary coding, used DDR method. resulting gradual measure DDR measure passed logistic regression, binary coding dependent variable. Binary predictions calculated logistic model compared binary coding. F1 score harmonic mean Recall Precision (Chinchor, 1992).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"Chinchor, N. (1992). MUC-4 evaluation metrics. Proceedings 4th Conference Message Understanding, 22–29. https://doi.org/10.3115/1072064.1072067","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_many_RPFs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Recall, Precision, F1 for many — get_many_RPFs","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) tw_annot %<>% clean_text(text_field = \"full_text\") dict_df <- data.frame(id = 1:3) dict_df$combis <- list(c(\"mehrheit deutschen\", \"merkel\", \"skandal\"),                       c(\"steuerzahler\", \"bundesregierung\",                       \"komplett gescheitert\"),                       c( \"arbeitnehmer\", \"groko\", \"wahnsinn\")) get_many_RPFs(keyword_df = dict_df,        keyword_field = \"combis\",        model = model,        text_df = tw_annot, reference = \"pop\") #> Joining, by = \"rowid\" #>   id                                              combis     recall precision #> 1  1                 mehrheit deutschen, merkel, skandal 0.08796296 0.5428571 #> 2  2 steuerzahler, bundesregierung, komplett gescheitert 0.16666667 0.6101695 #> 3  3                       arbeitnehmer, groko, wahnsinn 0.05092593 0.6111111 #>           F1 #> 1 0.15139442 #> 2 0.26181818 #> 3 0.09401709"},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Get binary prediction — get_prediction","title":"Get binary prediction — get_prediction","text":"Returns prediction binary variable gradual measurement using logistic regression.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get binary prediction — get_prediction","text":"","code":"get_prediction(data, dv, iv)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get binary prediction — get_prediction","text":"data data frame. dv binary variable data frame. iv gradual variable data frame used predicting dv.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get binary prediction — get_prediction","text":"factor levels 0, 1.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get binary prediction — get_prediction","text":"Predictions considered '1' predicted probability >= .5.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_prediction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get binary prediction — get_prediction","text":"","code":"data(mtcars) mtcars$pred <- get_prediction(mtcars, mtcars$am, mtcars$drat)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":null,"dir":"Reference","previous_headings":"","what":"Get word representations. — get_word_representations","title":"Get word representations. — get_word_representations","text":"Wrapper around get_sentence_representation return fastText word-vector representation words multiwords, stored column data frame 'word_df.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get word representations. — get_word_representations","text":"","code":"get_word_representations(word_df, model, word_field = \"words\", normalize = T)"},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get word representations. — get_word_representations","text":"word_df data.frame containing column words multiword expressions. model fastText model, loaded fastrtext::load_model(). word_field character string indicating name column word_df contains words. normalize Logical. Default TRUE. Normalize vectors Euclidean norm?","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get word representations. — get_word_representations","text":"sparse matrix class dgCMatrix returned Matrix. number rows word_df number columns dimensions model.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/get_word_representations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get word representations. — get_word_representations","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                            \"tw_demo_model_sml.bin\",                                             package = \"dictvectoR\")) word_df <- data.frame(words = c(\"das ist\", \"ein\", \"test\")) word_rep <- get_word_representations(word_df, model)"},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize a vector — normalize","title":"Normalize a vector — normalize","text":"Normalizes (matrix) vector Euclidean norm, 'L2'-norm.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize a vector — normalize","text":"","code":"normalize(x)"},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize a vector — normalize","text":"x numeric vector, matrix vectors.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Normalize a vector — normalize","text":"norm defined $$||x|| = \\sqrt{\\sum(x^2)}$$ Euclidean norm can interpreted length connection zero point specified vector. normalized vector, , computed : $$x' = x/||x||$$","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize a vector — normalize","text":"","code":"v <- rnorm(10) normalize(v) == v/sqrt(sum(v^2)) #>  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE"},{"path":"https://thieled.github.io/dictvectoR/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://thieled.github.io/dictvectoR/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare text for fastText-model-training — prepare_train_data","title":"Prepare text for fastText-model-training — prepare_train_data","text":"Helper function prepare text training fastText model. (Caution: Tailored texts German language).","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare text for fastText-model-training — prepare_train_data","text":"","code":"prepare_train_data(df, text_field = \"text\", seed = 1)"},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare text for fastText-model-training — prepare_train_data","text":"df data frame. text_field Name column df contains text cleaned. Default 'text'. seed Used random shuffling. Default 1.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare text for fastText-model-training — prepare_train_data","text":"character vector can directly used train fasttext model build_vectors.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare text for fastText-model-training — prepare_train_data","text":"Takes data.frame containing text. First checks length texts specified text_field using nsentence. Texts 3 sentences tokenized tokens sentences. texts passed clean_text fixed settings: tolower = T remove_punct = T replace_emojis = T replace_numbers = T remove_stopwords = F store_uncleaned = F count = T cleaned, short texts shuffled returned character vector.","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/prepare_train_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare text for fastText-model-training — prepare_train_data","text":"","code":"texts <- prepare_train_data(head(tw_data, 10), text_field = 'full_text')"},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove too similar terms — remove_similar_words","title":"Remove too similar terms — remove_similar_words","text":"Removes similar terms data.frame word_df.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove too similar terms — remove_similar_words","text":"","code":"remove_similar_words(   word_df,   model,   word_field = \"words\",   compare_by = NULL,   compare_hits = T,   min_simil = 0.7,   win_threshold = 0.5 )"},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove too similar terms — remove_similar_words","text":"word_df data.frame containing column words multi-word expressions. model fastText model, loaded load_model. word_field character. name column word_df contains words. compare_by character. Default NULL. name column compared. compare_hits logical. Default TRUE. true counts often one word 'beats' similar regard occurrences. min_simil Numerical (0-1). Default .7. Similarity threshold. Word pairs threshold considered dissimilar. win_threshold Numerical (0-1). Default .5. Determines threshold drop words, defined proportion won pairwise comparisons; resp., compare_by compare_hit set, mean proportional wins.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove too similar terms — remove_similar_words","text":"data.frame. Containing pairwise similarity table similar words.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove too similar terms — remove_similar_words","text":"Detects similar words multiword expressions word_df, using fastText model. cosine similarity threshold set min_simil. requested, similar words compared along variable specified compare_by, /number occurrences stored variable named hits. threshold dropping terms set win_threshold. indicates proportion many pairwise comparisons 'won' word question. compare_hits = T compare_by set, mean proportions. word 'wins' comparisions regarding frequency (wins_hits1 == 1), loses comparisions regaring set score ('wins_score1' == 0), value .5. Forces 'word_df' contain unique identifier called 'word_id'; re-uses first variable named 'id' unique identifier.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/remove_similar_words.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove too similar terms — remove_similar_words","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                           \"tw_demo_model_sml.bin\",                                           package = \"dictvectoR\")) set.seed(1) word_df <- data.frame(words = c(\"unsere steuern\",                                \"steuerzahler\",                                \"unsere\",                                \"steuern\"),                      hits = c(2, 3, 15, 4),                      score = rnorm(4)) remove_similar_words(word_df,                     model,                     compare_by = \"score\",                     compare_hits = FALSE,                     win_threshold = .4) #>            words hits      score word_id #> 1 unsere steuern    2 -0.6264538       1 #> 2        steuern    4  1.5952808       4"},{"path":"https://thieled.github.io/dictvectoR/reference/repl_na.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace missing values. — repl_na","title":"Replace missing values. — repl_na","text":"Helper replace missing values. Default repalce NAs 'mean' - 1 'sd'.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/repl_na.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace missing values. — repl_na","text":"","code":"repl_na(x, replace_na = c(\"mean-sd\", \"min\", 0, F))"},{"path":"https://thieled.github.io/dictvectoR/reference/repl_na.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace missing values. — repl_na","text":"x numerical vector. replace_na Specifies value used replace NAs. Can take values: 'mean-sd' (charcter): replace NAs mean - 1sd. Default. 'min' (charcter): replace NAs minimum. 0 (numerical): replace NAs 0. FALSE (logical): replace NAs.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/repl_na.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Replace missing values. — repl_na","text":"","code":"a <- c(rnorm(7), NA, NA, NA) repl_na(a) == repl_na(a, 'mean-sd') #>  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE repl_na(a, 'min') #>  [1]  0.3295078 -0.8204684  0.4874291  0.7383247  0.5757814 -0.3053884 #>  [7]  1.5117812 -0.8204684 -0.8204684 -0.8204684 repl_na(a, 0) #>  [1]  0.3295078 -0.8204684  0.4874291  0.7383247  0.5757814 -0.3053884 #>  [7]  1.5117812  0.0000000  0.0000000  0.0000000 repl_na(a, FALSE) #>  [1]  0.3295078 -0.8204684  0.4874291  0.7383247  0.5757814 -0.3053884 #>  [7]  1.5117812         NA         NA         NA"},{"path":"https://thieled.github.io/dictvectoR/reference/simil_words2rep.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine similarity between words and a given vector. — simil_words2rep","title":"Cosine similarity between words and a given vector. — simil_words2rep","text":"Returns cosine similarity word data frame given vector representation.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/simil_words2rep.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine similarity between words and a given vector. — simil_words2rep","text":"","code":"simil_words2rep(word_df, word_field = \"words\", rep, model)"},{"path":"https://thieled.github.io/dictvectoR/reference/simil_words2rep.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine similarity between words and a given vector. — simil_words2rep","text":"word_df data.frame containing column words multiword expressions. word_field character string indicating name column word_df contains words. rep given word-vector representation, stored numerical vector, matrix, sparse matrix object. length, resp. ncol rep must equal dimensions used fastText model. model fastText model, loaded fastrtext::load_model().","code":""},{"path":[]},{"path":"https://thieled.github.io/dictvectoR/reference/simil_words2rep.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cosine similarity between words and a given vector. — simil_words2rep","text":"","code":"model <- fastrtext::load_model(system.file(\"extdata\",                                \"tw_demo_model_sml.bin\",                                 package = \"dictvectoR\")) pop_rep <- tw_annot %>%            dplyr::filter(pop == 1) %>%            clean_text(text_field = \"full_text\") %>%            get_corpus_representation(model = model) words_df <- data.frame(words = c(\"coronadeutschland\", \"skandal\")) words_df$popsimil <- simil_words2rep(words_df,                                      word_field = \"words\",                                      rep = pop_rep,                                      model)"},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":null,"dir":"Reference","previous_headings":"","what":"Annotated Tweets from German politicians. — tw_annot","title":"Annotated Tweets from German politicians. — tw_annot","text":"dataset containing 1,000 hand-coded Tweets, drawn [tw_data].","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Annotated Tweets from German politicians. — tw_annot","text":"","code":"tw_annot"},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Annotated Tweets from German politicians. — tw_annot","text":"data frame 20838 rows 8 variables: status_id Tweet ID ane Binary hand-coding anti-elitism ppc Binary hand-coding people-centrism pop Binary hand-coding populism. 1 either ane ppc 1 coder Coder identifier. B. AB Tweet parallel code. case, coding 1 least one coder decided code 1. user_id Twitter user ID twitter_handle Twitter handle party Political party followers_count number followers Aug 2022 created_at Date time Tweet created full_text Uncleaned Tweet rel_test Indicates Tweet parallel-coded reliability test ane_A Binary hand-coding anti-elitism coder ane_B Binary hand-coding anti-elitism coder B ppc_A Binary hand-coding people-centrism coder ppc_B Binary hand-coding people-centrism coder B","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Annotated Tweets from German politicians. — tw_annot","text":"dataset contains 1,000 Tweets drawn stratified random sample population data [tw_data]. political party represented (least) 120 Tweets. populist parties AfD (250 Tweets) Die Linke (150 Tweets) oversampled, anticipated populist content parties. TWo expert coders, one author package, hand-coded Tweets along two binary categories populist communication: Anti-elitism people-centrism. coding followed instructions documented online supplementary files Thiele (2022). 90 Tweets parallel-coded reliability testing, resulting Krippendorff's Alphas .86 anti-elitism, .71 people-centrism, documented variables ane_A, ane_B, ppc_A, ppc_B.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_annot.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Annotated Tweets from German politicians. — tw_annot","text":"Thiele, D. (2022). Pandemic Populism? Covid-19 Triggered Populist Facebook User Comments Germany Austria. Politics Governance, 10(1), 185–196. https://doi.org/10.17645/pag.v10i1.4712","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Tweets from German politicians. — tw_data","title":"Tweets from German politicians. — tw_data","text":"dataset containing 20,838 Tweets 32 German politicians, posted 2020-03-11 2021-09-25.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tweets from German politicians. — tw_data","text":"","code":"tw_data"},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Tweets from German politicians. — tw_data","text":"data frame 20838 rows 8 variables: user_id Twitter user ID twitter_handle Twitter handle party Political party followers_count number followers Aug 2022 status_id Tweet ID created_at Date time Tweet created full_text Uncleaned text Tweet poppa_populism Mean populism score POPPA expert survey","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Tweets from German politicians. — tw_data","text":"Twitter, EPINet Twitter Dataset, POPPA","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tweets from German politicians. — tw_data","text":"Twitter accounts selected EPINetz Twitter Politicians Dataset 2021 (König et al., 2022). seven political parties represented German Parliament, five popular Twitter accounts politicians active federal level party representatives selected. accounts 30,000 followers selected. timeframe starts beginning Covid-19 pandemic ends one day German general elections 2021. account, maximum number (3,200) Tweets returned API v2 downloaded August 2022, using rtweet (Kearney, 2022). Quotes, re-tweets, Tweets outside timeframe excluded. Completeness dataset guaranteed. dataset also includes variable extracted POPPA Populism Political Parties Expert Survey, indicating mean expert rating populism per political party.","code":""},{"path":"https://thieled.github.io/dictvectoR/reference/tw_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Tweets from German politicians. — tw_data","text":"Kearney, M. W., Sancho, L. R., Wickham, H., Heiss, ., Briatte, F., & Sidi, J. (2022). rtweet: Collecting Twitter Data. Retrieved https://CRAN.R-project.org/package=rtweet König, T., Schünemann, W. J., Brand, ., Freyberg, J., & Gertz, M. (2022). EPINetz Twitter Politicians Dataset 2021. New Resource Study German Twittersphere Application 2021 Federal Elections. Politische Vierteljahresschrift. https://doi.org/10.1007/s11615-022-00405-7 Meijers, M., & Zaslove, . (2020). Populism Political Parties Expert Survey 2018 (POPPA) (Data set). Harvard Dataverse. https://doi.org/10.7910/DVN/8NEL7B","code":""},{"path":"https://thieled.github.io/dictvectoR/news/index.html","id":"dictvector-0009000","dir":"Changelog","previous_headings":"","what":"dictvectoR 0.0.0.9000","title":"dictvectoR 0.0.0.9000","text":"Added NEWS.md file track changes package.","code":""}]
